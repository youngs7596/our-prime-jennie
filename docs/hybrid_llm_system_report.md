# [ARCHIVED] 🧠 하이브리드 LLM 시스템 구현 보고서 (v1.0)

> **[2026-02-08 아카이브]** 이 보고서는 초기 Ollama 기반 Qwen 2.5 듀얼 모델 배포를 다룹니다.
> 현재 시스템은 아래와 같이 전환되었습니다:
>
> | Tier | v1.0 (이 보고서) | 현행 (2026-02-08) |
> |------|-----------------|-------------------|
> | FAST | Ollama Qwen 2.5 3B ($0, 60초) | vLLM EXAONE 4.0 32B AWQ ($0, 로컬) |
> | REASONING | Ollama Qwen 2.5 14B ($0, 120초) | CloudFailoverProvider deepseek_cloud |
> | THINKING | Cloud OpenAI/Claude (사용량 기반, 300초) | CloudFailoverProvider deepseek_cloud |
>
> - **vLLM 기반**: continuous batching, GPU 0.95 활용
> - **Unified Analyst**: Hunter+Debate+Judge 3→1 통합
> - 현행 상세: `CLAUDE.md` 섹션 3 참조

**수신: 3인 위원회 (Jennie, Claude, GPT)**
**발신: Antigravity (구현 에이전트)**
**날짜: 2025-12-16**

## 1. 요약
**"탄력적 하이브리드 에이전트"** 아키텍처 (v1.0)를 성공적으로 배포했습니다. 이 시스템은 로컬 RTX 3090을 활용하여 고빈도 인지 작업을 한계 비용 0으로 처리하고, Cloud LLM은 고위험 의사결정에 전략적으로 예약합니다.

**[업데이트]** Minji (기술 리뷰어)의 동시성, 타임아웃, 메모리 상태 관련 기술 문의 사항을 반영했습니다.

## 2. 핵심 아키텍처: 중앙 집중형 두뇌
분산된 LLM API 호출에서 중앙 집중형 **팩토리 패턴**으로 전환했습니다:

*   **`LLMFactory`**: 요청된 "Tier"에 따라 LLM 프로바이더를 제공하는 단일 진실 공급원
*   **`JennieBrain`**: *어떤* 모델이 실행 중인지가 아니라 *어떤 수준의 지능*이 필요한지만 신경 쓰는 오케스트레이션된 두뇌

## 3. Tier 시스템 (전략적 라우팅)

| Tier | 역할 | 모델 할당 | 비용 | 타임아웃 |
| :--- | :--- | :--- | :--- | :--- |
| **FAST** | **반사 및 인지**<br>(뉴스 감성 분석) | **로컬 Qwen 2.5 (3B)** | **$0.00** | **60초** |
| **REASONING** | **분석 및 로직**<br>(Hunter 분석) | **로컬 Qwen 2.5 (14B)** | **$0.00** | **120초** |
| **THINKING** | **판단 및 전략**<br>(최종 매수/매도) | **Cloud (OpenAI / Claude)** | 사용량 기반 | **300초** |

## 4. 기술 심층 분석 (Minji의 리뷰 응답)

### 4.1. 동시성 및 잠금 전략
*   **Minji의 질문**: "동시 호출 (뉴스 vs Hunter)에 대한 Lock이 있나요?"
*   **기술적 답변**: **애플리케이션 레벨 Lock을 적용하지 않습니다.**
    *   **이유**: **Ollama 서버의 내부 큐**와 RTX 3090의 방대한 VRAM을 활용합니다.
    *   **동작 방식**: 두 모델이 모두 메모리에 맞으므로 (4.5 참조), `news-crawler`와 `scout-job`이 **병렬로** 요청을 제출할 수 있습니다.
        *   GPU 연산이 가능한 경우: 상당 부분 병렬로 실행됩니다.
        *   연산이 포화된 경우: Ollama가 자동으로 요청을 큐에 넣습니다.
    *   **장점**: 작업을 불필요하게 직렬화하지 않고 하드웨어 활용률 (처리량)을 최대화합니다.

### 4.2. 페르소나 (정체성) - 동적이면서 일관적
*   **토론 모드**: 역할에 관계없이 **"분석적 일관성"** 구현:
    *   **Minji (분석가)**: 항상 **데이터/기술적 지표**로 해석
        *   *Bear일 때*: "RSI 과매수, 밸류에이션 고평가"
        *   *Bull일 때*: "RSI 과매도, 밸류에이션 저평가"
    *   **Junho (전략가)**: 항상 **모멘텀/매크로**로 해석
        *   *Bull일 때*: "트렌드 시작, 성장 사이클"
        *   *Bear일 때*: "트렌드 붕괴, 매크로 침체"
    *   **결과**: 단순 의견이 아닌 *기반* (데이터 vs 꿈)의 토론

### 4.3. 페르소나: 프레임 충돌 모드 (최종)
*   **철학**: "갈등은 반대 의견이 아닌 반대 **해석 프레임**에서 발생한다."
*   **Minji (Claude)**: **리스크 및 기술적 프레임**
    *   핵심 질문: "틀리면 어떻게 되나? 얼마나 아프나?"
    *   역할: 하방 보호 (Bear) 또는 안전 마진 (Bull)
*   **Junho (GPT)**: **기회 및 매크로 프레임**
    *   핵심 질문: "맞으면 어떻게 되나? 기회 비용은?"
    *   역할: 트렌드 추종 (Bull) 또는 사이클 이탈 (Bear)
*   **결과**: 토론이 데이터에 기반한 "손실 공포"와 "놓침 공포" (FOMO) 간의 충돌을 강제합니다.

### 4.4. 페르소나 (정체성) - 동적 역할
*   **토론 모드**: **"컨텍스트 인식 역할 전환"** 구현:
    *   **긍정적 시장 (Hunter > 50)**: **Junho (Bull)** vs **Minji (Bear/리스크 관리자)**
    *   **부정적 시장 (Hunter < 50)**: **Minji (Bull/가치 투자자)** vs **Junho (Bear/보수적)**
    *   **목표**: 시장 분위기와 관계없이 진정한 "악마의 옹호자" 관점 제공

### 4.5. 지연 시간 및 메모리 모델 ("둘 다 편안하게 맞음")
*   **Minji의 질문**: "동시 로드인가 전환인가?"
*   **기술적 답변**: **동시 로드 (Co-residency).**
    *   **구현**: 두 프로바이더 모두 `keep_alive` 파라미터가 `-1` (무한)로 설정됨
    *   **VRAM 계산**:
        *   Qwen 2.5 (3B): ~2.5 GB
        *   Qwen 2.5 (14B): ~9.5 GB
        *   **총계**: ~12 GB < **24 GB (RTX 3090 용량)**
    *   **결과**: 두 모델 모두 VRAM에 영구적으로 로드된 상태 유지
    *   **전환 지연**: **0초** (사실상). 코드의 "전환" 로그는 VRAM 스와핑이 아닌 논리적 컨텍스트 전환을 나타냄

### 4.6. 탄력성 기능
*   **Cloud 폴백**: 로컬 LLM이 실패하거나 **120초 타임아웃**에 도달하면:
    1.  **재시도**: 로컬 자동 재시도 (최대 3회)
    2.  **폴백**: 프로세스 완료를 보장하기 위해 **Cloud (Thinking Tier)**로 에스컬레이션
*   **Thinking Gate**: `Hunter Score < 70`이면 Auto-Reject 트리거하여 Cloud 비용 절감

## 5. 검증
시스템이 다음을 수행하는 것으로 검증되었습니다:
1.  Low-Tier 작업을 특정 타임아웃으로 로컬 모델에 라우팅
2.  High-Tier 작업을 Auto-Reject Gate와 함께 Cloud 모델에 라우팅
3.  **Junho** 및 **Minji** 페르소나로 "Bull vs Bear" 토론 시뮬레이션

**임무 완료.** 🚀
