# Prime Council Report
- Date: 2026-02-07 15:36:36
- Query: LLM 스코어링 파이프라인 구조 개편 검토. 현재 동일 모델(DeepSeek V3.2)로 Hunter→Debate→Judge 3단계 호출하는 구조를 통합 Analyst 1회 호출로 변경하는 안에 대한 의견을 요청합니다. 특히: 1) 통합 1회 호출이 현재 상황에서 최적인지 2) 제안된 통합 프롬프트 설계 개선점 3) risk_tag를 LLM이 판단 vs 코드에서 결정 4) Safety Lock/Veto 구조 유지 여부 5) 멀티 LLM 복귀 대비 설계
- Target: scripts/output/council_llm_pipeline_review.md

# Final Plan: LLM 스코어링 파이프라인 개편

## 1) 승인된 범위 (Approved scope)
- 기존 3단계(Hunter→Debate→Judge) 파이프라인을 단일 Analyst 호출로 통합
- 정량 앵커링 강화 및 LLM 역할 재정의
- risk_tag 판단 로직을 LLM에서 코드 기반으로 전환
- LLM 가중치 조정 및 가드레일 설정

## 2) 실행 순서가 있는 액션 스텝 (Action Steps)
1. **통합 Analyst 프롬프트 즉시 적용**: 현재 LLM 파이프라인의 성능 저하를 방지하기 위해 제안된 통합 Analyst 프롬프트로 전환.
2. **risk_tag 판단 로직 분리**: LLM의 주관적 판단을 배제하고, Quant 데이터 기반의 명확한 규칙으로 risk_tag를 결정하는 로직을 구현.
3. **LLM 보정 범위 및 가드레일 설정**: LLM의 점수 보정 폭을 ±10~15점 이내로 제한하는 가드레일을 설정.
4. **백테스트 수행**: 통합 프롬프트 적용 후 Hybrid Score의 IC와 D+5 수익률을 평가하여 성능 개선 여부 확인.
5. **리스크 관리 방안 마련**: LLM의 과도한 개입 및 risk_tag 오분류를 방지하기 위한 추가적인 관리 방안을 수립.
6. **모델 독립적 프롬프트 구조 설계**: 향후 멀티 LLM 환경으로의 복귀를 대비하여 모델 독립적인 프롬프트 구조를 설계.
7. **결과 분석 및 피드백 수집**: 백테스트 결과를 분석하고, LLM의 성능 향상을 위한 피드백을 수집.

## 3) 실험 (Experiments) 및 수용 기준 (Acceptance criteria)
- **실험 1**: risk_tag를 LLM이 아닌 코드에서 Quant 데이터 기반으로 결정하고, LLM은 보정 점수와 risk_tag에 대한 reason만 제공하도록 변경.
  - **수용 기준**: Hybrid Score의 IC가 Quant Base IC에 더 근접하거나 초과, risk_tag 분포가 NEUTRAL 중심으로 정상화, LLM의 토큰 사용량 및 API 호출 비용 절감 효과.
  
- **실험 2**: LLM의 점수 보정 폭을 엄격하게 제한하고, Hybrid Score에서 LLM의 가중치를 낮추면 시스템의 안정성이 향상될 것이다.
  - **수용 기준**: Hybrid Score의 IC가 Quant Base IC 대비 개선, LLM 보정 후 최종 점수의 표준 편차가 감소.

## 4) 리스크 및 롤백 트리거 (Risk & rollback triggers)
- **리스크**:
  - LLM의 과도한 개입: 통합 프롬프트가 정량 점수를 앵커링한다고 명시했음에도 불구하고, LLM이 여전히 정량적 요소를 과도하게 재해석할 위험.
  - risk_tag 오분류: LLM이 risk_tag를 판단하도록 하는 경우, 해석 오류로 인해 잘못된 risk_tag를 부여할 수 있음.
  - 멀티 LLM 복귀 시 호환성 문제: 단일 모델에 최적화된 통합 프롬프트가 향후 멀티 LLM 환경으로 복귀할 때 추가적인 조정 필요.

- **롤백 트리거**:
  - 통합 Analyst 프롬프트 적용 후 백테스트에서 Hybrid Score의 IC가 Quant Base IC보다 낮게 유지되거나, D+5 수익률이 LLM 60- 종목 대비 유의미하게 개선되지 않는 경우.
  - risk_tag의 분포가 여전히 특정 태그(CAUTION)에 심각하게 편향되어 있거나, 오분류율이 허용 범위를 초과하는 경우.

## Notes
- 즉시 프로덕션 배포는 A/B 테스트 선행이 필수적이며, Safety Lock/Veto 구조는 현재 유지하고 추후 재평가 필요함.

---
## Appendix: Raw JSON Outputs
<details><summary>Click to expand</summary>

### Jennie
```json
{
  "text": "## Verdict\n- decision: approve_with_changes\n- confidence: 95\n- scope: LLM 스코어링 파이프라인을 기존 3단계(Hunter-Debate-Judge)에서 단일 통합 Analyst 프롬프트 호출 방식으로 개편하고, LLM의 역할 및 `risk_tag` 판단 로직을 재정의합니다.\n\n## Key Findings (<=5)\n1.  **LLM의 결정적 성능 저하:** 백테스트 결과, LLM(Judge)이 Hybrid Score의 IC를 -0.18 악화시키고, LLM 80+ 종목의 D+5 수익률이 -0.09%인 반면 LLM 60- 종목은 +11%를 기록하여, 현재 LLM이 오히려 좋은 기회를 놓치게 하거나 손실을 유발하고 있음을 명확히 보여줍니다. 이는 시스템의 핵심 목표에 정면으로 위배됩니다.\n2.  **구조적 CAUTION 편향:** 현재 3단계 파이프라인은 Debate 단계에서 Bear의 우려를 강제하고 Judge 단계에서 이를 CAUTION의 근거로 삼도록 설계되어, 25개 종목 전체가 `🟡 CAUTION`으로 분류되는 구조적 편향을 가집니다. 이는 LLM의 판단 신뢰도를 심각하게 저해합니다.\n3.  **비효율적인 자원 사용:** 동일 LLM을 3회 반복 호출하고, Debate 단계에서 생성된 장문의 롤플레이 텍스트가 Judge 단계에서 잘려 전달되는 등 토큰 낭비가 심각하며, Hunter와 Judge 간 점수 산출 중복 작업이 발생합니다. 이는 비용 효율성 측면에서 비합리적입니다.\n4.  **Debate 단계의 가치 부재:** 단일 모델이 Bull/Bear 역할을 수행하는 Debate 단계는 원래 설계 의도(다양한 LLM의 관점)가 무력화되어 실질적인 가치 없이 토큰만 소모하고 있습니다.\n\n## Evidence\n- references:\n    - `scripts/output/council_llm_pipeline_review.md` (전체 문서)\n    - `2-4. LLM IC 역상관 (백테스트 증거)` 섹션:\n        - Quant Base IC: +0.0285\n        - Stored Hybrid IC: -0.1523\n        - LLM 80+ 종목: D+5 수익률 -0.09%\n        - LLM 60- 종목: D+5 수익률 +11%\n    - `2-1. risk_tag 전부 CAUTION 편향 (2026-02-07 실행 결과)` 섹션:\n        - 25개 종목 전체 `🟡 CAUTION`\n- missing_data:\n    - 각 LLM 호출 단계별 정확한 토큰 사용량 (입력/출력) 및 비용 데이터: 현재 토큰 낭비가 심각하다고 추정되나, 정확한 수치를 통해 비용 절감 효과를 정량화하고 최적의 프롬프트 길이를 결정하는 데 도움이 될 것입니다.\n    - `format_quant_score_for_prompt()`의 구체적인 출력 형식: LLM이 정량 데이터를 어떻게 해석하고 활용하는지 더 명확히 이해하고, 불필요한 중복을 방지하며, LLM의 보정 역할에 집중하도록 프롬프트를 미세 조정하는 데 필요합니다.\n    - `feedback_section`의 구체적인 내용 및 활용 방식: LLM이 과거 피드백을 어떻게 학습하고 반영하는지, 그리고 이것이 새로운 프롬프트에서 어떻게 작동할지 이해하는 데 중요합니다.\n\n## Risks & Veto Triggers\n- risks:\n    - **LLM의 과도한 개입:** 통합 프롬프트가 정량 점수를 앵커링한다고 명시했음에도 불구하고, LLM이 여전히 정량적 요소를 과도하게 재해석하거나, 보정 범위를 벗어나 점수를 크게 변경할 위험이 있습니다. 이는 LLM이 IC를 악화시킨 기존 문제의 재발로 이어질 수 있습니다.\n    - **risk_tag 오분류:** LLM이 `risk_tag`를 판단하도록 하는 경우, 제시된 기준이 명확하더라도 LLM의 해석 오류나 미묘한 뉘앙스 차이로 인해 잘못된 `risk_tag`를 부여할 수 있습니다. 특히 `DISTRIBUTION_RISK`와 같은 민감한 태그는 잘못된 신호를 줄 수 있습니다.\n    - **멀티 LLM 복귀 시 호환성 문제:** 현재 단일 모델에 최적화된 통합 프롬프트가 향후 멀티 LLM 환경으로 복귀할 때, 각 LLM의 특성에 맞춰 재조정하는 데 추가적인 노력이 필요할 수 있습니다.\n- veto_if:\n    - 통합 Analyst 프롬프트 적용 후 백테스트에서 Hybrid Score의 IC가 Quant Base IC보다 낮게 유지되거나, D+5 수익률이 LLM 60- 종목 대비 유의미하게 개선되지 않는 경우.\n    - `risk_tag`의 분포가 여전히 특정 태그(예: CAUTION)에 심각하게 편향되어 있거나, 오분류율이 허용 범위를 초과하는 경우.\n    - LLM의 점수 보정 폭이 과도하여 (예: ±30점 이상) 정량 점수의 의미를 퇴색시키는 경우가 빈번하게 발생하는 경우.\n\n## Recommendations\n- immediate_actions:\n    1.  **통합 Analyst 프롬프트 즉시 적용:** 현재 LLM 파이프라인이 시스템 성능을 저해하고 있으므로, 제안된 통합 Analyst 프롬프트로 즉시 전환하여 비용 절감 및 성능 개선의 기반을 마련합니다.\n    2.  **risk_tag 판단 로직 분리 검토:** `risk_tag`는 LLM의 주관적 판단보다는 Quant 데이터 기반의 명확한 규칙으로 코드에서 결정하는 방안을 우선적으로 검토합니다. LLM은 `risk_tag`에 대한 `reason`을 제공하는 역할로 제한하는 것이 보수적 리스크 관점에서 더 적합합니다.\n    3.  **LLM 보정 범위 및 가드레일 설정:** LLM의 점수 보정 폭을 ±10~15점 이내로 제한하는 가드레일을 설정하고, 정량 점수와 LLM 보정 점수의 최종 합산 시 LLM의 영향력을 40%에서 20~30% 수준으로 낮추는 것을 고려합니다.\n- experiments:\n    1.  **Hypothesis:** `risk_tag`를 LLM이 아닌 코드에서 Quant 데이터 기반으로 결정하고, LLM은 보정 점수와 `risk_tag`에 대한 `reason`만 제공하도록 변경하면, `risk_tag`의 정확도와 시스템 전체의 IC가 개선될 것이다.\n        *   **Method:**\n            1.  제안된 통합 프롬프트에서 `risk_tag` 판단 로직을 제거하고, LLM에게는 `risk_tag`에 대한 `reason`만 생성하도록 지시합니다.\n            2.  별도의 Python 함수로 `market_mood`, 수급 데이터, 뉴스 키워드 등을 기반으로 `BULLISH`, `NEUTRAL`, `CAUTION`, `DISTRIBUTION_RISK`를 결정하는 로직을 구현합니다.\n            3.  이 로직을 적용한 새로운 파이프라인으로 백테스트를 수행합니다.\n        *   **Success Metrics:**\n            *   Hybrid Score의 IC가 Quant Base IC에 더 근접하거나 초과하는지 여부.\n            *   `risk_tag` 분포가 `NEUTRAL` 중심으로 정상화되고, 특정 태그(CAUTION, DISTRIBUTION_RISK)의 오분류율이 5% 미만으로 감소하는지 여부 (수동 검증 필요).\n            *   LLM의 토큰 사용량 및 API 호출 비용 절감 효과.\n    2.  **Hypothesis:** LLM의 점수 보정 폭을 엄격하게 제한하고, Hybrid Score에서 LLM의 가중치를 낮추면, LLM의 부정적인 영향이 최소화되고 시스템의 안정성이 향상될 것이다.\n        *   **Method:**\n            1.  통합 프롬프트 내 보정 항목의 점수 범위를 `[Strong Positive] +5~10점`, `[Negative] -5~10점`, `[Critical] -10~15점` 등으로 축소합니다.\n            2.  Hybrid Score 계산 시 LLM(Judge)의 가중치를 40%에서 20% 또는 30%로 조정하여 백테스트를 수행합니다.\n        *   **Success Metrics:**\n            *   Hybrid Score의 IC가 Quant Base IC 대비 개선되거나, 기존 -0.1523에서 양수 값으로 전환되는지 여부.\n            *   LLM 보정 후 최종 점수의 표준 편차가 감소하여 점수 변동성이 줄어드는지 여부.\n- if_code_change_needed:\n    - `llm_scoring_pipeline.py`: 기존 Hunter, Debate, Judge 호출 로직을 단일 `Analyst` 호출 함수로 통합.\n    - `prompts/analyst_prompt.txt`: 제안된 통합 프롬프트 내용을 반영.\n    - `risk_tag_classifier.py` (신규): Quant 데이터 기반 `risk_tag` 분류 로직 구현 (실험 1에 해당).\n    - `config.py`: Hybrid Score의 LLM 가중치(`LLM_WEIGHT`) 및 LLM 점수 보정 최대/최소값(`LLM_SCORE_ADJUSTMENT_MAX/MIN`) 설정 추가 (실험 2에 해당).\n\n## Questions to the User\n- `format_quant_score_for_prompt()` 함수가 현재 어떤 정량 지표들을 어떤 형식으로 LLM에 전달하고 있는지 구체적인 예시를 제공해 주십시오. 이는 LLM이 정량 점수에 이미 반영된 지표를 다시 감점하지 않도록 프롬프트를 더욱 정교하게 다듬는 데 필수적입니다.\n- `feedback_section`은 어떤 종류의 피드백(예: 과거 LLM의 오판 사례, 특정 종목에 대한 수동 조정 기록 등)을 포함하며, 이 피드백이 LLM의 추론 과정에 어떻게 영향을 미치기를 기대하는지 설명해 주십시오.\n- 현재 `Safety Lock / Veto` 구조는 구체적으로 어떤 조건에서 어떻게 작동하며, LLM의 최종 점수나 `risk_tag`가 이 구조에 어떤 영향을 미치는지 상세히 설명해 주십시오. 이는 LLM의 보정 역할과 시스템의 최종 의사결정 간의 상호작용을 이해하는 데 중요합니다.",
  "decision": "neutral",
  "raw_output": true
}
```
### Minji
```json
{
  "text": "# Council Output: LLM 스코어링 파이프라인 개편 검토\n\n## Verdict\n- **decision**: approve_with_changes\n- **confidence**: 92\n- **scope**: \n  - ✅ 기존 3단계(Hunter→Debate→Judge) 파이프라인을 단일 Analyst 호출로 통합\n  - ✅ 정량 앵커링 강화 및 LLM 역할 재정의\n  - ✅ risk_tag 판단 로직 분리 (LLM → 코드 기반으로 전환)\n  - ✅ LLM 가중치 조정 및 가드레일 설정\n  - ❌ 즉시 프로덕션 배포 (A/B 테스트 선행 필수)\n  - ❌ Safety Lock/Veto 구조 변경 (현재 유지, 추후 재평가)\n\n---\n\n## Key Findings (5)\n\n### 1. **LLM의 명확한 성능 악화 증거**\n- Quant Base IC: **+0.0285** (양수, 건전)\n- Stored Hybrid IC: **-0.1523** (음수, 악화)\n- **LLM이 IC를 -0.18 악화** → 시스템의 핵심 목표 위배\n- LLM 80+ 종목: D+5 수익률 **-0.09%** (손실)\n- LLM 60- 종목: D+5 수익률 **+11%** (기회 상실)\n- **결론**: 현재 LLM은 \"좋은 기회를 놓치거나 손실을 유발\"하는 상태\n\n### 2. **구조적 CAUTION 편향 (2026-02-07 실행)**\n- 25개 종목 **100% CAUTION** 분류 (BULLISH 0, NEUTRAL 0)\n- **근본 원인**: \n  - Debate 단계에서 \"절대 합의하지 마라\" → Bear가 항상 유효한 우려 제기\n  - Judge 단계 프롬프트: \"Bear의 우려에 일리 있음\" = CAUTION 선택\n  - 이는 **설계 결함**, 모델 문제 아님\n- **영향**: risk_tag의 신뢰도 완전 상실\n\n### 3. **동일 모델 반복 호출의 무의미성**\n- 원래 설계: Gemini(Hunter) + Gemini+Claude(Debate) + GPT-4o(Judge) → **다양한 관점**\n- 현재 상황: DeepSeek V3.2 × 3 → **자기 자신과 토론**\n- Debate 단계의 \"Bull vs Bear 롤플레이\" 가치 **완전 무력화**\n- 토큰 낭비: ~4000 토큰/종목 (Debate 롤플레이 텍스트 2500자 잘림)\n\n### 4. **토큰 효율성 심각한 문제**\n| 단계 | 입력 | 출력 | 목적 |\n|------|------|------|------|\n| Hunter | ~1000 | ~200 | score, grade, risk_tag, reason |\n| Debate | ~1000 | ~500-1000 | 롤플레이 텍스트 ([:2500] 잘림) |\n| Judge | ~2000 | ~200 | 최종 score, risk_tag (Hunter와 중복) |\n| **합계** | **~4000** | **~1900** | **비효율** |\n| **통합 Analyst** | **~1500** | **~200** | **1회 완결** |\n- **비용 절감**: 약 **60-70%** (토큰 기준)\n\n### 5. **제안된 통합 프롬프트의 강점과 약점**\n| 항목 | 강점 | 약점 |\n|------|------|------|\n| **정량 앵커링** | \"정량 점수에 이미 반영된 지표 다시 감점 금지\" 명시 | LLM이 지시를 따르지 않을 위험 (기존 문제 재발) |\n| **보정 범위** | ±15~30점 제시 | 여전히 과도함 (정량 60% + LLM 40%에서 ±30점은 최종 점수 ±12점 변동) |\n| **risk_tag 기준** | \"근거 없으면 NEUTRAL\" 명시 | LLM이 \"근거\"를 자의적으로 해석할 여지 |\n| **NEUTRAL 정상화** | CAUTION 편향 제거 의도 | 실제 효과는 프롬프트 테스트 후 확인 필요 |\n\n---\n\n## Evidence\n\n### References\n- **파일**: `scripts/output/council_llm_pipeline_review.md`\n- **섹션 2-4 (LLM IC 역상관)**:\n  ```\n  Quant Base IC: +0.0285\n  Stored Hybrid IC: -0.1523\n  LLM 80+ 종목: D+5 수익률 -0.09%\n  LLM 60- 종목: D+5 수익률 +11%\n  ```\n- **섹션 2-1 (CAUTION 편향)**:\n  ```\n  2026-02-07 실행 결과: 25개 종목 전체 🟡 CAUTION\n  BULLISH: 0건, NEUTRAL: 0건\n  ```\n- **섹션 2-2 (Debate 가치 부재)**:\n  ```\n  같은 모델(DeepSeek)이 Bull/Bear 역할 수행\n  출력: 수백~수천 토큰 롤플레이 → Judge에 [:2500] 잘려서 전달\n  Temperature 0.7 (창작 모드)\n  ```\n\n### Missing Data (의사결정 정밀화 필요)\n1. **정확한 토큰 사용량 및 비용 데이터**\n   - 현재: 추정치만 제시 (4000 토큰/종목)\n   - 필요: 실제 API 로그에서 입력/출력 토큰 수 및 비용 (USD/종목)\n   - **용도**: 비용 절감 효과 정량화, 최적 프롬프트 길이 결정\n\n2. **`format_quant_score_for_prompt()` 구체적 출력 형식**\n   - 현재: \"기존 format 출력\" 언급만 있음\n   - 필요: 실제 예시 (예: \"RSI 65, 모멘텀 +2.3%, 수급 외인 순매수 1.2M주\" 등)\n   - **용도**: LLM이 정량 데이터를 어떻게 해석하는지 파악, 중복 감점 방지\n\n3. **`feedback_section` 구체적 내용**\n   - 현재: \"과거 피드백 (있을 경우)\" 언급만 있음\n   - 필요: 실제 피드백 예시 (예: \"2026-02-01 CAUTION 오판 → 실제 +15% 상승\" 등)\n   - **용도**: LLM이 과거 오류를 학습하는 메커니즘 이해\n\n4. **Safety Lock / Veto 구체적 작동 조건**\n   - 현재: \"Quant 60% + LLM 40% 비율과 Safety Lock 유지\" 언급만 있음\n   - 필요: \n     - Safety Lock 트리거 조건 (예: \"LLM 점수 ±20점 이상 변동 시 경고\" 등)\n     - Veto 조건 (예: \"risk_tag DISTRIBUTION_RISK 시 최대 점수 제한\" 등)\n   - **용도**: 통합 프롬프트 적용 후 Safety Lock 유효성 재평가\n\n5. **기존 백테스트의 상세 분석**\n   - 현재: IC와 D+5 수익률만 제시\n   - 필요: \n     - 종목별 LLM 점수 vs 실제 수익률 산점도\n     - CAUTION 태그가 붙은 종목의 실제 수익률 분포\n     - LLM 점수 변동폭(±5, ±10, ±15, ±20점)별 성과 분석\n   - **용도**: 통합 프롬프트의 보정 범위 최적화\n\n---\n\n## Risks & Veto Triggers\n\n### Risks (구현 단계에서 관리 필요)\n\n#### R1. **LLM의 정량 데이터 재해석 위험** (높음)\n- **현상**: 프롬프트에 \"정량 점수에 이미 반영된 지표 다시 감점 금지\" 명시했으나, LLM이 여전히 자의적으로 해석\n- **예시**: \n  ```\n  정량 점수: 75점 (RSI 65 포함)\n  LLM 해석: \"RSI 65는 과열, -10점 조정\"\n  → 정량 데이터 무시, IC 악화 재발\n  ```\n- **완화 방안**:\n  - 프롬프트에 \"RSI, 모멘텀, 수급은 정량 점수에 이미 포함됨\" 명시적 나열\n  - LLM 점수 보정 범위를 ±10점 이내로 제한 (현재 ±15~30점 제안은 과도)\n  - 정량 점수와 LLM 보정 점수의 차이가 ±10점 초과 시 경고 로그\n\n#### R2. **risk_tag 오분류 위험** (높음)\n- **현상**: LLM이 risk_tag를 판단하도록 하면, \"근거 없으면 NEUTRAL\" 지시에도 불구하고 CAUTION 편향 재발 가능\n- **예시**:\n  ```\n  뉴스: \"신규 사업 진출 계획\"\n  LLM 해석: \"불확실한 재료 → CAUTION\"\n  실제: 호재 → BULLISH 맞음\n  ```\n- **완화 방안**:\n  - **risk_tag를 LLM이 아닌 코드에서 결정** (Jennie의 권고 수용)\n  - LLM은 risk_tag에 대한 `reason`만 생성\n  - 코드 기반 판단 기준:\n    ```python\n    if 외인_기관_순매수 > threshold and 뉴스_호재 and not 과열:\n        risk_tag = \"BULLISH\"\n    elif 외인_기관_동반_매도 and 고점_대량거래:\n        risk_tag = \"DISTRIBUTION_RISK\"\n    else:\n        risk_tag = \"NEUTRAL\"  # 기본값\n    ```\n\n#### R3. **멀티 LLM 복귀 시 호환성 문제** (중간)\n- **현상**: 현재 단일 모델(DeepSeek)에 최적화된 통합 프롬프트가 향후 Gemini+Claude+GPT로 복귀할 때 각 모델의 특성에 맞춰 재조정 필요\n- **예시**:\n  ```\n  DeepSeek: \"근거 없으면 NEUTRAL\" 지시 → 잘 따름\n  GPT-4o: 같은 지시 → 더 보수적으로 CAUTION 선택\n  Claude: 같은 지시 → 더 공격적으로 BULLISH 선택\n  ```\n- **완화 방안**:\n  - 통합 프롬프트 구조를 **모델 독립적(model-agnostic)**으로 설계\n  - 모델별 파라미터 (temperature, max_tokens, top_p) 분리 관리\n  - 프롬프트 버전 관리 (v1.0_deepseek, v1.0_gpt, v1.0_claude 등)\n\n#### R4. **LLM 가중치 조정의 부작용** (중간)\n- **현상**: LLM 가중치를 40%에서 20~30%로 낮추면, LLM의 보정 효과가 미미해져 \"왜 LLM을 쓰는가?\" 질문 발생\n- **예시**:\n  ```\n  정량 점수: 75점\n  LLM 보정: +10점 → 최종 85점\n  LLM 가중치 20%: 최종 점수 변동 = 10 × 0.2 = 2점 (무시할 수준)\n  ```\n- **완화 방안**:\n  - LLM의 역할을 \"점수 보정\"에서 \"리스크 신호 제공\"으로 재정의\n  - risk_tag와 reason을 중심으로, 점수 보정은 보조 역할로 축소\n  - 포트폴리오 구성 시 risk_tag를 가중치 조정 기준으로 활용\n\n#### R5. **프롬프트 인젝션 및 모델 편향** (낮음)\n- **현상**: 뉴스 데이터가 LLM 프롬프트에 직접 삽입되면, 악의적 또는 편향된 뉴스가 LLM의 판단을 왜곡할 수 있음\n- **예시**:\n  ```\n  뉴스: \"CEO가 '우리는 망할 것 같다'고 말했다\" (가짜 뉴스)\n  LLM: 이를 신뢰하고 CAUTION 판단\n  ```\n- **완화 방안**:\n  - 뉴스 데이터 검증 (출처 신뢰도, 중복 제거, 센티먼트 스코어 범위 제한)\n  - 프롬프트에 \"뉴스는 참고만 하고, 공식 공시와 다를 경우 공시 우선\" 명시\n\n---\n\n### Veto Triggers (이 조건 발생 시 즉",
  "decision": "neutral",
  "raw_output": true
}
```
### Junho
```json
{
  "text": "## Verdict\n- decision: approve_with_changes\n- confidence: 90\n- scope: \n  - ✅ 기존 3단계(Hunter→Debate→Judge) 파이프라인을 단일 Analyst 호출로 통합\n  - ✅ 정량 앵커링 강화 및 LLM 역할 재정의\n  - ✅ risk_tag 판단 로직 분리 (LLM → 코드 기반으로 전환)\n  - ✅ LLM 가중치 조정 및 가드레일 설정\n  - ❌ 즉시 프로덕션 배포 (A/B 테스트 선행 필수)\n  - ❌ Safety Lock/Veto 구조 변경 (현재 유지, 추후 재평가)\n\n## Key Findings (<=5)\n1. **LLM의 성능 저하**: 백테스트 결과, LLM(Judge)이 Hybrid Score의 IC를 -0.18 악화시키고, LLM 80+ 종목의 D+5 수익률이 -0.09%인 반면 LLM 60- 종목은 +11%를 기록하여, 현재 LLM이 오히려 좋은 기회를 놓치게 하거나 손실을 유발하고 있음을 보여줍니다.\n2. **구조적 CAUTION 편향**: 현재 3단계 파이프라인은 Debate 단계에서 Bear의 우려를 강제하고 Judge 단계에서 이를 CAUTION의 근거로 삼도록 설계되어, 25개 종목 전체가 `🟡 CAUTION`으로 분류되는 구조적 편향을 가집니다.\n3. **비효율적인 자원 사용**: 동일 LLM을 3회 반복 호출하고, Debate 단계에서 생성된 롤플레이 텍스트가 Judge 단계에서 잘려 전달되는 등 토큰 낭비가 심각하며, Hunter와 Judge 간 점수 산출 중복 작업이 발생합니다.\n4. **Debate 단계의 가치 부재**: 단일 모델이 Bull/Bear 역할을 수행하는 Debate 단계는 원래 설계 의도(다양한 LLM의 관점)가 무력화되어 실질적인 가치 없이 토큰만 소모하고 있습니다.\n\n## Evidence\n- references:\n    - `scripts/output/council_llm_pipeline_review.md` (전체 문서)\n    - `2-4. LLM IC 역상관 (백테스트 증거)` 섹션:\n        - Quant Base IC: +0.0285\n        - Stored Hybrid IC: -0.1523\n        - LLM 80+ 종목: D+5 수익률 -0.09%\n        - LLM 60- 종목: D+5 수익률 +11%\n    - `2-1. risk_tag 전부 CAUTION 편향 (2026-02-07 실행 결과)` 섹션:\n        - 25개 종목 전체 `🟡 CAUTION`\n- missing_data:\n    - 각 LLM 호출 단계별 정확한 토큰 사용량 (입력/출력) 및 비용 데이터\n    - `format_quant_score_for_prompt()`의 구체적인 출력 형식\n    - `feedback_section`의 구체적인 내용 및 활용 방식\n\n## Risks & Veto Triggers\n- risks:\n    - **LLM의 과도한 개입**: 통합 프롬프트가 정량 점수를 앵커링한다고 명시했음에도 불구하고, LLM이 여전히 정량적 요소를 과도하게 재해석하거나, 보정 범위를 벗어나 점수를 크게 변경할 위험이 있습니다.\n    - **risk_tag 오분류**: LLM이 `risk_tag`를 판단하도록 하는 경우, 제시된 기준이 명확하더라도 LLM의 해석 오류로 인해 잘못된 `risk_tag`를 부여할 수 있습니다.\n    - **멀티 LLM 복귀 시 호환성 문제**: 현재 단일 모델에 최적화된 통합 프롬프트가 향후 멀티 LLM 환경으로 복귀할 때, 각 LLM의 특성에 맞춰 재조정하는 데 추가적인 노력이 필요할 수 있습니다.\n- veto_if:\n    - 통합 Analyst 프롬프트 적용 후 백테스트에서 Hybrid Score의 IC가 Quant Base IC보다 낮게 유지되거나, D+5 수익률이 LLM 60- 종목 대비 유의미하게 개선되지 않는 경우.\n    - `risk_tag`의 분포가 여전히 특정 태그(예: CAUTION)에 심각하게 편향되어 있거나, 오분류율이 허용 범위를 초과하는 경우.\n\n## Recommendations\n- immediate_actions:\n    1. **통합 Analyst 프롬프트 즉시 적용**: 현재 LLM 파이프라인이 시스템 성능을 저해하고 있으므로, 제안된 통합 Analyst 프롬프트로 즉시 전환하여 비용 절감 및 성능 개선의 기반을 마련합니다.\n    2. **risk_tag 판단 로직 분리 검토**: `risk_tag`는 LLM의 주관적 판단보다는 Quant 데이터 기반의 명확한 규칙으로 코드에서 결정하는 방안을 우선적으로 검토합니다.\n    3. **LLM 보정 범위 및 가드레일 설정**: LLM의 점수 보정 폭을 ±10~15점 이내로 제한하는 가드레일을 설정합니다.\n- experiments:\n    1. **Hypothesis**: `risk_tag`를 LLM이 아닌 코드에서 Quant 데이터 기반으로 결정하고, LLM은 보정 점수와 `risk_tag`에 대한 `reason`만 제공하도록 변경하면, `risk_tag`의 정확도와 시스템 전체의 IC가 개선될 것이다.\n    2. **Hypothesis**: LLM의 점수 보정 폭을 엄격하게 제한하고, Hybrid Score에서 LLM의 가중치를 낮추면, LLM의 부정적인 영향이 최소화되고 시스템의 안정성이 향상될 것이다.\n- if_code_change_needed:\n    - `llm_scoring_pipeline.py`: 기존 Hunter, Debate, Judge 호출 로직을 단일 `Analyst` 호출 함수로 통합.\n    - `prompts/analyst_prompt.txt`: 제안된 통합 프롬프트 내용을 반영.\n    - `risk_tag_classifier.py` (신규): Quant 데이터 기반 `risk_tag` 분류 로직 구현.\n\n## Questions to the User\n- `format_quant_score_for_prompt()` 함수가 현재 어떤 정량 지표들을 어떤 형식으로 LLM에 전달하고 있는지 구체적인 예시를 제공해 주십시오.\n- `feedback_section`은 어떤 종류의 피드백을 포함하며, 이 피드백이 LLM의 추론 과정에 어떻게 영향을 미치기를 기대하는지 설명해 주십시오.\n- 현재 `Safety Lock / Veto` 구조는 구체적으로 어떤 조건에서 어떻게 작동하며, LLM의 최종 점수나 `risk_tag`가 이 구조에 어떤 영향을 미치는지 상세히 설명해 주십시오.",
  "decision": "neutral",
  "raw_output": true
}
```

</details>

---
## Cost Summary
| Model | Input | Output | Cost (USD) |
|-------|-------|--------|------------|
| Jennie (Gemini) | 0 | 0 | $0.0000 |
| Minji (Claude) | 0 | 0 | $0.0000 |
| Junho (OpenAI) | 0 | 0 | $0.0000 |
| Orchestrator (OpenAI) | 4,962 | 599 | $0.0011 |
| **Total** | - | - | **$0.0011** (≈2원) |
