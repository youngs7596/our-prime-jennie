# Session Handoff: vLLM max-model-len 확장 & 토큰 초과 통계 추가

**일시**: 2026-02-06 15:38 KST
**브랜치**: development

## 요약 (Summary)
vLLM 로컬 LLM 호출이 500 에러로 실패하는 원인을 분석하여 수정.
`max-model-len=4096`이 너무 작아서 입력+출력 토큰이 한도를 초과하는 문제였음.
vLLM 서버를 `max-model-len=8192`로 재시작하고, ollama-gateway에 max_tokens 클램핑 및
토큰 초과 통계 기능을 추가하여 모니터링 가능하게 함.

## 변경 사항 (Changes)

### vLLM 서버 설정 변경 (컨테이너 재생성)
- `vllm-exaone4-test`: `--max-model-len 4096 → 8192`, `--gpu-memory-utilization 0.82 → 0.88`
- GPU VRAM 사용량: ~23.3GB / 24.6GB (여유 ~1.3GB)

### ollama-gateway 수정 (`services/ollama-gateway/main.py`)
- `VLLM_MAX_MODEL_LEN` 환경변수 추가 (기본값 8192)
- `_call_vllm_llm()`: max_tokens 클램핑 (max_model_len // 2 초과 시 자동 축소)
- `_call_vllm_llm()`: vLLM 400 에러 시 토큰 초과 여부 감지 및 통계 기록
- `stats` 딕셔너리: `token_clamped_count`, `token_exceeded_count`, `token_exceeded_history` 추가
- `/stats` 엔드포인트: `token_stats` 섹션 노출

## 검증 결과
- `num_predict=4096` 요청 → 클램핑 후 정상 응답 (기존: 500 에러)
- `num_predict=8000` 요청 → 클램핑 → 정상 응답, `clamped_count` 증가 확인
- 뉴스 감성 분석 서비스: 요청당 ~5초, 에러 없이 정상 동작
- `/stats` 엔드포인트에서 `token_stats` 정상 노출 확인

## 다음 단계 (Next Steps)
- [ ] `token_exceeded_count` 모니터링 → 축적되면 Map-Reduce 청킹 패턴 도입 검토
- [ ] vLLM OOM 모니터링 (gpu-memory-utilization=0.88이 한계치에 가까움)
- [ ] vLLM 컨테이너를 docker-compose로 통합 관리 (현재 수동 `docker run`)

## Context for Next Session
- `services/ollama-gateway/main.py`: VLLM_MAX_MODEL_LEN, 클램핑 로직, token_stats
- vLLM 실행 커맨드: `docker run --gpus all -v ~/.cache/huggingface:/root/.cache/huggingface -p 8001:8000 vllm/vllm-openai:latest --model LGAI-EXAONE/EXAONE-4.0-32B-AWQ --quantization awq_marlin --dtype half --max-model-len 8192 --gpu-memory-utilization 0.88 --trust-remote-code`
- 토큰 통계 확인: `curl localhost:11500/stats | jq .token_stats`
