# Session: Ollama → vLLM 마이그레이션 구현

**날짜**: 2026-02-06 14:00 ~ 14:45
**브랜치**: development
**커밋**: `15c5dc0` - Ollama → vLLM 마이그레이션: Gateway 레벨 투명 변환

## 작업 내용

### 핵심: ollama-gateway에 vLLM 백엔드 지원 추가

기존 Ollama 기반 LLM 서빙을 vLLM(EXAONE 4.0 32B AWQ + KURE-v1)으로 전환.
**Gateway 레벨 변환** 전략으로 서비스 코드(scout-job, buy-scanner 등) 변경 없이 구현.

```
서비스 → [Ollama API] → ollama-gateway → [OpenAI API] → vLLM
```

### 변경 파일

| 파일 | 변경 내용 |
|------|----------|
| `services/ollama-gateway/main.py` | BACKEND_MODE, vLLM 변환 로직, 헬스체크 |
| `docker-compose.yml` | vllm-llm/vllm-embed 서비스 추가, rate limit 완화 |
| `tests/services/ollama-gateway/test_vllm_backend.py` | 27개 테스트 (신규) |
| `tests/services/ollama-gateway/__init__.py` | 패키지 init (신규) |

### Gateway 변환 로직

- `BACKEND_MODE` 환경변수: `"vllm"` | `"ollama"` (기본 ollama)
- `VLLM_MODEL_MAP`: Ollama 모델명 → HuggingFace 모델명 매핑
  - `exaone3.5:7.8b` → `LGAI-EXAONE/EXAONE-4.0-32B-AWQ`
  - `daynice/kure-v1` → `nlpai-lab/KURE-v1`
- `/api/generate` → `/v1/chat/completions` (응답을 Ollama 형식으로 재변환)
- `/api/chat` → `/v1/chat/completions`
- `/api/embed` → `/v1/embeddings`
- `format: "json"` → `response_format: {"type": "json_object"}`
- Cloud Proxy (`:cloud` 접미사)는 vLLM 우회하여 기존대로 작동

### Docker Compose 변경

- `vllm-llm`: EXAONE 4.0 32B AWQ (port 8001, profiles: gpu)
- `vllm-embed`: KURE-v1 (port 8002, profiles: gpu)
- `ollama`: profiles `["gpu", "infra"]` → `["gpu-legacy"]` (롤백용)
- Rate limit 5x 완화 (vLLM continuous batching 대응)
  - 60/min → 300/min, 120/min → 600/min, embed 3000 → 6000/min
  - MAX_CONCURRENT: 3 → 10

### 트러블슈팅

1. 처음 배포 시 Circuit Breaker OPEN 발생
   - 원인: gateway가 old 이미지(vLLM 미지원)로 실행 + ollama 중단 상태
   - 해결: gateway 리빌드 (`docker compose --profile real up -d --build ollama-gateway`)

### 검증 결과

- vLLM generate: `대한민국의 수도는 서울특별시입니다.` (EXAONE 4.0 32B)
- vLLM embed: 1024차원 벡터 정상 반환 (KURE-v1)
- Gateway health: `{"backend_mode": "vllm", "status": "healthy", "circuit_breaker": "closed"}`
- 기존 테스트: 999 passed, 1 skipped (shared)
- 신규 테스트: 27 passed (ollama-gateway vLLM)

## 롤백 방법

```bash
# 1. docker-compose.yml에서 BACKEND_MODE=ollama로 변경
# 2. ollama 컨테이너 시작
docker compose -p my-prime-jennie --profile gpu-legacy up -d ollama
# 3. gateway 리빌드
docker compose -p my-prime-jennie --profile real up -d --build ollama-gateway
```

## 현재 시스템 상태

- **vLLM LLM**: `vllm-exaone4-test` (port 8001) - 수동 실행 중
- **vLLM Embed**: `vllm-kure-test` (port 8002) - 수동 실행 중
- **ollama-gateway**: BACKEND_MODE=vllm, healthy
- **ollama**: 컨테이너 제거됨 (gpu-legacy 프로필로 필요시 재시작 가능)

## 미커밋 변경 (이전 세션)

- `shared/llm_providers.py` - CloudFailoverProvider
- `shared/llm_factory.py` - deepseek_cloud provider 지원
- `shared/auth.py` - API key 매핑
- `tests/shared/test_cloud_failover_provider.py` - CloudFailover 테스트
- `scripts/benchmark_*.py` - 벤치마크 스크립트
- `scripts/poc_cloud_failover.py` - PoC 스크립트

## 다음 세션 TODO

- [ ] vLLM 컨테이너를 docker-compose 관리로 전환 (현재 수동 실행)
- [ ] CloudFailoverProvider 관련 미커밋 파일 정리/커밋
- [ ] vLLM 성능 모니터링 (gateway `/stats` 활용)
- [ ] 필요시 vllm-llm/vllm-embed GPU 메모리 설정 최적화
