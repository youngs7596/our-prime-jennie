#!/usr/bin/env python
"""
test_judge_logic.py - Scout Pipeline V2 Judge ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸

ì£¼ìš” ê²€ì¦ í•­ëª©:
1. Judge Loginc: ì´ì¤‘ ê°ì  ë°©ì§€ (RSI 85 ì‹œë‚˜ë¦¬ì˜¤)
2. Grade Calculation: ì½”ë“œ ê¸°ë°˜ ë“±ê¸‰ ì‚°ì • ê²€ì¦
3. Logging: ì‹¤ì œ ëª¨ë¸ëª…(target_model) ë¡œê¹… í™•ì¸
"""

import os
import sys
import logging
import json
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ PYTHONPATHì— ì¶”ê°€
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, PROJECT_ROOT)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# [ê²€ì¦ ëª¨ë“œ] LLM ë¡œê·¸ ë³„ë„ íŒŒì¼ë¡œ ì €ì¥
TEST_LOG_PATH = "/app/logs/test_judge_logs.jsonl"
os.environ["LLM_DEBUG_LOG_PATH"] = TEST_LOG_PATH
os.environ["USE_OLLAMA_GATEWAY"] = "true"
os.environ["OLLAMA_GATEWAY_URL"] = "http://localhost:11500"

def test_judge_double_penalty():
    print("\n" + "="*60)
    print("ğŸ§ª Test: Judge ì´ì¤‘ ê°ì  ë°©ì§€ í…ŒìŠ¤íŠ¸")
    print("="*60)
    
    try:
        from shared.llm import JennieBrain
        
        brain = JennieBrain()
        
        # [ì‹œë‚˜ë¦¬ì˜¤] RSIê°€ 85ë¡œ ë§¤ìš° ë†’ì•„ ì •ëŸ‰ ì ìˆ˜ê°€ 55ì ìœ¼ë¡œ ê¹ì¸ ìƒíƒœ
        # Judgeê°€ ì´ë¥¼ ë³´ê³  ë˜ ê°ì í•˜ëŠ”ì§€ í™•ì¸
        stock_info = {
            "code": "005930",
            "name": "í…ŒìŠ¤íŠ¸ì „ì",
            "hunter_score": 75.0, # Gatekeeper Pass (>=70)
            "per": 12.5,
            "pbr": 1.2,
            "news_reason": "ìµœê·¼ AI ë°˜ë„ì²´ ìˆ˜ì£¼ ê¸°ëŒ€ê° ìˆìœ¼ë‚˜ ë‹¨ê¸° ê¸‰ë“± ë¶€ë‹´.",
            "dominant_keywords": ["ë°˜ë„ì²´", "AI"]
        }
        
        quant_context = """
        [ì •ëŸ‰ ë¶„ì„ ê²°ê³¼]
        - ì¢…í•© ì ìˆ˜: 75ì  (ë§¤ìˆ˜)
        - RSI: 85.0 (ë§¤ìš° ë†’ìŒ, ê³¼ì—´ê¶Œ ì§„ì…ì´ì§€ë§Œ ìˆ˜ê¸‰ì´ ê°•ë ¥í•¨)
        - ê¸°ìˆ ì  ì§€í‘œê°€ ê³¼ì—´ì„ ê°€ë¦¬í‚¤ê³  ìˆìœ¼ë‚˜ ì•„ì§ ì¶”ì„¸ëŠ” ì‚´ì•„ìˆìŒ.
        """
        
        debate_log = """
        ì¤€í˜¸: ì•¼ ë¯¼ì§€ì•¼, ì´ê±° RSI 85ì•¼. ë¯¸ì¹œ ê±° ì•„ë‹ˆì•¼? ë‹¹ì¥ íŒ”ì•„ì•¼ ë¼. ê³¼ì—´ì´ë¼ê³ ! ê°ì í•´! [ì •ëŸ‰]
        ë¯¼ì§€: íŒ€ì¥ë‹˜, RSI ë†’ì€ ê±´ ë§ì§€ë§Œ ìˆ˜ì£¼ ëª¨ë©˜í…€ì´ ì‚´ì•„ìˆì–ì•„ìš”. [ë‰´ìŠ¤] ê¸°ê³„ì ìœ¼ë¡œ íŒ” í•„ìš”ëŠ” ì—†ì–´ìš”.
        ì¤€í˜¸: ì•„ë‹ˆì•¼, í†µê³„ì ìœ¼ë¡œ RSI 80 ë„˜ìœ¼ë©´ ë¬´ì¡°ê±´ ì¡°ì •ì„ ë°›ì•˜ì–´. ì´ê±´ ìœ„í—˜í•´. [í†µê³„]
        """
        
        logger.info(f"Input Hunter Score: {stock_info['hunter_score']}")
        logger.info("Running Judge Scoring V5...")
        
        result = brain.run_judge_scoring_v5(stock_info, debate_log, quant_context)
        
        logger.info(f"âœ… Judge Result: {json.dumps(result, ensure_ascii=False, indent=2)}")
        
        score = float(result.get('score', 0))
        grade = result.get('grade', 'F')
        reason = result.get('reason', '')
        
        # [ê²€ì¦]
        # 1. ì ìˆ˜ê°€ 55ì ì—ì„œ í¬ê²Œ ê¹ì´ì§€ ì•Šì•˜ëŠ”ì§€ (-10ì  ì´ë‚´)
        if score < 45:
             logger.error(f"âŒ ì´ì¤‘ ê°ì  ì˜ì‹¬! Score: {stock_info['hunter_score']} -> {score}")
        else:
             logger.info(f"âœ… ì ìˆ˜ ë°©ì–´ ì„±ê³µ: {stock_info['hunter_score']} -> {score}")
             
        # 2. ë“±ê¸‰ì´ ì½”ë“œë¡œ ê³„ì‚°ë˜ì—ˆëŠ”ì§€ (LLM ì˜ì¡´ X)
        # 55ì ì´ë©´ Cë“±ê¸‰ì´ì–´ì•¼ í•¨ (build_judge_promptì˜ ì˜ˆì „ ê¸°ì¤€ì´ ì•„ë‹Œ ì½”ë“œ ë¡œì§ ê¸°ì¤€)
        # _calculate_grade: >=50 is C
        expected_grade = 'C'
        if grade == expected_grade:
            logger.info(f"âœ… ë“±ê¸‰ ì‚°ì • ì •í™•í•¨: {grade}")
        else:
            logger.error(f"âŒ ë“±ê¸‰ ì‚°ì • ì˜¤ë¥˜: Expected {expected_grade}, Got {grade}")

        # 3. ëª¨ë¸ëª… ë¡œê¹… í™•ì¸
        check_log_file()

    except Exception as e:
        logger.error(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}", exc_info=True)

def check_log_file():
    print("\n" + "-"*60)
    print("ğŸ“ Log File Check")
    print("-"*(60))
    
    if not os.path.exists(TEST_LOG_PATH):
        logger.error("âŒ ë¡œê·¸ íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•ŠìŒ!")
        return
        
    with open(TEST_LOG_PATH, 'r', encoding='utf-8') as f:
        lines = f.readlines()
        
    if not lines:
        logger.error("âŒ ë¡œê·¸ íŒŒì¼ì´ ë¹„ì–´ìˆìŒ!")
        return

    last_log = json.loads(lines[-1])
    logger.info(f"Last Log Entry Type: {last_log.get('type')}")
    logger.info(f"Model Name Logged: {last_log.get('model')}")
    
    # Verify Model Name
    expected_model = "gpt-oss" # llm_factory settings might point to gpt-oss which maps to qwen3:32b?
    # Actually OllamaLLMProvider should log 'qwen3:32b' if that's what's passed, or 'gpt-oss' if it's the alias.
    # In V2 we fixed it to use 'target_model'. Let's see what it logs.
    
    if "qwen" in str(last_log.get('model')).lower() or "gpt-oss" in str(last_log.get('model')).lower():
         logger.info("âœ… ëª¨ë¸ëª… ë¡œê¹… í™•ì¸ë¨.")
    else:
         logger.warning(f"âš ï¸ ëª¨ë¸ëª…ì´ ì˜ˆìƒê³¼ ë‹¤ë¦„: {last_log.get('model')}")

if __name__ == "__main__":
    test_judge_double_penalty()
