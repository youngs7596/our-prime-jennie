#!/usr/bin/env python
"""
test_debate_issue.py - Bull vs Bear í† ë¡  ë¹ˆ ë¡œê·¸ ë¬¸ì œ ë””ë²„ê¹…

Scout Judge ë‹¨ê³„ì—ì„œ "Bull vs Bear í† ë¡  ë¡œê·¸ê°€ ë¹„ì–´ ìˆì–´" ë¬¸ì œ ì¬í˜„ ë° ë””ë²„ê¹…
"""

import os
import sys
import logging
import json
import requests

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ PYTHONPATHì— ì¶”ê°€
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, PROJECT_ROOT)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def test_gateway_chat_direct():
    """
    1. Ollama Gateway /api/chat ì—”ë“œí¬ì¸íŠ¸ ì§ì ‘ í…ŒìŠ¤íŠ¸
    """
    print("\n" + "="*60)
    print("ğŸ§ª Test 1: Ollama Gateway /api/chat ì§ì ‘ í˜¸ì¶œ")
    print("="*60)
    
    gateway_url = os.getenv("OLLAMA_GATEWAY_URL", "http://localhost:11500")
    
    payload = {
        "model": "qwen3:32b",
        "messages": [
            {"role": "user", "content": "ê°„ë‹¨íˆ 'ì•ˆë…•í•˜ì„¸ìš”'ë¼ê³ ë§Œ ë‹µí•´ì£¼ì„¸ìš”."}
        ],
        "options": {
            "temperature": 0.7,
            "num_ctx": 8192
        }
    }
    
    try:
        logger.info(f"Gateway URL: {gateway_url}/api/chat")
        logger.info(f"Payload: {json.dumps(payload, ensure_ascii=False, indent=2)}")
        
        response = requests.post(
            f"{gateway_url}/api/chat",
            json=payload,
            timeout=120
        )
        
        logger.info(f"Status Code: {response.status_code}")
        logger.info(f"Response Headers: {dict(response.headers)}")
        
        result = response.json()
        logger.info(f"Response JSON: {json.dumps(result, ensure_ascii=False, indent=2)}")
        
        # ì‘ë‹µ êµ¬ì¡° í™•ì¸
        if "message" in result:
            content = result["message"].get("content", "")
            logger.info(f"âœ… message.content: '{content[:200]}...' (len={len(content)})")
        else:
            logger.warning(f"âš ï¸ 'message' í‚¤ê°€ ì—†ìŒ! Keys: {result.keys()}")
        
        return result
        
    except Exception as e:
        logger.error(f"âŒ Gateway í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        return None


def test_ollama_chat_direct():
    """
    2. Ollama ì„œë²„ /api/chat ì§ì ‘ í…ŒìŠ¤íŠ¸ (Gateway ìš°íšŒ)
    """
    print("\n" + "="*60)
    print("ğŸ§ª Test 2: Ollama ì„œë²„ /api/chat ì§ì ‘ í˜¸ì¶œ (Gateway ìš°íšŒ)")
    print("="*60)
    
    ollama_host = os.getenv("OLLAMA_HOST", "http://localhost:11434")
    
    payload = {
        "model": "qwen3:32b",
        "messages": [
            {"role": "user", "content": "ê°„ë‹¨íˆ 'ì•ˆë…•í•˜ì„¸ìš”'ë¼ê³ ë§Œ ë‹µí•´ì£¼ì„¸ìš”."}
        ],
        "stream": False,
        "keep_alive": -1,
        "options": {
            "temperature": 0.7,
            "num_ctx": 8192
        }
    }
    
    try:
        logger.info(f"Ollama URL: {ollama_host}/api/chat")
        
        response = requests.post(
            f"{ollama_host}/api/chat",
            json=payload,
            timeout=120
        )
        
        logger.info(f"Status Code: {response.status_code}")
        
        result = response.json()
        logger.info(f"Response JSON: {json.dumps(result, ensure_ascii=False, indent=2)}")
        
        if "message" in result:
            content = result["message"].get("content", "")
            logger.info(f"âœ… message.content: '{content[:200]}...' (len={len(content)})")
        
        return result
        
    except Exception as e:
        logger.error(f"âŒ Ollama ì§ì ‘ í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        return None


def test_debate_session_via_llm():
    """
    3. JennieBrain.run_debate_session() ì§ì ‘ í…ŒìŠ¤íŠ¸
    """
    print("\n" + "="*60)
    print("ğŸ§ª Test 3: JennieBrain.run_debate_session() í…ŒìŠ¤íŠ¸")
    print("="*60)
    
    # í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (Gateway ëª¨ë“œ)
    os.environ["USE_OLLAMA_GATEWAY"] = "true"
    os.environ["OLLAMA_GATEWAY_URL"] = "http://localhost:11500"
    
    try:
        from shared.llm import JennieBrain
        
        brain = JennieBrain()
        
        # í…ŒìŠ¤íŠ¸ìš© stock_info
        stock_info = {
            "code": "005930",
            "name": "ì‚¼ì„±ì „ì",
            "sector": "ë°˜ë„ì²´",
            "current_price": 55000,
            "hunter_score": 85.0,
            "quant_score": 60.0,
            "dominant_keywords": ["AI", "ë°˜ë„ì²´"]
        }
        
        logger.info(f"Input stock_info: {stock_info}")
        
        result = brain.run_debate_session(
            stock_info=stock_info,
            hunter_score=85
        )
        
        logger.info(f"Debate Result Type: {type(result)}")
        logger.info(f"Debate Result Length: {len(result) if result else 0}")
        logger.info(f"Debate Result: {result[:500] if result else 'EMPTY'}...")
        
        if not result or result.strip() == "" or "Error" in result:
            logger.warning("âš ï¸ í† ë¡  ê²°ê³¼ê°€ ë¹„ì–´ìˆê±°ë‚˜ ì—ëŸ¬ ë°œìƒ!")
        else:
            logger.info("âœ… í† ë¡  ê²°ê³¼ ì •ìƒ ìƒì„±")
        
        return result
        
    except Exception as e:
        logger.error(f"âŒ Debate ì„¸ì…˜ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}", exc_info=True)
        return None


def test_llm_provider_generate_chat():
    """
    4. OllamaLLMProvider.generate_chat() ì§ì ‘ í…ŒìŠ¤íŠ¸
    """
    print("\n" + "="*60)
    print("ğŸ§ª Test 4: OllamaLLMProvider.generate_chat() ì§ì ‘ í…ŒìŠ¤íŠ¸")
    print("="*60)
    
    os.environ["USE_OLLAMA_GATEWAY"] = "true"
    os.environ["OLLAMA_GATEWAY_URL"] = "http://localhost:11500"
    
    try:
        from shared.llm_factory import OllamaStateManager
        from shared.llm_providers import OllamaLLMProvider
        
        state_manager = OllamaStateManager()
        provider = OllamaLLMProvider(
            model="qwen3:32b",
            state_manager=state_manager,
            is_fast_tier=False,
            is_thinking_tier=False
        )
        
        chat_history = [
            {"role": "user", "content": "ê°„ë‹¨íˆ 'ì•ˆë…•í•˜ì„¸ìš”'ë¼ê³ ë§Œ ë‹µí•´ì£¼ì„¸ìš”."}
        ]
        
        logger.info(f"Chat History: {chat_history}")
        
        result = provider.generate_chat(chat_history, temperature=0.7)
        
        logger.info(f"Result Type: {type(result)}")
        logger.info(f"Result: {result}")
        
        if isinstance(result, dict):
            text = result.get("text", "")
            logger.info(f"Text Length: {len(text)}")
            if not text.strip():
                logger.warning("âš ï¸ textê°€ ë¹„ì–´ìˆìŒ!")
            else:
                logger.info(f"âœ… Response: {text[:200]}...")
        
        return result
        
    except Exception as e:
        logger.error(f"âŒ generate_chat í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}", exc_info=True)
        return None


if __name__ == "__main__":
    print("="*60)
    print("ğŸ” Bull vs Bear í† ë¡  ë¹ˆ ë¡œê·¸ ë¬¸ì œ ë””ë²„ê¹…")
    print("="*60)
    
    # Test 1: Gateway ì§ì ‘ í˜¸ì¶œ
    result1 = test_gateway_chat_direct()
    
    # Test 2: Ollama ì„œë²„ ì§ì ‘ í˜¸ì¶œ
    result2 = test_ollama_chat_direct()
    
    # Test 3: JennieBrain í…ŒìŠ¤íŠ¸
    result3 = test_debate_session_via_llm()
    
    # Test 4: Provider í…ŒìŠ¤íŠ¸
    result4 = test_llm_provider_generate_chat()
    
    print("\n" + "="*60)
    print("ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    print("="*60)
    print(f"Test 1 (Gateway /api/chat): {'âœ… PASS' if result1 else 'âŒ FAIL'}")
    print(f"Test 2 (Ollama Direct):     {'âœ… PASS' if result2 else 'âŒ FAIL'}")
    print(f"Test 3 (JennieBrain):       {'âœ… PASS' if result3 and len(str(result3)) > 20 else 'âŒ FAIL'}")
    print(f"Test 4 (Provider):          {'âœ… PASS' if result4 else 'âŒ FAIL'}")
