#!/usr/bin/env python3
"""
scripts/test_hunter_ollama.py

Hunter ë‹¨ê³„ Ollama ëª¨ë¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
- ëª¨ë¸ë³„ ì‘ë‹µ ì‹œê°„ ì¸¡ì •
- ë³‘ë ¬ ì²˜ë¦¬ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸
- JSON íŒŒì‹± ì„±ê³µë¥  í™•ì¸

ì‚¬ìš©ë²•:
    python scripts/test_hunter_ollama.py --model gemma3:27b --workers 4 --count 5
"""

import os
import sys
import time
import json
import argparse
import logging
import pytest

# CI í™˜ê²½ì—ì„œëŠ” ì™¸ë¶€ Ollama/gateway ì˜ì¡´ì„±ì´ ì—†ìœ¼ë¯€ë¡œ ìŠ¤í‚µ
pytest.skip("Requires external Ollama/gateway; skip in CI", allow_module_level=True)
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Tuple

# Add project root to path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%H:%M:%S"
)
logger = logging.getLogger("Hunter-Test")

# í…ŒìŠ¤íŠ¸ìš© ì¢…ëª© ë°ì´í„° (ì‹¤ì œ Hunterì—ì„œ ì‚¬ìš©í•˜ëŠ” í˜•íƒœ)
TEST_STOCKS = [
    {"code": "005930", "name": "ì‚¼ì„±ì „ì", "per": 12.3, "pbr": 1.05, "news": "ë°˜ë„ì²´ ì—…í™© ê°œì„  ê¸°ëŒ€"},
    {"code": "000660", "name": "SKí•˜ì´ë‹‰ìŠ¤", "per": 8.5, "pbr": 1.32, "news": "HBM ìˆ˜ìš” ê¸‰ì¦"},
    {"code": "035420", "name": "NAVER", "per": 25.1, "pbr": 1.45, "news": "AI ì„œë¹„ìŠ¤ í™•ëŒ€"},
    {"code": "051910", "name": "LGí™”í•™", "per": 15.2, "pbr": 0.95, "news": "ë°°í„°ë¦¬ ì‚¬ì—… ë¶„ë¦¬ ê²€í† "},
    {"code": "006400", "name": "ì‚¼ì„±SDI", "per": 18.3, "pbr": 1.12, "news": "ì „ê³ ì²´ ë°°í„°ë¦¬ ê°œë°œ ì§„ì²™"},
    {"code": "035720", "name": "ì¹´ì¹´ì˜¤", "per": 45.2, "pbr": 2.15, "news": "ê´‘ê³  ë§¤ì¶œ íšŒë³µ"},
    {"code": "105560", "name": "KBê¸ˆìœµ", "per": 5.8, "pbr": 0.45, "news": "NIM ê°œì„  ê¸°ëŒ€"},
    {"code": "055550", "name": "ì‹ í•œì§€ì£¼", "per": 5.2, "pbr": 0.42, "news": "ë°°ë‹¹ í™•ëŒ€ ë°œí‘œ"},
]

# Hunter í”„ë¡¬í”„íŠ¸ (ê°„ì†Œí™” ë²„ì „)
HUNTER_PROMPT_TEMPLATE = """ë‹¹ì‹ ì€ ë°ì´í„° ê¸°ë°˜ ì£¼ì‹ ë¶„ì„ AIì…ë‹ˆë‹¤.

## ì¢…ëª© ì •ë³´
ì¢…ëª©: {name} ({code})

## ì •ëŸ‰ ì§€í‘œ
- PER: {per}
- PBR: {pbr}

## ìµœì‹  ë‰´ìŠ¤
{news}

## í‰ê°€ ê¸°ì¤€
- 80ì  ì´ìƒ: ê°•ë ¥ ì¶”ì²œ
- 60-79ì : ì¶”ì²œ
- 40-59ì : ì¤‘ë¦½
- 40ì  ë¯¸ë§Œ: ë¹„ì¶”ì²œ

JSON ì‘ë‹µ: {{"score": ìˆ«ì, "grade": "ë“±ê¸‰(S/A/B/C/D)", "reason": "íŒë‹¨ ì´ìœ (í•œêµ­ì–´ 20ì ì´ë‚´)"}}

âš ï¸ ë°˜ë“œì‹œ ìœ„ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.
"""

def create_ollama_provider(model_name: str):
    """Ollama Provider ìƒì„±"""
    from shared.llm_providers import OllamaLLMProvider
    from shared.llm_factory import ModelStateManager
    
    return OllamaLLMProvider(
        model=model_name,
        state_manager=ModelStateManager(),
        is_fast_tier=False,
        is_thinking_tier=False
    )


def test_single_stock(provider, stock: Dict, test_id: int) -> Tuple[bool, float, Dict]:
    """
    ë‹¨ì¼ ì¢…ëª© Hunter ë¶„ì„ í…ŒìŠ¤íŠ¸
    
    Returns:
        (success, elapsed_time, result)
    """
    prompt = HUNTER_PROMPT_TEMPLATE.format(
        name=stock["name"],
        code=stock["code"],
        per=stock["per"],
        pbr=stock["pbr"],
        news=stock["news"]
    )
    
    response_schema = {
        "type": "object",
        "properties": {
            "score": {"type": "number"},
            "grade": {"type": "string"},
            "reason": {"type": "string"}
        },
        "required": ["score", "grade", "reason"]
    }
    
    start_time = time.time()
    
    try:
        result = provider.generate_json(
            prompt=prompt,
            response_schema=response_schema,
            temperature=0.2
        )
        elapsed = time.time() - start_time
        
        # ê²°ê³¼ ê²€ì¦
        if "score" in result and "grade" in result:
            logger.info(f"   âœ… [{test_id}] {stock['name']}: {result['score']}ì  ({result['grade']}) - {elapsed:.1f}ì´ˆ")
            return True, elapsed, result
        else:
            logger.warning(f"   âš ï¸ [{test_id}] {stock['name']}: ë¶ˆì™„ì „í•œ ì‘ë‹µ - {elapsed:.1f}ì´ˆ")
            return False, elapsed, result
            
    except Exception as e:
        elapsed = time.time() - start_time
        logger.error(f"   âŒ [{test_id}] {stock['name']}: {str(e)[:50]} - {elapsed:.1f}ì´ˆ")
        return False, elapsed, {"error": str(e)}


def run_sequential_test(model_name: str, count: int) -> Dict:
    """ìˆœì°¨ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
    logger.info(f"\n{'='*60}")
    logger.info(f"ğŸ”„ ìˆœì°¨ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ (model: {model_name}, count: {count})")
    logger.info(f"{'='*60}")
    
    provider = create_ollama_provider(model_name)
    
    results = []
    total_start = time.time()
    
    for i in range(count):
        stock = TEST_STOCKS[i % len(TEST_STOCKS)]
        success, elapsed, result = test_single_stock(provider, stock, i + 1)
        results.append({"success": success, "elapsed": elapsed, "result": result})
    
    total_elapsed = time.time() - total_start
    
    success_count = sum(1 for r in results if r["success"])
    avg_time = sum(r["elapsed"] for r in results) / len(results) if results else 0
    
    return {
        "mode": "sequential",
        "model": model_name,
        "count": count,
        "success": success_count,
        "failed": count - success_count,
        "success_rate": f"{success_count/count*100:.1f}%",
        "total_time": f"{total_elapsed:.1f}s",
        "avg_time": f"{avg_time:.1f}s"
    }


def run_parallel_test(model_name: str, count: int, workers: int) -> Dict:
    """ë³‘ë ¬ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
    logger.info(f"\n{'='*60}")
    logger.info(f"âš¡ ë³‘ë ¬ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ (model: {model_name}, count: {count}, workers: {workers})")
    logger.info(f"{'='*60}")
    
    provider = create_ollama_provider(model_name)
    
    results = []
    total_start = time.time()
    
    with ThreadPoolExecutor(max_workers=workers) as executor:
        futures = {}
        for i in range(count):
            stock = TEST_STOCKS[i % len(TEST_STOCKS)]
            future = executor.submit(test_single_stock, provider, stock, i + 1)
            futures[future] = i
        
        for future in as_completed(futures):
            try:
                success, elapsed, result = future.result()
                results.append({"success": success, "elapsed": elapsed, "result": result})
            except Exception as e:
                logger.error(f"   âŒ Thread error: {e}")
                results.append({"success": False, "elapsed": 0, "result": {"error": str(e)}})
    
    total_elapsed = time.time() - total_start
    
    success_count = sum(1 for r in results if r["success"])
    avg_time = sum(r["elapsed"] for r in results) / len(results) if results else 0
    
    return {
        "mode": f"parallel (workers={workers})",
        "model": model_name,
        "count": count,
        "success": success_count,
        "failed": count - success_count,
        "success_rate": f"{success_count/count*100:.1f}%",
        "total_time": f"{total_elapsed:.1f}s",
        "avg_time": f"{avg_time:.1f}s",
        "throughput": f"{count/total_elapsed:.2f} req/s"
    }


def main():
    parser = argparse.ArgumentParser(description="Hunter Ollama ëª¨ë¸ í…ŒìŠ¤íŠ¸")
    parser.add_argument("--model", type=str, default="gemma3:27b", help="í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ëª…")
    parser.add_argument("--workers", type=int, default=4, help="ë³‘ë ¬ ì²˜ë¦¬ worker ìˆ˜")
    parser.add_argument("--count", type=int, default=5, help="í…ŒìŠ¤íŠ¸í•  ì¢…ëª© ìˆ˜")
    parser.add_argument("--sequential-only", action="store_true", help="ìˆœì°¨ ì²˜ë¦¬ë§Œ í…ŒìŠ¤íŠ¸")
    parser.add_argument("--parallel-only", action="store_true", help="ë³‘ë ¬ ì²˜ë¦¬ë§Œ í…ŒìŠ¤íŠ¸")
    
    args = parser.parse_args()
    
    logger.info(f"ğŸš€ Hunter Ollama í…ŒìŠ¤íŠ¸ ì‹œì‘")
    logger.info(f"   ëª¨ë¸: {args.model}")
    logger.info(f"   ì¢…ëª© ìˆ˜: {args.count}")
    logger.info(f"   ë³‘ë ¬ Workers: {args.workers}")
    
    results = []
    
    # ìˆœì°¨ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
    if not args.parallel_only:
        seq_result = run_sequential_test(args.model, args.count)
        results.append(seq_result)
    
    # ë³‘ë ¬ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
    if not args.sequential_only:
        par_result = run_parallel_test(args.model, args.count, args.workers)
        results.append(par_result)
    
    # ê²°ê³¼ ìš”ì•½
    logger.info(f"\n{'='*60}")
    logger.info(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    logger.info(f"{'='*60}")
    
    for r in results:
        logger.info(f"\n[{r['mode']}]")
        logger.info(f"   ëª¨ë¸: {r['model']}")
        logger.info(f"   ì„±ê³µ/ì‹¤íŒ¨: {r['success']}/{r['failed']} ({r['success_rate']})")
        logger.info(f"   ì´ ì†Œìš”ì‹œê°„: {r['total_time']}")
        logger.info(f"   í‰ê·  ì‘ë‹µì‹œê°„: {r['avg_time']}")
        if "throughput" in r:
            logger.info(f"   ì²˜ë¦¬ëŸ‰: {r['throughput']}")
    
    logger.info(f"\n{'='*60}")
    
    # ëª¨ë“  í…ŒìŠ¤íŠ¸ ì„±ê³µ ì—¬ë¶€
    all_success = all(r["failed"] == 0 for r in results)
    if all_success:
        logger.info("ğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!")
        return 0
    else:
        logger.warning("âš ï¸ ì¼ë¶€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
        return 1


if __name__ == "__main__":
    sys.exit(main())

