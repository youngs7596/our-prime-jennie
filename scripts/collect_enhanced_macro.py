#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
scripts/collect_enhanced_macro.py
---------------------------------
Enhanced Macro Data ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸.

ì—¬ëŸ¬ ì†ŒìŠ¤ì—ì„œ ê¸€ë¡œë²Œ ë§¤í¬ë¡œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³ 
GlobalMacroSnapshotìœ¼ë¡œ í†µí•©í•©ë‹ˆë‹¤.

Usage:
    python scripts/collect_enhanced_macro.py              # ì „ì²´ ìˆ˜ì§‘
    python scripts/collect_enhanced_macro.py --quick      # ë¹ ë¥¸ ìˆ˜ì§‘ (í•µì‹¬ ì§€í‘œë§Œ)
    python scripts/collect_enhanced_macro.py --dry-run    # ìˆ˜ì§‘ë§Œ (ì €ì¥ ì•ˆí•¨)
    python scripts/collect_enhanced_macro.py --sources finnhub,fred  # íŠ¹ì • ì†ŒìŠ¤ë§Œ
"""

import argparse
import asyncio
import json
import logging
import os
import sys
from datetime import datetime
from pathlib import Path
from typing import List, Optional

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì„¤ì •
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from zoneinfo import ZoneInfo

KST = ZoneInfo("Asia/Seoul")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
)
logger = logging.getLogger("EnhancedMacroCollector")


async def collect_macro_data(
    sources: Optional[List[str]] = None,
    quick: bool = False,
    dry_run: bool = False,
) -> int:
    """
    ë§¤í¬ë¡œ ë°ì´í„° ìˆ˜ì§‘.

    Args:
        sources: ìˆ˜ì§‘í•  ì†ŒìŠ¤ ëª©ë¡ (Noneì´ë©´ ì „ì²´)
        quick: ë¹ ë¥¸ ìˆ˜ì§‘ ëª¨ë“œ (í•µì‹¬ ì§€í‘œë§Œ)
        dry_run: Trueë©´ ì €ì¥ ì•ˆí•¨

    Returns:
        0: ì„±ê³µ, 1: ì‹¤íŒ¨
    """
    from shared.macro_data import (
        EnhancedMacroAggregator,
        AggregatorConfig,
        save_snapshot_to_db,
        get_macro_cache,
    )

    logger.info("=" * 60)
    logger.info("ğŸŒ Enhanced Macro Data Collection ì‹œì‘")
    logger.info("=" * 60)

    # Aggregator ì„¤ì •
    config = AggregatorConfig(
        enable_finnhub=sources is None or "finnhub" in sources,
        enable_fred=sources is None or "fred" in sources,
        enable_bok_ecos=sources is None or "bok_ecos" in sources,
        enable_pykrx=sources is None or "pykrx" in sources,
        enable_rss=sources is None or "rss" in sources,
        allow_partial_failure=True,
        use_cache=not dry_run,
    )

    if quick:
        # ë¹ ë¥¸ ìˆ˜ì§‘: pykrx + rssë§Œ (API í‚¤ ë¶ˆí•„ìš”)
        config.enable_finnhub = False
        config.enable_fred = False
        config.enable_bok_ecos = False
        logger.info("âš¡ Quick ëª¨ë“œ: pykrx, rssë§Œ ìˆ˜ì§‘")

    # Aggregator ìƒì„±
    aggregator = EnhancedMacroAggregator(config)

    try:
        # ì‚¬ìš© ê°€ëŠ¥í•œ ì†ŒìŠ¤ í™•ì¸
        available = aggregator.get_available_sources()
        logger.info(f"ì‚¬ìš© ê°€ëŠ¥í•œ ì†ŒìŠ¤: {available}")

        if not available:
            logger.error("ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„° ì†ŒìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
            logger.error("API í‚¤ ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”:")
            logger.error("  - FINNHUB_API_KEY")
            logger.error("  - FRED_API_KEY")
            logger.error("  - BOK_ECOS_API_KEY")
            return 1

        # ë°ì´í„° ìˆ˜ì§‘ ë° í†µí•©
        start_time = datetime.now()
        snapshot = await aggregator.collect_and_aggregate()
        collection_time = (datetime.now() - start_time).total_seconds()

        if snapshot is None:
            logger.error("ìŠ¤ëƒ…ìƒ· ìƒì„± ì‹¤íŒ¨")
            return 1

        # ê²°ê³¼ ì¶œë ¥
        print("\n" + "=" * 60)
        print("ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼")
        print("=" * 60)
        print(f"  ìŠ¤ëƒ…ìƒ· ë‚ ì§œ: {snapshot.snapshot_date}")
        print(f"  ìŠ¤ëƒ…ìƒ· ì‹œê°„: {snapshot.snapshot_time.strftime('%Y-%m-%d %H:%M:%S KST')}")
        print(f"  ìˆ˜ì§‘ ì‹œê°„: {collection_time:.2f}ì´ˆ")
        print(f"  ë°ì´í„° ì†ŒìŠ¤: {snapshot.data_sources}")
        print(f"  ì™„ì„±ë„: {snapshot.get_completeness_score():.0%}")
        print(f"  ì‹ ì„ ë„: {snapshot.data_freshness:.0%}")

        print("\n--- ì£¼ìš” ì§€í‘œ ---")
        if snapshot.vix is not None:
            print(f"  VIX: {snapshot.vix:.2f} ({snapshot.vix_regime})")
        if snapshot.fed_rate is not None:
            print(f"  Fed Rate: {snapshot.fed_rate:.2f}%")
        if snapshot.bok_rate is not None:
            print(f"  BOK Rate: {snapshot.bok_rate:.2f}%")
        if snapshot.rate_differential is not None:
            print(f"  ê¸ˆë¦¬ì°¨: {snapshot.rate_differential:+.2f}%")
        if snapshot.usd_krw is not None:
            print(f"  USD/KRW: {snapshot.usd_krw:.2f}")
        if snapshot.kospi_index is not None:
            change = f" ({snapshot.kospi_change_pct:+.2f}%)" if snapshot.kospi_change_pct else ""
            print(f"  KOSPI: {snapshot.kospi_index:.2f}{change}")
        if snapshot.kosdaq_index is not None:
            change = f" ({snapshot.kosdaq_change_pct:+.2f}%)" if snapshot.kosdaq_change_pct else ""
            print(f"  KOSDAQ: {snapshot.kosdaq_index:.2f}{change}")

        if snapshot.missing_indicators:
            print(f"\nâš ï¸ ëˆ„ë½ëœ ì§€í‘œ: {snapshot.missing_indicators}")

        if snapshot.is_risk_off_environment:
            print("\nğŸš¨ RISK-OFF í™˜ê²½ ê°ì§€!")

        # ì €ì¥
        if not dry_run:
            logger.info("DBì— ìŠ¤ëƒ…ìƒ· ì €ì¥ ì¤‘...")
            save_success = save_snapshot_to_db(snapshot)

            if save_success:
                logger.info("âœ… ìŠ¤ëƒ…ìƒ· ì €ì¥ ì™„ë£Œ")
            else:
                logger.error("âŒ ìŠ¤ëƒ…ìƒ· ì €ì¥ ì‹¤íŒ¨")
                return 1

            # Redis ìºì‹œë„ ì €ì¥
            cache = get_macro_cache()
            cache.set_snapshot(snapshot)
            logger.info("âœ… Redis ìºì‹œ ì €ì¥ ì™„ë£Œ")
        else:
            logger.info("[DRY RUN] ì €ì¥ ìŠ¤í‚µ")

        print("=" * 60)
        print("âœ… ìˆ˜ì§‘ ì™„ë£Œ")
        print("=" * 60)

        return 0

    except Exception as e:
        logger.error(f"ìˆ˜ì§‘ ì‹¤íŒ¨: {e}", exc_info=True)
        return 1

    finally:
        await aggregator.close()


async def show_status():
    """í˜„ì¬ ìƒíƒœ í‘œì‹œ"""
    from shared.macro_data import (
        EnhancedMacroAggregator,
        get_macro_cache,
        get_today_snapshot,
    )

    print("\n" + "=" * 60)
    print("ğŸ“ˆ Enhanced Macro Data ìƒíƒœ")
    print("=" * 60)

    # ìºì‹œ ìƒíƒœ
    cache = get_macro_cache()
    stats = cache.get_stats()
    print(f"\n--- ìºì‹œ ìƒíƒœ ---")
    print(f"  ë°ì´í„° í¬ì¸íŠ¸: {stats.get('data_points', 0)}")
    print(f"  ìŠ¤ëƒ…ìƒ·: {stats.get('snapshots', 0)}")
    print(f"  ë‰´ìŠ¤: {stats.get('news', 0)}")
    print(f"  ì†ŒìŠ¤: {stats.get('sources', [])}")

    # ì˜¤ëŠ˜ ìŠ¤ëƒ…ìƒ·
    snapshot = get_today_snapshot()
    if snapshot:
        print(f"\n--- ì˜¤ëŠ˜ ìŠ¤ëƒ…ìƒ· ---")
        print(f"  ì‹œê°„: {snapshot.snapshot_time}")
        print(f"  ì™„ì„±ë„: {snapshot.get_completeness_score():.0%}")
        print(f"  ì‹ ì„ ë„: {snapshot.data_freshness:.0%}")
        print(f"  ì†ŒìŠ¤: {snapshot.data_sources}")
    else:
        print("\nâš ï¸ ì˜¤ëŠ˜ ìŠ¤ëƒ…ìƒ· ì—†ìŒ")

    # Aggregator ìƒíƒœ
    aggregator = EnhancedMacroAggregator()
    source_status = aggregator.get_source_status()
    print(f"\n--- ì†ŒìŠ¤ ìƒíƒœ ---")
    for source, status in source_status.items():
        available = "âœ…" if status["available"] else "âŒ"
        last_fetch = status.get("last_fetch", "N/A")
        print(f"  {available} {source}: {last_fetch}")

    await aggregator.close()


def main():
    parser = argparse.ArgumentParser(description="Enhanced Macro Data ìˆ˜ì§‘")
    parser.add_argument("--quick", action="store_true", help="ë¹ ë¥¸ ìˆ˜ì§‘ (pykrx, rssë§Œ)")
    parser.add_argument("--dry-run", action="store_true", help="ìˆ˜ì§‘ë§Œ (ì €ì¥ ì•ˆí•¨)")
    parser.add_argument("--sources", type=str, help="ìˆ˜ì§‘í•  ì†ŒìŠ¤ (ì½¤ë§ˆ êµ¬ë¶„)")
    parser.add_argument("--status", action="store_true", help="í˜„ì¬ ìƒíƒœ í‘œì‹œ")
    args = parser.parse_args()

    if args.status:
        asyncio.run(show_status())
        return 0

    sources = None
    if args.sources:
        sources = [s.strip() for s in args.sources.split(",")]

    exit_code = asyncio.run(collect_macro_data(
        sources=sources,
        quick=args.quick,
        dry_run=args.dry_run,
    ))

    sys.exit(exit_code)


if __name__ == "__main__":
    main()
