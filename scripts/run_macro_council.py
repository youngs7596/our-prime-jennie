#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
scripts/run_macro_council.py
----------------------------
ì¼ì¼ ë§¤í¬ë¡œ ì¸ì‚¬ì´íŠ¸ ë¶„ì„ ì‘ì—….

ë§¤ì¼ 07:30 KSTì— ì‹¤í–‰:
1. @hedgecat0301 ì±„ë„ì—ì„œ ìµœì‹  "ì¥ ì‹œì‘ ì „ ë¸Œë¦¬í•‘" ìˆ˜ì§‘
2. 3í˜„ì Council ë¶„ì„ ì‹¤í–‰
3. êµ¬ì¡°í™”ëœ ì¸ì‚¬ì´íŠ¸ë¥¼ DB/Redisì— ì €ì¥

Usage:
    python scripts/run_macro_council.py              # ì˜¤ëŠ˜ ë¶„ì„
    python scripts/run_macro_council.py --dry-run    # ë¶„ì„ë§Œ (ì €ì¥ ì•ˆí•¨)
    python scripts/run_macro_council.py --date 2026-01-29  # íŠ¹ì • ë‚ ì§œ (í…ŒìŠ¤íŠ¸ìš©)
"""

import argparse
import asyncio
import json
import logging
import os
import re
import sys
from datetime import date, datetime, timedelta
from pathlib import Path
from typing import Any, Dict, Optional

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì„¤ì •
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))
sys.path.insert(0, str(PROJECT_ROOT / "services" / "telegram-collector"))

from zoneinfo import ZoneInfo

KST = ZoneInfo("Asia/Seoul")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
)
logger = logging.getLogger("MacroCouncil")

# ==============================================================================
# ë¶„ì„ í”„ë¡¬í”„íŠ¸
# ==============================================================================

MACRO_ANALYSIS_QUERY = """ì•„ë˜ 'ì¥ ì‹œì‘ ì „ ë¸Œë¦¬í•‘' ë©”ì‹œì§€ì™€ ê¸€ë¡œë²Œ ë§¤í¬ë¡œ ë°ì´í„°, ê·¸ë¦¬ê³  **ê¸€ë¡œë²Œ ì •ì¹˜/ì§€ì •í•™ì  ë‰´ìŠ¤**ë¥¼ ì¢…í•© ë¶„ì„í•˜ì—¬ **í•œêµ­ ì£¼ì‹ì‹œì¥(KOSPI/KOSDAQ) íŠ¸ë ˆì´ë”©**ì— ëŒ€í•œ ì¸ì‚¬ì´íŠ¸ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

## ì¤‘ìš” ì›ì¹™
- VIX ë“± ë¯¸êµ­ ì§€í‘œëŠ” **ì°¸ê³ ìš©**ìœ¼ë¡œë§Œ í™œìš© (í•œêµ­ ì‹œì¥ì— ì§ì ‘ ì ìš© X)
- í•œêµ­ ì‹œì¥ ê³ ìœ ì˜ ìˆ˜ê¸‰, ëª¨ë©˜í…€, ì‹¤ì  ë“±ì„ ìš°ì„  ê³ ë ¤
- íŠ¸ë ˆì´ë”© ê¶Œê³ ëŠ” **í•œêµ­ ì‹œì¥ ë§¥ë½**ì—ì„œ íŒë‹¨
- **ì •ì¹˜/ì§€ì •í•™ì  ë¦¬ìŠ¤í¬**ê°€ í•œêµ­ ì‹œì¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë°˜ë“œì‹œ í‰ê°€

## ë°˜ë“œì‹œ í¬í•¨í•´ì•¼ í•  í•­ëª© (JSON í˜•ì‹)

### ì‹œì¥ ë¶„ì„
1. **overall_sentiment**: bullish, neutral_to_bullish, neutral, neutral_to_bearish, bearish ì¤‘ í•˜ë‚˜
2. **sentiment_score**: 0-100 (50=ì¤‘ë¦½, 70+=ê°•ì„¸, 30-=ì•½ì„¸)
3. **regime_hint**: ì‹œì¥ ë ˆì§ íŒíŠ¸ (ì˜ˆ: KOSDAQ_Momentum, Trend_Following, Mean_Reversion, Defensive ë“±)
4. **key_themes**: í•µì‹¬ í…Œë§ˆ 3-5ê°œ (rank, theme, description, impact, duration)
5. **sector_signals**: ì„¹í„°ë³„ ì‹ í˜¸ (semiconductor, automotive, bio, battery, financials, shipbuilding, defense ë“±)
   - signal: bullish/neutral/bearish
   - confidence: 0-100
   - drivers: ìƒìŠ¹ ìš”ì¸
   - risks: í•˜ë½ ìš”ì¸
6. **risk_factors**: ë¦¬ìŠ¤í¬ ìš”ì¸ ë¦¬ìŠ¤íŠ¸
7. **opportunity_factors**: ê¸°íšŒ ìš”ì¸ ë¦¬ìŠ¤íŠ¸
8. **key_stocks**: ì£¼ëª©í•  ì¢…ëª©ëª… ë¦¬ìŠ¤íŠ¸

### ì •ì¹˜/ì§€ì •í•™ì  ë¦¬ìŠ¤í¬ ë¶„ì„
9. **political_risk_level**: low, medium, high, critical ì¤‘ í•˜ë‚˜
   - ë¯¸êµ­ í–‰ì •ë¶€ ì •ì±… ë³€í™” (ê´€ì„¸, ì œì¬, ì—°ì¤€ ì¸ì‚¬ ë“±)
   - ì§€ì •í•™ì  ê¸´ì¥ (ë¶í•œ, ì¤‘êµ­-ëŒ€ë§Œ, ëŸ¬ì‹œì•„-ìš°í¬ë¼ì´ë‚˜ ë“±)
   - í•œêµ­ ì‹œì¥ì— ì§ì ‘ì  ì˜í–¥ì´ ìˆëŠ”ì§€ íŒë‹¨
10. **political_risk_summary**: ì •ì¹˜ ë¦¬ìŠ¤í¬ ìš”ì•½ (1-2ë¬¸ì¥)
    - êµ¬ì²´ì ì¸ ì´ë²¤íŠ¸ëª…ê³¼ í•œêµ­ ì‹œì¥ ì˜í–¥ ëª…ì‹œ

### íŠ¸ë ˆì´ë”© ê¶Œê³  (Councilì´ ì§ì ‘ íŒë‹¨)
11. **position_size_pct**: ê¶Œì¥ í¬ì§€ì…˜ ì‚¬ì´ì¦ˆ (50~130, ê¸°ë³¸ê°’ 100)
    - 100 = í‰ì†ŒëŒ€ë¡œ, 80 = 20% ì¶•ì†Œ, 120 = 20% í™•ëŒ€
    - ì •ì¹˜ ë¦¬ìŠ¤í¬ê°€ high/criticalì´ë©´ ì¶•ì†Œ ê³ ë ¤
12. **stop_loss_adjust_pct**: ì†ì ˆí­ ì¡°ì • (80~150, ê¸°ë³¸ê°’ 100)
    - 100 = í‰ì†ŒëŒ€ë¡œ, 130 = 30% í™•ëŒ€ (ë³€ë™ì„± ëŒ€ë¹„)
13. **strategies_to_favor**: ì˜¤ëŠ˜ ìœ ë¦¬í•œ ì „ëµ (ì•„ë˜ ëª©ë¡ì—ì„œ ì„ íƒ, ì´ìœ  í¬í•¨)
    - GOLDEN_CROSS, RSI_REBOUND, MOMENTUM, RECON_BULL_ENTRY
    - MOMENTUM_CONTINUATION, SHORT_TERM_HIGH_BREAKOUT, VOLUME_BREAKOUT_1MIN
    - BULL_PULLBACK, VCP_BREAKOUT, INSTITUTIONAL_ENTRY
14. **strategies_to_avoid**: ì˜¤ëŠ˜ í”¼í•´ì•¼ í•  ì „ëµ (ìœ„ ëª©ë¡ì—ì„œ ì„ íƒ, ì´ìœ  í¬í•¨)
15. **sectors_to_favor**: ì˜¤ëŠ˜ ìœ ë§ ì„¹í„° (í•œêµ­ì–´ë¡œ)
16. **sectors_to_avoid**: ì˜¤ëŠ˜ íšŒí”¼ ì„¹í„° (í•œêµ­ì–´ë¡œ)
17. **trading_reasoning**: ìœ„ ê¶Œê³ ì— ëŒ€í•œ ì¢…í•© ê·¼ê±° (2-3ë¬¸ì¥, ì •ì¹˜ ë¦¬ìŠ¤í¬ í¬í•¨)

JSON í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”."""


# ==============================================================================
# í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ ìˆ˜ì§‘
# ==============================================================================

async def fetch_morning_briefing(target_date: date = None, hours_ago: int = 48) -> Optional[Dict[str, Any]]:
    """
    @hedgecat0301ì—ì„œ íŠ¹ì • ë‚ ì§œì˜ ë¸Œë¦¬í•‘ ë©”ì‹œì§€ ìˆ˜ì§‘.

    Args:
        target_date: ë¶„ì„ ëŒ€ìƒ ë‚ ì§œ (KST ê¸°ì¤€)
        hours_ago: ê²€ìƒ‰ ë²”ìœ„ (ê¸°ë³¸ 48ì‹œê°„)

    Returns:
        {"content": str, "published_at": datetime, "raw_messages": list} ë˜ëŠ” None
    """
    try:
        from collector import collect_channel_messages

        if target_date is None:
            target_date = datetime.now(KST).date()

        messages = await collect_channel_messages(
            channel_username="hedgecat0301",
            max_messages=10,
            hours_ago=hours_ago,
        )

        # ë‚ ì§œ ê¸°ì¤€ í•„í„°ë§ (KST ê¸°ì¤€ìœ¼ë¡œ target_dateì— í•´ë‹¹í•˜ëŠ” ë©”ì‹œì§€)
        # ë©”ì‹œì§€ì˜ published_atì„ KSTë¡œ ë³€í™˜í•˜ì—¬ ë¹„êµ
        daily_messages = []
        for m in messages:
            msg_date_kst = m.published_at.astimezone(KST).date()
            if msg_date_kst == target_date and len(m.content) > 300:
                daily_messages.append(m)

        if not daily_messages:
            logger.warning(f"{target_date} ë‚ ì§œì˜ ë¸Œë¦¬í•‘ ë©”ì‹œì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None

        # ì—¬ëŸ¬ ë©”ì‹œì§€ê°€ ìˆìœ¼ë©´ í•©ì³ì„œ ë¶„ì„ (ê°€ì¥ ê¸´ ê²ƒ ìš°ì„  ì •ë ¬)
        daily_messages.sort(key=lambda m: len(m.content), reverse=True)

        if len(daily_messages) == 1:
            combined_content = daily_messages[0].content
        else:
            # ì—¬ëŸ¬ ë©”ì‹œì§€ í†µí•©
            combined_content = f"=== {target_date} í‚¤ì›€ì¦ê¶Œ í•œì§€ì˜ ë¸Œë¦¬í•‘ ({len(daily_messages)}ê±´) ===\n\n"
            for i, m in enumerate(daily_messages, 1):
                msg_time = m.published_at.astimezone(KST).strftime('%H:%M')
                combined_content += f"--- [{i}] {msg_time} ({len(m.content)}ì) ---\n"
                combined_content += m.content + "\n\n"

        logger.info(f"ë¸Œë¦¬í•‘ ìˆ˜ì§‘ ì™„ë£Œ: {target_date}, {len(daily_messages)}ê±´, ì´ {len(combined_content)} chars")

        return {
            "content": combined_content,
            "published_at": daily_messages[0].published_at,
            "raw_messages": daily_messages,  # DB ì €ì¥ìš©
        }

    except Exception as e:
        logger.error(f"ë¸Œë¦¬í•‘ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}", exc_info=True)
        return None


# ==============================================================================
# 3í˜„ì Council ì‹¤í–‰
# ==============================================================================

def run_council_analysis(
    message_content: str,
    target_file: str,
    global_snapshot: Optional[Dict[str, Any]] = None,
    political_news: Optional[list] = None,
) -> Dict[str, Any]:
    """
    3í˜„ì Council ë¶„ì„ ì‹¤í–‰.

    Args:
        message_content: ë¶„ì„í•  ë©”ì‹œì§€ ë‚´ìš©
        target_file: ì„ì‹œ íŒŒì¼ ê²½ë¡œ
        global_snapshot: ê¸€ë¡œë²Œ ë§¤í¬ë¡œ ìŠ¤ëƒ…ìƒ· (Enhanced Macro)
        political_news: ì •ì¹˜/ì§€ì •í•™ì  ë‰´ìŠ¤ í—¤ë“œë¼ì¸ ë¦¬ìŠ¤íŠ¸

    Returns:
        Council ë¶„ì„ ê²°ê³¼
    """
    import subprocess

    # ê¸€ë¡œë²Œ ë§¤í¬ë¡œ ë°ì´í„° ì„¹ì…˜ ìƒì„±
    global_data_section = ""
    if global_snapshot:
        global_data_section = f"""
## ê¸€ë¡œë²Œ ë§¤í¬ë¡œ ë°ì´í„° (Enhanced Macro Insight)

> ì•„ë˜ ë°ì´í„°ëŠ” ìë™ ìˆ˜ì§‘ëœ ê¸€ë¡œë²Œ ê²½ì œ ì§€í‘œì…ë‹ˆë‹¤.
> í…”ë ˆê·¸ë¨ ë¸Œë¦¬í•‘ê³¼ í•¨ê»˜ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”.

### US Economy
- Fed Rate: {global_snapshot.get('fed_rate', 'N/A')}%
- 10Y Treasury: {global_snapshot.get('treasury_10y', 'N/A')}%
- US CPI YoY: {global_snapshot.get('us_cpi_yoy', 'N/A')}%
- Unemployment: {global_snapshot.get('us_unemployment', 'N/A')}%

### Volatility & Risk
- VIX: {global_snapshot.get('vix', 'N/A')} (regime: {global_snapshot.get('vix_regime', 'N/A')})
- Risk-Off í™˜ê²½: {'ì˜ˆ' if global_snapshot.get('is_risk_off') else 'ì•„ë‹ˆì˜¤'}

### Currency
- DXY Index: {global_snapshot.get('dxy_index', 'N/A')}
- USD/KRW: {global_snapshot.get('usd_krw', 'N/A')}
- ì›í™” ì••ë ¥: {global_snapshot.get('krw_pressure', 'neutral')}

### Korea
- BOK Rate: {global_snapshot.get('bok_rate') or 'N/A'}%
- ê¸ˆë¦¬ì°¨ (Fed-BOK): {global_snapshot.get('rate_differential') or 'N/A'}%
- KOSPI: {global_snapshot.get('kospi_index') or 'N/A'} ({(global_snapshot.get('kospi_change_pct') or 0):+.2f}%)
- KOSDAQ: {global_snapshot.get('kosdaq_index') or 'N/A'} ({(global_snapshot.get('kosdaq_change_pct') or 0):+.2f}%)

### Sentiment
- ê¸€ë¡œë²Œ ë‰´ìŠ¤ ì„¼í‹°ë¨¼íŠ¸: {global_snapshot.get('global_news_sentiment', 'N/A')}
- í•œêµ­ ë‰´ìŠ¤ ì„¼í‹°ë¨¼íŠ¸: {global_snapshot.get('korea_news_sentiment', 'N/A')}

### Data Quality
- ì™„ì„±ë„: {(global_snapshot.get('completeness_score') or 0):.0%}
- ë°ì´í„° ì†ŒìŠ¤: {', '.join(global_snapshot.get('data_sources', []))}
- ëˆ„ë½ ì§€í‘œ: {', '.join(global_snapshot.get('missing_indicators', [])) or 'ì—†ìŒ'}

---
"""

    # ì •ì¹˜/ì§€ì •í•™ì  ë‰´ìŠ¤ ì„¹ì…˜ ì¶”ê°€
    political_news_section = ""
    if political_news:
        political_news_section = """
## ê¸€ë¡œë²Œ ì •ì¹˜/ì§€ì •í•™ì  ë‰´ìŠ¤ (ìµœê·¼ 24ì‹œê°„)

> ì•„ë˜ëŠ” ì‹œì¥ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆëŠ” ì •ì¹˜/ì§€ì •í•™ì  ë‰´ìŠ¤ í—¤ë“œë¼ì¸ì…ë‹ˆë‹¤.
> í•œêµ­ ì‹œì¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ í‰ê°€í•´ì£¼ì„¸ìš”.

"""
        for i, news in enumerate(political_news[:15], 1):  # ìµœëŒ€ 15ê°œ
            political_news_section += f"{i}. [{news.get('category', 'news')}] {news.get('title', '')}\n"
            if news.get('source'):
                political_news_section += f"   - ì¶œì²˜: {news['source']}\n"
        political_news_section += "\n---\n"

    # ìš”ì²­ íŒŒì¼ ìƒì„±
    request_content = f"""# Macro Analysis Request

## ë¶„ì„ ëŒ€ìƒ
- Source: @hedgecat0301 (í‚¤ì›€ í•œì§€ì˜)
- Type: ì¥ ì‹œì‘ ì „ ë¸Œë¦¬í•‘
{global_data_section}{political_news_section}
## ì›ë¬¸ ë©”ì‹œì§€ (í…”ë ˆê·¸ë¨ ë¸Œë¦¬í•‘)

```
{message_content}
```
"""
    with open(target_file, "w", encoding="utf-8") as f:
        f.write(request_content)

    # Council ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
    # ëª…ì‹œì ìœ¼ë¡œ sys.executable ì‚¬ìš© (ë¹„ì–´ìˆìœ¼ë©´ ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©)
    python_cmd = sys.executable or "/usr/local/bin/python"
    council_script = str(PROJECT_ROOT / "scripts" / "ask_prime_council.py")

    cmd = [
        python_cmd,
        council_script,
        "--query", MACRO_ANALYSIS_QUERY,
        "--file", str(target_file),
    ]

    logger.info(f"3í˜„ì Council ë¶„ì„ ì‹œì‘... (python={python_cmd})")

    result = subprocess.run(
        cmd,
        capture_output=True,
        text=True,
        timeout=300,  # 5ë¶„ íƒ€ì„ì•„ì›ƒ
    )

    if result.returncode != 0:
        logger.error(f"Council ì‹¤í–‰ ì‹¤íŒ¨: {result.stderr}")
        return {"error": result.stderr}

    # ìƒì„±ëœ ë¦¬í¬íŠ¸ íŒŒì¼ ì°¾ê¸°
    reports_dir = PROJECT_ROOT / ".ai" / "reviews"
    report_files = sorted(reports_dir.glob("council_report_*.md"), reverse=True)

    if not report_files:
        logger.error("Council ë¦¬í¬íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return {"error": "Report not found"}

    latest_report = report_files[0]
    logger.info(f"Council ë¦¬í¬íŠ¸ ìƒì„±: {latest_report.name}")

    # ë¦¬í¬íŠ¸ íŒŒì‹±
    with open(latest_report, "r", encoding="utf-8") as f:
        report_content = f.read()

    # ë¹„ìš© ì¶”ì¶œ
    cost_match = re.search(r'\*\*\$([0-9.]+)\*\*', report_content)
    cost_usd = float(cost_match.group(1)) if cost_match else 0.0

    return {
        "report_path": str(latest_report),
        "report_content": report_content,
        "cost_usd": cost_usd,
    }


# ==============================================================================
# ê²°ê³¼ íŒŒì‹±
# ==============================================================================

def parse_council_output(report_content: str) -> Dict[str, Any]:
    """
    Council ë¦¬í¬íŠ¸ì—ì„œ êµ¬ì¡°í™”ëœ ë°ì´í„° ì¶”ì¶œ.

    Args:
        report_content: ë¦¬í¬íŠ¸ ë§ˆí¬ë‹¤ìš´ ë‚´ìš©

    Returns:
        íŒŒì‹±ëœ ì¸ì‚¬ì´íŠ¸ ë°ì´í„°
    """
    result = {
        "sentiment": "neutral",
        "sentiment_score": 50,
        "regime_hint": "",
        "sector_signals": {},
        "key_themes": [],
        "risk_factors": [],
        "opportunity_factors": [],
        "key_stocks": [],
        # Trading recommendations (Councilì´ ì§ì ‘ íŒë‹¨)
        "position_size_pct": 100,  # ê¸°ë³¸ê°’ 100%
        "stop_loss_adjust_pct": 100,  # ê¸°ë³¸ê°’ 100%
        "strategies_to_favor": [],
        "strategies_to_avoid": [],
        "sectors_to_favor": [],
        "sectors_to_avoid": [],
        "trading_reasoning": "",
        # Political Risk (Councilì´ ì§ì ‘ íŒë‹¨)
        "political_risk_level": "low",
        "political_risk_summary": "",
    }

    try:
        # Appendixì—ì„œ JSON ì¶”ì¶œ ì‹œë„
        # Minjiì˜ ì¶œë ¥ì—ì„œ êµ¬ì¡°í™”ëœ JSON ì°¾ê¸°
        json_patterns = [
            r'"overall_sentiment":\s*"([^"]+)"',
            r'"sentiment":\s*"([^"]+)"',
        ]

        for pattern in json_patterns:
            match = re.search(pattern, report_content)
            if match:
                result["sentiment"] = match.group(1)
                break

        # sentiment_score ì¶”ì¶œ
        score_match = re.search(r'"sentiment_score":\s*(\d+)', report_content)
        if score_match:
            result["sentiment_score"] = int(score_match.group(1))

        # regime_hint ì¶”ì¶œ
        regime_patterns = [
            r'"regime_hint":\s*"([^"]+)"',
            r'Regime Hint[^:]*:\s*[`"]?([^`"\n]+)',
        ]
        for pattern in regime_patterns:
            match = re.search(pattern, report_content, re.IGNORECASE)
            if match:
                result["regime_hint"] = match.group(1).strip()
                break

        # sector_signals ì¶”ì¶œ (JSON ë¸”ë¡ì—ì„œ)
        sector_match = re.search(r'"sector_signals":\s*(\{[^}]+\})', report_content, re.DOTALL)
        if sector_match:
            try:
                # ë¶ˆì™„ì „í•œ JSON ì²˜ë¦¬
                sector_json = sector_match.group(1)
                # ê°„ë‹¨í•œ íŒŒì‹± ì‹œë„
                result["sector_signals"] = json.loads(sector_json)
            except json.JSONDecodeError:
                pass

        # key_themes ì¶”ì¶œ (íŒ¨í„´ ë§¤ì¹­)
        theme_matches = re.findall(
            r'\*\*Key Theme \d+[^*]*\*\*[:\s-]*([^\n]+)',
            report_content,
            re.IGNORECASE
        )
        if theme_matches:
            result["key_themes"] = [
                {"rank": i + 1, "theme": t.strip(), "impact": "high"}
                for i, t in enumerate(theme_matches[:5])
            ]

        # risk_factors ì¶”ì¶œ
        risk_section = re.search(
            r'[Rr]isk[s_]*[Ff]actors?[:\s]*\n(.*?)(?=\n\n|\n##|\Z)',
            report_content,
            re.DOTALL
        )
        if risk_section:
            risks = re.findall(r'[-*]\s*\*?\*?([^*\n]+)', risk_section.group(1))
            result["risk_factors"] = [r.strip() for r in risks[:5]]

        # opportunity_factors ì¶”ì¶œ
        opp_section = re.search(
            r'[Oo]pportunity[_\s]*[Ff]actors?[:\s]*\n(.*?)(?=\n\n|\n##|\Z)',
            report_content,
            re.DOTALL
        )
        if opp_section:
            opps = re.findall(r'[-*]\s*\*?\*?([^*\n]+)', opp_section.group(1))
            result["opportunity_factors"] = [o.strip() for o in opps[:5]]

        # key_stocks ì¶”ì¶œ (ì¢…ëª©ì½”ë“œ íŒ¨í„´)
        stock_codes = re.findall(r'([ê°€-í£]+)\s*\(\d{6}\.KS\)', report_content)
        stock_names = re.findall(r'(ì‚¼ì„±ì „ì|SKí•˜ì´ë‹‰ìŠ¤|í˜„ëŒ€ì°¨|LGì—ë„ˆì§€ì†”ë£¨ì…˜|ì‚¼ì„±ë°”ì´ì˜¤|ì¹´ì¹´ì˜¤|ë„¤ì´ë²„|ì…€íŠ¸ë¦¬ì˜¨|í˜„ëŒ€ëª¨ë¹„ìŠ¤|ê¸°ì•„|POSCOí™€ë”©ìŠ¤|KBê¸ˆìœµ|ì‹ í•œì§€ì£¼|í•˜ë‚˜ê¸ˆìœµ|ì‚¼ì„±SDI|LGí™”í•™|ì—”ë¹„ë””ì•„|ë©”íƒ€|ì• í”Œ|MS|ë§ˆì´í¬ë¡ |ìƒŒë””ìŠ¤í¬)', report_content)
        result["key_stocks"] = list(set(stock_codes + stock_names))[:10]

        # ========== Trading Recommendations (Council ì§ì ‘ íŒë‹¨) ==========

        # position_size_pct ì¶”ì¶œ (50~130)
        pos_match = re.search(r'"position_size_pct":\s*(\d+)', report_content)
        if pos_match:
            pct = int(pos_match.group(1))
            result["position_size_pct"] = max(50, min(130, pct))  # ë²”ìœ„ ì œí•œ

        # stop_loss_adjust_pct ì¶”ì¶œ (80~150)
        sl_match = re.search(r'"stop_loss_adjust_pct":\s*(\d+)', report_content)
        if sl_match:
            pct = int(sl_match.group(1))
            result["stop_loss_adjust_pct"] = max(80, min(150, pct))

        # strategies_to_favor ì¶”ì¶œ
        favor_match = re.search(r'"strategies_to_favor":\s*\[(.*?)\]', report_content, re.DOTALL)
        if favor_match:
            strategies = re.findall(r'"([A-Z_]+)"', favor_match.group(1))
            result["strategies_to_favor"] = strategies

        # strategies_to_avoid ì¶”ì¶œ
        avoid_match = re.search(r'"strategies_to_avoid":\s*\[(.*?)\]', report_content, re.DOTALL)
        if avoid_match:
            strategies = re.findall(r'"([A-Z_]+)"', avoid_match.group(1))
            result["strategies_to_avoid"] = strategies

        # sectors_to_favor ì¶”ì¶œ
        sec_favor_match = re.search(r'"sectors_to_favor":\s*\[(.*?)\]', report_content, re.DOTALL)
        if sec_favor_match:
            sectors = re.findall(r'"([^"]+)"', sec_favor_match.group(1))
            result["sectors_to_favor"] = [s for s in sectors if s]

        # sectors_to_avoid ì¶”ì¶œ
        sec_avoid_match = re.search(r'"sectors_to_avoid":\s*\[(.*?)\]', report_content, re.DOTALL)
        if sec_avoid_match:
            sectors = re.findall(r'"([^"]+)"', sec_avoid_match.group(1))
            result["sectors_to_avoid"] = [s for s in sectors if s]

        # trading_reasoning ì¶”ì¶œ
        reason_match = re.search(r'"trading_reasoning":\s*"([^"]+)"', report_content, re.DOTALL)
        if reason_match:
            result["trading_reasoning"] = reason_match.group(1).strip()

        # ========== Political Risk (Council ì§ì ‘ íŒë‹¨) ==========

        # political_risk_level ì¶”ì¶œ (low, medium, high, critical)
        pol_level_match = re.search(r'"political_risk_level":\s*"([^"]+)"', report_content)
        if pol_level_match:
            level = pol_level_match.group(1).lower().strip()
            if level in ["low", "medium", "high", "critical"]:
                result["political_risk_level"] = level

        # political_risk_summary ì¶”ì¶œ
        pol_summary_match = re.search(r'"political_risk_summary":\s*"([^"]+)"', report_content, re.DOTALL)
        if pol_summary_match:
            result["political_risk_summary"] = pol_summary_match.group(1).strip()

        logger.info(f"íŒŒì‹± ê²°ê³¼: sentiment={result['sentiment']}, score={result['sentiment_score']}, position={result['position_size_pct']}%, political_risk={result['political_risk_level']}")

    except Exception as e:
        logger.error(f"íŒŒì‹± ì˜¤ë¥˜: {e}", exc_info=True)

    return result


# ==============================================================================
# ì €ì¥
# ==============================================================================

def save_telegram_briefings(
    insight_date: date,
    raw_messages: list,
    channel_username: str = "hedgecat0301",
    channel_name: str = "í•œì§€ì˜ - í‚¤ì›€ì¦ê¶Œ",
    analyst_name: str = "í•œì§€ì˜",
) -> int:
    """
    í…”ë ˆê·¸ë¨ ë¸Œë¦¬í•‘ ë©”ì‹œì§€ë¥¼ DBì— ì €ì¥.

    Args:
        insight_date: ì¸ì‚¬ì´íŠ¸ ë‚ ì§œ
        raw_messages: ìˆ˜ì§‘ëœ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ (CollectedMessage ê°ì²´)
        channel_username: ì±„ë„ username
        channel_name: ì±„ë„ ì´ë¦„
        analyst_name: ë¶„ì„ê°€ ì´ë¦„

    Returns:
        ì €ì¥ëœ ë©”ì‹œì§€ ìˆ˜
    """
    if not raw_messages:
        return 0

    try:
        from shared.db.connection import get_session
        from sqlalchemy import text

        saved_count = 0
        with get_session() as session:
            for msg in raw_messages:
                try:
                    # UTC -> KST ë³€í™˜ (Telegram APIëŠ” UTCë¡œ ë°˜í™˜)
                    published_at_kst = msg.published_at.astimezone(KST) if msg.published_at.tzinfo else msg.published_at
                    collected_at_kst = msg.collected_at.astimezone(KST) if msg.collected_at.tzinfo else msg.collected_at

                    # UPSERT: ì´ë¯¸ ìˆìœ¼ë©´ ìŠ¤í‚µ
                    session.execute(text("""
                        INSERT IGNORE INTO TELEGRAM_BRIEFINGS
                        (MESSAGE_ID, CHANNEL_USERNAME, CHANNEL_NAME, ANALYST_NAME,
                         CONTENT, PUBLISHED_AT, COLLECTED_AT, INSIGHT_DATE)
                        VALUES
                        (:message_id, :channel_username, :channel_name, :analyst_name,
                         :content, :published_at, :collected_at, :insight_date)
                    """), {
                        "message_id": msg.message_id,
                        "channel_username": channel_username,
                        "channel_name": channel_name,
                        "analyst_name": analyst_name,
                        "content": msg.content,
                        "published_at": published_at_kst.replace(tzinfo=None),  # KSTë¡œ ì €ì¥ (timezone ì—†ì´)
                        "collected_at": collected_at_kst.replace(tzinfo=None),
                        "insight_date": insight_date,
                    })
                    saved_count += 1
                except Exception as e:
                    logger.warning(f"ë©”ì‹œì§€ ì €ì¥ ì‹¤íŒ¨ (ID={msg.message_id}): {e}")

            session.commit()

        logger.info(f"âœ… í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ {saved_count}ê±´ ì €ì¥ ì™„ë£Œ")
        return saved_count

    except Exception as e:
        logger.error(f"í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ ì €ì¥ ì‹¤íŒ¨: {e}")
        return 0


async def get_political_news_headlines(max_items: int = 15) -> list:
    """
    ì •ì¹˜/ì§€ì •í•™ì  ë‰´ìŠ¤ í—¤ë“œë¼ì¸ ìˆ˜ì§‘.

    PoliticalNewsClientì˜ í‚¤ì›Œë“œ ê°ì§€ ê¸°ëŠ¥ì„ í™œìš©í•˜ì—¬
    ì‹œì¥ ì˜í–¥ë ¥ ìˆëŠ” ë‰´ìŠ¤ë§Œ í•„í„°ë§í•©ë‹ˆë‹¤.

    Returns:
        [{"title": str, "source": str, "category": str}, ...]
    """
    try:
        from shared.macro_data.clients.political_news_client import PoliticalNewsClient

        client = PoliticalNewsClient()
        try:
            alerts = await client.fetch_alerts(max_age_hours=24, min_severity="medium")

            headlines = []
            seen_titles = set()
            for alert in alerts[:max_items]:
                if alert.title not in seen_titles:
                    headlines.append({
                        "title": alert.title,
                        "source": alert.source,
                        "category": alert.category,
                        "severity": alert.severity,
                    })
                    seen_titles.add(alert.title)

            logger.info(f"ì •ì¹˜ ë‰´ìŠ¤ ìˆ˜ì§‘: {len(headlines)}ê±´ (critical: {sum(1 for h in headlines if h.get('severity') == 'critical')})")
            return headlines

        finally:
            await client.close()

    except ImportError:
        logger.warning("PoliticalNewsClient ëª¨ë“ˆ ì—†ìŒ")
        return []
    except Exception as e:
        logger.warning(f"ì •ì¹˜ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return []


def get_global_macro_snapshot() -> Optional[Dict[str, Any]]:
    """
    ì˜¤ëŠ˜ì˜ ê¸€ë¡œë²Œ ë§¤í¬ë¡œ ìŠ¤ëƒ…ìƒ· ì¡°íšŒ.

    Returns:
        ìŠ¤ëƒ…ìƒ· ë”•ì…”ë„ˆë¦¬ ë˜ëŠ” None
    """
    try:
        from shared.macro_data import get_today_snapshot

        snapshot = get_today_snapshot()
        if snapshot:
            logger.info(f"ê¸€ë¡œë²Œ ìŠ¤ëƒ…ìƒ· ë¡œë“œ: ì™„ì„±ë„ {snapshot.get_completeness_score():.0%}")
            return snapshot.to_dict()
        else:
            logger.warning("ì˜¤ëŠ˜ ê¸€ë¡œë²Œ ìŠ¤ëƒ…ìƒ· ì—†ìŒ")
            return None
    except ImportError:
        logger.warning("shared.macro_data ëª¨ë“ˆ ì—†ìŒ (ê¸€ë¡œë²Œ ë°ì´í„° ìŠ¤í‚µ)")
        return None
    except Exception as e:
        logger.warning(f"ê¸€ë¡œë²Œ ìŠ¤ëƒ…ìƒ· ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return None


def save_macro_insight(
    insight_date: date,
    briefing: Dict[str, Any],
    council_result: Dict[str, Any],
    parsed_data: Dict[str, Any],
    global_snapshot: Optional[Dict[str, Any]] = None,
    dry_run: bool = False,
) -> bool:
    """
    ë§¤í¬ë¡œ ì¸ì‚¬ì´íŠ¸ ì €ì¥ (DB + Redis).

    Args:
        insight_date: ì¸ì‚¬ì´íŠ¸ ë‚ ì§œ
        briefing: ì›ë³¸ ë¸Œë¦¬í•‘ ë°ì´í„°
        council_result: Council ë¶„ì„ ê²°ê³¼
        parsed_data: íŒŒì‹±ëœ ë°ì´í„°
        global_snapshot: ê¸€ë¡œë²Œ ë§¤í¬ë¡œ ìŠ¤ëƒ…ìƒ· (ë¯¸ë¦¬ ì¡°íšŒëœ ê²½ìš°)
        dry_run: Trueë©´ ì €ì¥ ì•ˆí•¨

    Returns:
        ì €ì¥ ì„±ê³µ ì—¬ë¶€
    """
    from shared.macro_insight import (
        DailyMacroInsight,
        save_insight_to_db,
        save_insight_to_redis,
    )

    # ê¸€ë¡œë²Œ ìŠ¤ëƒ…ìƒ·ì´ ì—†ìœ¼ë©´ ì¡°íšŒ ì‹œë„
    if global_snapshot is None:
        global_snapshot = get_global_macro_snapshot()

    # VIX regime ë° ê¸ˆë¦¬ì°¨ ì¶”ì¶œ
    vix_regime = ""
    rate_differential = None
    data_sources_used = []

    if global_snapshot:
        vix_regime = global_snapshot.get("vix_regime", "")
        rate_differential = global_snapshot.get("rate_differential")
        data_sources_used = global_snapshot.get("data_sources", [])

    insight = DailyMacroInsight(
        insight_date=insight_date,
        source_channel="hedgecat0301",
        source_analyst="í‚¤ì›€ í•œì§€ì˜",
        sentiment=parsed_data.get("sentiment", "neutral"),
        sentiment_score=parsed_data.get("sentiment_score", 50),
        regime_hint=parsed_data.get("regime_hint", ""),
        sector_signals=parsed_data.get("sector_signals", {}),
        key_themes=parsed_data.get("key_themes", []),
        risk_factors=parsed_data.get("risk_factors", []),
        opportunity_factors=parsed_data.get("opportunity_factors", []),
        key_stocks=parsed_data.get("key_stocks", []),
        raw_message=briefing.get("content", ""),
        raw_council_output={
            "report_content": council_result.get("report_content", "")[:10000],  # 10KB ì œí•œ
        },
        council_cost_usd=council_result.get("cost_usd", 0.0),
        # Enhanced fields
        global_snapshot=global_snapshot,
        data_sources_used=data_sources_used,
        vix_regime=vix_regime,
        rate_differential=rate_differential,
        # Trading Recommendations (Councilì´ ì§ì ‘ íŒë‹¨)
        position_size_pct=parsed_data.get("position_size_pct", 100),
        stop_loss_adjust_pct=parsed_data.get("stop_loss_adjust_pct", 100),
        strategies_to_favor=parsed_data.get("strategies_to_favor", []),
        strategies_to_avoid=parsed_data.get("strategies_to_avoid", []),
        sectors_to_favor=parsed_data.get("sectors_to_favor", []),
        sectors_to_avoid=parsed_data.get("sectors_to_avoid", []),
        trading_reasoning=parsed_data.get("trading_reasoning", ""),
        # Political Risk (Councilì´ ì§ì ‘ íŒë‹¨)
        political_risk_level=parsed_data.get("political_risk_level", "low"),
        political_risk_summary=parsed_data.get("political_risk_summary", ""),
    )

    if dry_run:
        logger.info("[DRY RUN] ì €ì¥ ìŠ¤í‚µ")
        print("\n" + "=" * 60)
        print("ğŸ“Š ë¶„ì„ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°")
        print("=" * 60)
        print(json.dumps(insight.to_dict(), ensure_ascii=False, indent=2, default=str))
        return True

    # DB ì €ì¥ (í•„ìˆ˜)
    db_success = save_insight_to_db(insight)

    # Redis ì €ì¥ (ì„ íƒ - ì‹¤íŒ¨í•´ë„ ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬)
    redis_success = save_insight_to_redis(insight)

    if db_success:
        logger.info(f"âœ… ë§¤í¬ë¡œ ì¸ì‚¬ì´íŠ¸ ì €ì¥ ì™„ë£Œ: {insight_date}")
        if not redis_success:
            logger.warning("âš ï¸ Redis ìºì‹œ ì €ì¥ ì‹¤íŒ¨ (DB ì €ì¥ì€ ì„±ê³µ)")
        return True
    else:
        logger.error(f"âŒ DB ì €ì¥ ì‹¤íŒ¨")
        return False


# ==============================================================================
# Main
# ==============================================================================

async def main(args):
    """ë©”ì¸ ì‹¤í–‰"""
    logger.info("=" * 60)
    logger.info("ğŸ›ï¸ 3í˜„ì Council ë§¤í¬ë¡œ ë¶„ì„ ì‹œì‘")
    logger.info("=" * 60)

    # ë‚ ì§œ ê²°ì •
    if args.date:
        target_date = datetime.strptime(args.date, "%Y-%m-%d").date()
        hours_ago = (datetime.now(KST).date() - target_date).days * 24 + 24
    else:
        target_date = datetime.now(KST).date()
        hours_ago = 24

    logger.info(f"ë¶„ì„ ëŒ€ìƒ ë‚ ì§œ: {target_date}")

    # 1. ê¸€ë¡œë²Œ ë§¤í¬ë¡œ ìŠ¤ëƒ…ìƒ· ìˆ˜ì§‘ (Enhanced Macro)
    global_snapshot = get_global_macro_snapshot()
    if global_snapshot:
        logger.info(f"âœ… ê¸€ë¡œë²Œ ë§¤í¬ë¡œ ìŠ¤ëƒ…ìƒ· ë¡œë“œ ì™„ë£Œ")
        logger.info(f"   - VIX: {global_snapshot.get('vix', 'N/A')} ({global_snapshot.get('vix_regime', 'N/A')})")
        logger.info(f"   - ê¸ˆë¦¬ì°¨: {global_snapshot.get('rate_differential', 'N/A')}%")
        logger.info(f"   - KOSPI: {global_snapshot.get('kospi_index', 'N/A')}")
    else:
        logger.warning("âš ï¸ ê¸€ë¡œë²Œ ë§¤í¬ë¡œ ìŠ¤ëƒ…ìƒ· ì—†ìŒ (í…”ë ˆê·¸ë¨ ë¸Œë¦¬í•‘ë§Œ ë¶„ì„)")

    # 2. í…”ë ˆê·¸ë¨ ë¸Œë¦¬í•‘ ìˆ˜ì§‘ (ë‚ ì§œ ê¸°ì¤€)
    briefing = await fetch_morning_briefing(target_date=target_date, hours_ago=hours_ago)
    if not briefing:
        logger.error("ë¸Œë¦¬í•‘ ìˆ˜ì§‘ ì‹¤íŒ¨. ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return 1

    # 2-1. í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ DB ì €ì¥ (ëŒ€ì‹œë³´ë“œ í‘œì‹œìš©)
    if not args.dry_run and briefing.get("raw_messages"):
        save_telegram_briefings(
            insight_date=target_date,
            raw_messages=briefing["raw_messages"],
        )

    # 2-2. ì •ì¹˜/ì§€ì •í•™ì  ë‰´ìŠ¤ ìˆ˜ì§‘
    political_news = await get_political_news_headlines(max_items=15)
    if political_news:
        logger.info(f"âœ… ì •ì¹˜ ë‰´ìŠ¤ ìˆ˜ì§‘: {len(political_news)}ê±´")
        critical_count = sum(1 for n in political_news if n.get("severity") == "critical")
        if critical_count > 0:
            logger.warning(f"âš ï¸ Critical ë‰´ìŠ¤ {critical_count}ê±´ ê°ì§€!")
    else:
        logger.info("â„¹ï¸ ì •ì¹˜ ë‰´ìŠ¤ ì—†ìŒ (ë˜ëŠ” ìˆ˜ì§‘ ì‹¤íŒ¨)")

    # 3. Council ë¶„ì„ (ê¸€ë¡œë²Œ ë°ì´í„° + í…”ë ˆê·¸ë¨ ë¸Œë¦¬í•‘ + ì •ì¹˜ ë‰´ìŠ¤ í†µí•©)
    reviews_dir = PROJECT_ROOT / ".ai" / "reviews"
    reviews_dir.mkdir(parents=True, exist_ok=True)
    temp_file = reviews_dir / f"council_request_macro_{target_date}.md"
    council_result = run_council_analysis(
        briefing["content"],
        str(temp_file),
        global_snapshot=global_snapshot,  # ê¸€ë¡œë²Œ ë°ì´í„° ì „ë‹¬
        political_news=political_news,  # ì •ì¹˜ ë‰´ìŠ¤ ì „ë‹¬
    )

    if "error" in council_result:
        logger.error(f"Council ë¶„ì„ ì‹¤íŒ¨: {council_result['error']}")
        return 1

    # 4. ê²°ê³¼ íŒŒì‹±
    parsed_data = parse_council_output(council_result["report_content"])

    # 5. ì €ì¥ (ê¸€ë¡œë²Œ ìŠ¤ëƒ…ìƒ· í¬í•¨)
    save_success = save_macro_insight(
        insight_date=target_date,
        briefing=briefing,
        council_result=council_result,
        parsed_data=parsed_data,
        global_snapshot=global_snapshot,  # ì´ë¯¸ ì¡°íšŒí•œ ìŠ¤ëƒ…ìƒ· ì „ë‹¬
        dry_run=args.dry_run,
    )

    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 60)
    print("ğŸ“‹ ë§¤í¬ë¡œ ì¸ì‚¬ì´íŠ¸ ìš”ì•½")
    print("=" * 60)
    print(f"  ë‚ ì§œ: {target_date}")
    print(f"  Sentiment: {parsed_data['sentiment']} (Score: {parsed_data['sentiment_score']})")
    print(f"  Regime Hint: {parsed_data['regime_hint']}")
    print(f"  Key Themes: {len(parsed_data['key_themes'])}ê°œ")
    print(f"  Sector Signals: {list(parsed_data['sector_signals'].keys())}")
    print(f"  Risk Factors: {parsed_data['risk_factors'][:2]}")
    print(f"  Key Stocks: {parsed_data['key_stocks'][:5]}")
    print(f"  Council Cost: ${council_result.get('cost_usd', 0):.4f}")

    # Trading Recommendations (Council ì§ì ‘ íŒë‹¨)
    print("\n--- Trading Recommendations (Council íŒë‹¨) ---")
    print(f"  Position Size: {parsed_data.get('position_size_pct', 100)}%")
    print(f"  Stop Loss Adjust: {parsed_data.get('stop_loss_adjust_pct', 100)}%")
    print(f"  ìœ ë¦¬í•œ ì „ëµ: {parsed_data.get('strategies_to_favor', [])}")
    print(f"  í”¼í•´ì•¼ í•  ì „ëµ: {parsed_data.get('strategies_to_avoid', [])}")
    print(f"  ìœ ë§ ì„¹í„°: {parsed_data.get('sectors_to_favor', [])}")
    print(f"  íšŒí”¼ ì„¹í„°: {parsed_data.get('sectors_to_avoid', [])}")
    print(f"  ê·¼ê±°: {(parsed_data.get('trading_reasoning', '') or 'N/A')[:100]}...")

    # Political Risk (Council ì§ì ‘ íŒë‹¨)
    print("\n--- Political Risk (Council íŒë‹¨) ---")
    pol_level = parsed_data.get('political_risk_level', 'low')
    pol_emoji = {"low": "ğŸŸ¢", "medium": "ğŸŸ¡", "high": "ğŸŸ ", "critical": "ğŸ”´"}.get(pol_level, "âšª")
    print(f"  Risk Level: {pol_emoji} {pol_level.upper()}")
    print(f"  ìš”ì•½: {(parsed_data.get('political_risk_summary', '') or 'N/A')[:150]}")

    # Enhanced Macro ì •ë³´
    if global_snapshot:
        print("\n--- Enhanced Macro Data ---")
        print(f"  VIX: {global_snapshot.get('vix', 'N/A')} ({global_snapshot.get('vix_regime', 'N/A')})")
        print(f"  ê¸ˆë¦¬ì°¨ (Fed-BOK): {global_snapshot.get('rate_differential', 'N/A')}%")
        print(f"  USD/KRW: {global_snapshot.get('usd_krw', 'N/A')}")
        print(f"  ë°ì´í„° ì†ŒìŠ¤: {', '.join(global_snapshot.get('data_sources', []))}")
    else:
        print("\nâš ï¸ Enhanced Macro ë°ì´í„° ì—†ìŒ")

    print("=" * 60)

    return 0 if save_success else 1


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ì¼ì¼ ë§¤í¬ë¡œ Council ë¶„ì„")
    parser.add_argument("--dry-run", action="store_true", help="ë¶„ì„ë§Œ ì‹¤í–‰ (ì €ì¥ ì•ˆí•¨)")
    parser.add_argument("--date", type=str, help="ë¶„ì„ ë‚ ì§œ (YYYY-MM-DD, ê¸°ë³¸: ì˜¤ëŠ˜)")
    args = parser.parse_args()

    exit_code = asyncio.run(main(args))
    sys.exit(exit_code)
