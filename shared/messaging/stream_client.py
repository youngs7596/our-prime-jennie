#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
shared/messaging/stream_client.py
---------------------------------
Redis Streams ê¸°ë°˜ ë©”ì‹œì§€ ë¸Œë¡œì»¤ í´ë¼ì´ì–¸íŠ¸.
ë‰´ìŠ¤ íŒŒì´í”„ë¼ì¸(Collector -> Archiver/Analyzer)ì˜ Pub/Sub í†µì‹ ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.
"""

import os
import json
import hashlib
import logging
import re
from datetime import datetime, timezone, timedelta
from typing import Dict, List, Optional, Any, Callable, Set

import redis

logger = logging.getLogger(__name__)

# ==============================================================================
# Configuration
# ==============================================================================

REDIS_URL = os.getenv("REDIS_URL", "redis://10.178.0.2:6379/0")

# Stream Names
STREAM_NEWS_RAW = "stream:news:raw"  # Collector -> Archiver/Analyzer
STREAM_MACRO_RAW = "stream:macro:raw"  # TelegramCollector -> MacroAnalyzer

# Consumer Groups
GROUP_ARCHIVER = "group_archiver"
GROUP_ANALYZER = "group_analyzer"
GROUP_MACRO_ANALYZER = "group_macro_analyzer"

# Default Settings
DEFAULT_BLOCK_MS = 2000  # 2 seconds
DEFAULT_BATCH_SIZE = 10


# ==============================================================================
# Redis Client Singleton
# ==============================================================================

_redis_client: Optional[redis.Redis] = None


def get_redis_client() -> redis.Redis:
    """Redis í´ë¼ì´ì–¸íŠ¸ ì‹±ê¸€í†¤ ë°˜í™˜"""
    global _redis_client
    if _redis_client is None:
        _redis_client = redis.from_url(REDIS_URL, decode_responses=False)
        logger.info(f"âœ… Redis ì—°ê²° ì™„ë£Œ: {REDIS_URL}")
    return _redis_client


# ==============================================================================
# Producer Functions (Collector)
# ==============================================================================

def publish_news(
    page_content: str,
    metadata: Dict[str, Any],
    stream_name: str = STREAM_NEWS_RAW
) -> str:
    """
    ë‰´ìŠ¤ ë©”ì‹œì§€ë¥¼ Redis Streamì— ë°œí–‰í•©ë‹ˆë‹¤.
    
    Args:
        page_content: ë‰´ìŠ¤ ë³¸ë¬¸
        metadata: ë©”íƒ€ë°ì´í„° (stock_code, source_url, created_at_utc ë“±)
        stream_name: ëŒ€ìƒ ìŠ¤íŠ¸ë¦¼ ì´ë¦„
    
    Returns:
        ë°œí–‰ëœ ë©”ì‹œì§€ ID
    """
    client = get_redis_client()
    
    # Serialize metadata to JSON (handles datetime, etc.)
    message = {
        b"page_content": page_content.encode("utf-8"),
        b"metadata": json.dumps(metadata, default=str).encode("utf-8"),
        b"published_at": datetime.now(timezone.utc).isoformat().encode("utf-8"),
    }
    
    msg_id = client.xadd(stream_name, message)
    logger.debug(f"âœ… [Stream] ë°œí–‰ ì™„ë£Œ: {msg_id.decode()} -> {stream_name}")
    return msg_id.decode() if isinstance(msg_id, bytes) else msg_id


def publish_news_batch(
    documents: List[Dict[str, Any]],
    stream_name: str = STREAM_NEWS_RAW
) -> int:
    """
    ì—¬ëŸ¬ ë‰´ìŠ¤ë¥¼ í•œ ë²ˆì— ë°œí–‰í•©ë‹ˆë‹¤.
    
    Args:
        documents: [{"page_content": str, "metadata": dict}, ...]
        stream_name: ëŒ€ìƒ ìŠ¤íŠ¸ë¦¼ ì´ë¦„
    
    Returns:
        ë°œí–‰ëœ ë©”ì‹œì§€ ìˆ˜
    """
    client = get_redis_client()
    pipeline = client.pipeline()
    
    for doc in documents:
        message = {
            b"page_content": doc["page_content"].encode("utf-8"),
            b"metadata": json.dumps(doc["metadata"], default=str).encode("utf-8"),
            b"published_at": datetime.now(timezone.utc).isoformat().encode("utf-8"),
        }
        pipeline.xadd(stream_name, message)
    
    results = pipeline.execute()
    published = sum(1 for r in results if r)
    logger.info(f"âœ… [Stream] ë°°ì¹˜ ë°œí–‰ ì™„ë£Œ: {published}/{len(documents)}ê°œ -> {stream_name}")
    return published


# ==============================================================================
# Consumer Functions (Archiver, Analyzer)
# ==============================================================================

def ensure_consumer_group(
    stream_name: str = STREAM_NEWS_RAW,
    group_name: str = GROUP_ARCHIVER,
    reset_cursor: bool = False
) -> bool:
    """
    Consumer Groupì´ ì—†ìœ¼ë©´ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        reset_cursor: Trueì´ë©´ ê·¸ë£¹ì´ ì´ë¯¸ ì¡´ì¬í•´ë„ ì»¤ì„œë¥¼ ì²˜ìŒ("0")ìœ¼ë¡œ ë¦¬ì…‹
    
    Returns:
        ìƒì„± ì„±ê³µ ì—¬ë¶€ (ì´ë¯¸ ì¡´ì¬í•˜ë©´ True)
    """
    client = get_redis_client()
    try:
        # Create group starting from the beginning ('0')
        client.xgroup_create(stream_name, group_name, id="0", mkstream=True)
        logger.info(f"âœ… [Stream] Consumer Group ìƒì„±: {group_name} @ {stream_name}")
        return True
    except redis.ResponseError as e:
        if "BUSYGROUP" in str(e):
            logger.info(f"â„¹ï¸ [Stream] Consumer Group ì´ë¯¸ ì¡´ì¬: {group_name}")
            if reset_cursor:
                # ê¸°ì¡´ ê·¸ë£¹ì˜ ì»¤ì„œë¥¼ ì²˜ìŒìœ¼ë¡œ ë¦¬ì…‹ (ëª¨ë“  ë©”ì‹œì§€ ì¬ì²˜ë¦¬)
                client.xgroup_setid(stream_name, group_name, id="0")
                logger.info(f"ğŸ”„ [Stream] Consumer Group ì»¤ì„œ ë¦¬ì…‹: {group_name} -> 0")
            return True
        raise


def consume_messages(
    group_name: str,
    consumer_name: str,
    handler: Callable[[str, Dict[str, Any]], bool],
    stream_name: str = STREAM_NEWS_RAW,
    batch_size: int = DEFAULT_BATCH_SIZE,
    block_ms: int = DEFAULT_BLOCK_MS,
    max_iterations: Optional[int] = None
) -> int:
    """
    Consumer Group ë°©ì‹ìœ¼ë¡œ ë©”ì‹œì§€ë¥¼ ì†Œë¹„í•©ë‹ˆë‹¤.
    
    Args:
        group_name: Consumer Group ì´ë¦„
        consumer_name: ì´ Consumerì˜ ê³ ìœ  ì´ë¦„
        handler: ë©”ì‹œì§€ ì²˜ë¦¬ í•¨ìˆ˜ (page_content, metadata) -> success
        stream_name: ìŠ¤íŠ¸ë¦¼ ì´ë¦„
        batch_size: í•œ ë²ˆì— ì½ì„ ë©”ì‹œì§€ ìˆ˜
        block_ms: ë©”ì‹œì§€ê°€ ì—†ì„ ë•Œ ëŒ€ê¸° ì‹œê°„ (ms)
        max_iterations: ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ (None=ë¬´í•œ)
    
    Returns:
        ì²˜ë¦¬í•œ ë©”ì‹œì§€ ì´ ìˆ˜
    """
    client = get_redis_client()
    ensure_consumer_group(stream_name, group_name)
    
    total_processed = 0
    iteration = 0
    
    logger.info(f"ğŸš€ [Stream] Consumer ì‹œì‘: {consumer_name} @ {group_name}/{stream_name}")
    
    while True:
        if max_iterations and iteration >= max_iterations:
            break
        iteration += 1
        
        try:
            # Read new messages
            messages = client.xreadgroup(
                group_name,
                consumer_name,
                {stream_name: ">"},
                count=batch_size,
                block=block_ms
            )
            
            if not messages:
                logger.debug(f"[Stream] ìƒˆ ë©”ì‹œì§€ ì—†ìŒ, ëŒ€ê¸° ì¤‘... (iteration={iteration})")
                continue
            
            for stream, msg_list in messages:
                for msg_id, msg_data in msg_list:
                    try:
                        # Deserialize
                        page_content = msg_data[b"page_content"].decode("utf-8")
                        metadata = json.loads(msg_data[b"metadata"].decode("utf-8"))
                        
                        # Call handler
                        success = handler(page_content, metadata)
                        
                        if success:
                            # ACK
                            client.xack(stream_name, group_name, msg_id)
                            total_processed += 1
                            logger.debug(f"âœ… [Stream] ì²˜ë¦¬ ì™„ë£Œ: {msg_id.decode()}")
                        else:
                            logger.warning(f"âš ï¸ [Stream] í•¸ë“¤ëŸ¬ ì‹¤íŒ¨: {msg_id.decode()}")
                            # Message will be retried (not ACKed)
                    
                    except Exception as e:
                        logger.error(f"âŒ [Stream] ë©”ì‹œì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        
        except KeyboardInterrupt:
            logger.info("ğŸ›‘ [Stream] Consumer ì¤‘ë‹¨ë¨ (KeyboardInterrupt)")
            break
        except Exception as e:
            logger.error(f"âŒ [Stream] Consumer ì˜¤ë¥˜: {e}")
            import time
            time.sleep(1)  # Backoff on error
    
    logger.info(f"âœ… [Stream] Consumer ì¢…ë£Œ: ì´ {total_processed}ê°œ ì²˜ë¦¬")
    return total_processed


# ==============================================================================
# Utility Functions
# ==============================================================================

def get_stream_length(stream_name: str = STREAM_NEWS_RAW) -> int:
    """ìŠ¤íŠ¸ë¦¼ì˜ í˜„ì¬ ê¸¸ì´ (ë°±ë¡œê·¸) ì¡°íšŒ"""
    client = get_redis_client()
    return client.xlen(stream_name)


def get_pending_count(
    stream_name: str,
    group_name: str
) -> int:
    """ì²˜ë¦¬ ëŒ€ê¸° ì¤‘ì¸ ë©”ì‹œì§€ ìˆ˜ ì¡°íšŒ"""
    client = get_redis_client()
    try:
        pending = client.xpending(stream_name, group_name)
        return pending.get("pending", 0) if pending else 0
    except Exception:
        return 0


def trim_stream(
    stream_name: str = STREAM_NEWS_RAW,
    maxlen: int = 100000
) -> int:
    """
    ìŠ¤íŠ¸ë¦¼ì„ ì§€ì •ëœ ê¸¸ì´ë¡œ íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤.
    
    Returns:
        ì‚­ì œëœ ë©”ì‹œì§€ ìˆ˜
    """
    client = get_redis_client()
    before = client.xlen(stream_name)
    client.xtrim(stream_name, maxlen=maxlen, approximate=True)
    after = client.xlen(stream_name)
    trimmed = before - after
    if trimmed > 0:
        logger.info(f"ğŸ§¹ [Stream] íŠ¸ë¦¬ë° ì™„ë£Œ: {trimmed}ê°œ ì‚­ì œ ({stream_name})")
    return trimmed


# ==============================================================================
# News Deduplicator (Redis SET ê¸°ë°˜ ì˜ì† ì¤‘ë³µ ì²´í¬)
# ==============================================================================


def compute_news_hash(text: str) -> str:
    """ë‰´ìŠ¤ í…ìŠ¤íŠ¸ì—ì„œ ì¤‘ë³µ ì²´í¬ìš© í•´ì‹œ ìƒì„± (ì •ê·œí™” í›„ MD5 12ìë¦¬)"""
    normalized = re.sub(r'[^\w]', '', text.lower())
    return hashlib.md5(normalized.encode()).hexdigest()[:12]


class NewsDeduplicator:
    """
    Redis SET ê¸°ë°˜ ë‰´ìŠ¤ ì¤‘ë³µ ì²´í¬.
    ë‚ ì§œë³„ SET í‚¤(dedup:news:YYYYMMDD)ë¥¼ ì‚¬ìš©í•˜ì—¬ ìë™ ë§Œë£Œ.
    ì»¨í…Œì´ë„ˆ ì¬ì‹œì‘ì—ë„ ì˜ì†ë¨.
    """

    DEDUP_TTL_SECONDS = 3 * 86400  # 3ì¼

    def __init__(self, prefix: str = "dedup:news"):
        self._prefix = prefix

    def _today_key(self) -> bytes:
        date_str = datetime.now(timezone.utc).strftime('%Y%m%d')
        return f"{self._prefix}:{date_str}".encode()

    def _recent_keys(self, days: int = 3) -> List[bytes]:
        today = datetime.now(timezone.utc)
        return [
            f"{self._prefix}:{(today - timedelta(days=d)).strftime('%Y%m%d')}".encode()
            for d in range(days)
        ]

    def is_seen(self, news_hash: str) -> bool:
        """ìµœê·¼ 3ì¼ ë‚´ ë™ì¼ í•´ì‹œê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸"""
        try:
            client = get_redis_client()
            hash_bytes = news_hash.encode()
            pipe = client.pipeline()
            for key in self._recent_keys():
                pipe.sismember(key, hash_bytes)
            return any(pipe.execute())
        except Exception as e:
            logger.warning(f"[Dedup] Redis ì¡°íšŒ ì‹¤íŒ¨, ì‹ ê·œë¡œ ì²˜ë¦¬: {e}")
            return False

    def mark_seen(self, news_hash: str):
        """ì˜¤ëŠ˜ ë‚ ì§œ SETì— í•´ì‹œ ì¶”ê°€"""
        try:
            client = get_redis_client()
            today_key = self._today_key()
            pipe = client.pipeline()
            pipe.sadd(today_key, news_hash.encode())
            pipe.expire(today_key, self.DEDUP_TTL_SECONDS)
            pipe.execute()
        except Exception as e:
            logger.warning(f"[Dedup] Redis ì €ì¥ ì‹¤íŒ¨: {e}")

    def check_and_mark(self, news_hash: str) -> bool:
        """
        ì¤‘ë³µ ì²´í¬ í›„ ì‹ ê·œë©´ ë§ˆí‚¹.
        Returns: True = ì‹ ê·œ (ì²˜ìŒ ë³¸ ë‰´ìŠ¤), False = ì¤‘ë³µ
        """
        if self.is_seen(news_hash):
            return False
        self.mark_seen(news_hash)
        return True

    def filter_new_batch(self, hashes: List[str]) -> Set[str]:
        """
        ë°°ì¹˜ ì¤‘ë³µ ì²´í¬. ì‹ ê·œ í•´ì‹œ set ë°˜í™˜ + ë§ˆí‚¹.
        Redis ì‹¤íŒ¨ ì‹œ ì „ì²´ë¥¼ ì‹ ê·œë¡œ ì²˜ë¦¬ (ì•ˆì „í•œ í´ë°±).
        """
        if not hashes:
            return set()

        try:
            client = get_redis_client()
            recent_keys = self._recent_keys()
            n_keys = len(recent_keys)

            # Pipelineìœ¼ë¡œ ì¼ê´„ ì¡°íšŒ
            pipe = client.pipeline()
            for h in hashes:
                h_bytes = h.encode()
                for key in recent_keys:
                    pipe.sismember(key, h_bytes)
            results = pipe.execute()

            # ê²°ê³¼ íŒŒì‹±: ê° hashì— ëŒ€í•´ 3ì¼ ì¤‘ í•˜ë‚˜ë¼ë„ Trueë©´ ì¤‘ë³µ
            new_hashes = set()
            for i, h in enumerate(hashes):
                seen = any(results[i * n_keys + j] for j in range(n_keys))
                if not seen:
                    new_hashes.add(h)

            # ì‹ ê·œ í•´ì‹œë§Œ ì˜¤ëŠ˜ SETì— ë§ˆí‚¹
            if new_hashes:
                today_key = self._today_key()
                pipe = client.pipeline()
                for h in new_hashes:
                    pipe.sadd(today_key, h.encode())
                pipe.expire(today_key, self.DEDUP_TTL_SECONDS)
                pipe.execute()

            return new_hashes
        except Exception as e:
            logger.warning(f"[Dedup] ë°°ì¹˜ ì²´í¬ ì‹¤íŒ¨, ì „ì²´ ì‹ ê·œ ì²˜ë¦¬: {e}")
            return set(hashes)


# ==============================================================================
# Module Init
# ==============================================================================

def init_streams():
    """ëª¨ë“  ìŠ¤íŠ¸ë¦¼ê³¼ Consumer Groupì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    ensure_consumer_group(STREAM_NEWS_RAW, GROUP_ARCHIVER)
    ensure_consumer_group(STREAM_NEWS_RAW, GROUP_ANALYZER)
    logger.info("âœ… [Stream] ëª¨ë“  ìŠ¤íŠ¸ë¦¼ ì´ˆê¸°í™” ì™„ë£Œ")


if __name__ == "__main__":
    # Quick test
    logging.basicConfig(level=logging.DEBUG)
    init_streams()
    print(f"Stream Length: {get_stream_length()}")
