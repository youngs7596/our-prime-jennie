#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
shared/messaging/stream_client.py
---------------------------------
Redis Streams ê¸°ë°˜ ë©”ì‹œì§€ ë¸Œë¡œì»¤ í´ë¼ì´ì–¸íŠ¸.
ë‰´ìŠ¤ íŒŒì´í”„ë¼ì¸(Collector -> Archiver/Analyzer)ì˜ Pub/Sub í†µì‹ ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.
"""

import os
import json
import logging
from datetime import datetime, timezone
from typing import Dict, List, Optional, Any, Callable

import redis

logger = logging.getLogger(__name__)

# ==============================================================================
# Configuration
# ==============================================================================

REDIS_URL = os.getenv("REDIS_URL", "redis://10.178.0.2:6379/0")

# Stream Names
STREAM_NEWS_RAW = "stream:news:raw"  # Collector -> Archiver/Analyzer
STREAM_MACRO_RAW = "stream:macro:raw"  # TelegramCollector -> MacroAnalyzer

# Consumer Groups
GROUP_ARCHIVER = "group_archiver"
GROUP_ANALYZER = "group_analyzer"
GROUP_MACRO_ANALYZER = "group_macro_analyzer"

# Default Settings
DEFAULT_BLOCK_MS = 2000  # 2 seconds
DEFAULT_BATCH_SIZE = 10


# ==============================================================================
# Redis Client Singleton
# ==============================================================================

_redis_client: Optional[redis.Redis] = None


def get_redis_client() -> redis.Redis:
    """Redis í´ë¼ì´ì–¸íŠ¸ ì‹±ê¸€í†¤ ë°˜í™˜"""
    global _redis_client
    if _redis_client is None:
        _redis_client = redis.from_url(REDIS_URL, decode_responses=False)
        logger.info(f"âœ… Redis ì—°ê²° ì™„ë£Œ: {REDIS_URL}")
    return _redis_client


# ==============================================================================
# Producer Functions (Collector)
# ==============================================================================

def publish_news(
    page_content: str,
    metadata: Dict[str, Any],
    stream_name: str = STREAM_NEWS_RAW
) -> str:
    """
    ë‰´ìŠ¤ ë©”ì‹œì§€ë¥¼ Redis Streamì— ë°œí–‰í•©ë‹ˆë‹¤.
    
    Args:
        page_content: ë‰´ìŠ¤ ë³¸ë¬¸
        metadata: ë©”íƒ€ë°ì´í„° (stock_code, source_url, created_at_utc ë“±)
        stream_name: ëŒ€ìƒ ìŠ¤íŠ¸ë¦¼ ì´ë¦„
    
    Returns:
        ë°œí–‰ëœ ë©”ì‹œì§€ ID
    """
    client = get_redis_client()
    
    # Serialize metadata to JSON (handles datetime, etc.)
    message = {
        b"page_content": page_content.encode("utf-8"),
        b"metadata": json.dumps(metadata, default=str).encode("utf-8"),
        b"published_at": datetime.now(timezone.utc).isoformat().encode("utf-8"),
    }
    
    msg_id = client.xadd(stream_name, message)
    logger.debug(f"âœ… [Stream] ë°œí–‰ ì™„ë£Œ: {msg_id.decode()} -> {stream_name}")
    return msg_id.decode() if isinstance(msg_id, bytes) else msg_id


def publish_news_batch(
    documents: List[Dict[str, Any]],
    stream_name: str = STREAM_NEWS_RAW
) -> int:
    """
    ì—¬ëŸ¬ ë‰´ìŠ¤ë¥¼ í•œ ë²ˆì— ë°œí–‰í•©ë‹ˆë‹¤.
    
    Args:
        documents: [{"page_content": str, "metadata": dict}, ...]
        stream_name: ëŒ€ìƒ ìŠ¤íŠ¸ë¦¼ ì´ë¦„
    
    Returns:
        ë°œí–‰ëœ ë©”ì‹œì§€ ìˆ˜
    """
    client = get_redis_client()
    pipeline = client.pipeline()
    
    for doc in documents:
        message = {
            b"page_content": doc["page_content"].encode("utf-8"),
            b"metadata": json.dumps(doc["metadata"], default=str).encode("utf-8"),
            b"published_at": datetime.now(timezone.utc).isoformat().encode("utf-8"),
        }
        pipeline.xadd(stream_name, message)
    
    results = pipeline.execute()
    published = sum(1 for r in results if r)
    logger.info(f"âœ… [Stream] ë°°ì¹˜ ë°œí–‰ ì™„ë£Œ: {published}/{len(documents)}ê°œ -> {stream_name}")
    return published


# ==============================================================================
# Consumer Functions (Archiver, Analyzer)
# ==============================================================================

def ensure_consumer_group(
    stream_name: str = STREAM_NEWS_RAW,
    group_name: str = GROUP_ARCHIVER,
    reset_cursor: bool = False
) -> bool:
    """
    Consumer Groupì´ ì—†ìœ¼ë©´ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        reset_cursor: Trueì´ë©´ ê·¸ë£¹ì´ ì´ë¯¸ ì¡´ì¬í•´ë„ ì»¤ì„œë¥¼ ì²˜ìŒ("0")ìœ¼ë¡œ ë¦¬ì…‹
    
    Returns:
        ìƒì„± ì„±ê³µ ì—¬ë¶€ (ì´ë¯¸ ì¡´ì¬í•˜ë©´ True)
    """
    client = get_redis_client()
    try:
        # Create group starting from the beginning ('0')
        client.xgroup_create(stream_name, group_name, id="0", mkstream=True)
        logger.info(f"âœ… [Stream] Consumer Group ìƒì„±: {group_name} @ {stream_name}")
        return True
    except redis.ResponseError as e:
        if "BUSYGROUP" in str(e):
            logger.info(f"â„¹ï¸ [Stream] Consumer Group ì´ë¯¸ ì¡´ì¬: {group_name}")
            if reset_cursor:
                # ê¸°ì¡´ ê·¸ë£¹ì˜ ì»¤ì„œë¥¼ ì²˜ìŒìœ¼ë¡œ ë¦¬ì…‹ (ëª¨ë“  ë©”ì‹œì§€ ì¬ì²˜ë¦¬)
                client.xgroup_setid(stream_name, group_name, id="0")
                logger.info(f"ğŸ”„ [Stream] Consumer Group ì»¤ì„œ ë¦¬ì…‹: {group_name} -> 0")
            return True
        raise


def consume_messages(
    group_name: str,
    consumer_name: str,
    handler: Callable[[str, Dict[str, Any]], bool],
    stream_name: str = STREAM_NEWS_RAW,
    batch_size: int = DEFAULT_BATCH_SIZE,
    block_ms: int = DEFAULT_BLOCK_MS,
    max_iterations: Optional[int] = None
) -> int:
    """
    Consumer Group ë°©ì‹ìœ¼ë¡œ ë©”ì‹œì§€ë¥¼ ì†Œë¹„í•©ë‹ˆë‹¤.
    
    Args:
        group_name: Consumer Group ì´ë¦„
        consumer_name: ì´ Consumerì˜ ê³ ìœ  ì´ë¦„
        handler: ë©”ì‹œì§€ ì²˜ë¦¬ í•¨ìˆ˜ (page_content, metadata) -> success
        stream_name: ìŠ¤íŠ¸ë¦¼ ì´ë¦„
        batch_size: í•œ ë²ˆì— ì½ì„ ë©”ì‹œì§€ ìˆ˜
        block_ms: ë©”ì‹œì§€ê°€ ì—†ì„ ë•Œ ëŒ€ê¸° ì‹œê°„ (ms)
        max_iterations: ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ (None=ë¬´í•œ)
    
    Returns:
        ì²˜ë¦¬í•œ ë©”ì‹œì§€ ì´ ìˆ˜
    """
    client = get_redis_client()
    ensure_consumer_group(stream_name, group_name)
    
    total_processed = 0
    iteration = 0
    
    logger.info(f"ğŸš€ [Stream] Consumer ì‹œì‘: {consumer_name} @ {group_name}/{stream_name}")
    
    while True:
        if max_iterations and iteration >= max_iterations:
            break
        iteration += 1
        
        try:
            # Read new messages
            messages = client.xreadgroup(
                group_name,
                consumer_name,
                {stream_name: ">"},
                count=batch_size,
                block=block_ms
            )
            
            if not messages:
                logger.debug(f"[Stream] ìƒˆ ë©”ì‹œì§€ ì—†ìŒ, ëŒ€ê¸° ì¤‘... (iteration={iteration})")
                continue
            
            for stream, msg_list in messages:
                for msg_id, msg_data in msg_list:
                    try:
                        # Deserialize
                        page_content = msg_data[b"page_content"].decode("utf-8")
                        metadata = json.loads(msg_data[b"metadata"].decode("utf-8"))
                        
                        # Call handler
                        success = handler(page_content, metadata)
                        
                        if success:
                            # ACK
                            client.xack(stream_name, group_name, msg_id)
                            total_processed += 1
                            logger.debug(f"âœ… [Stream] ì²˜ë¦¬ ì™„ë£Œ: {msg_id.decode()}")
                        else:
                            logger.warning(f"âš ï¸ [Stream] í•¸ë“¤ëŸ¬ ì‹¤íŒ¨: {msg_id.decode()}")
                            # Message will be retried (not ACKed)
                    
                    except Exception as e:
                        logger.error(f"âŒ [Stream] ë©”ì‹œì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        
        except KeyboardInterrupt:
            logger.info("ğŸ›‘ [Stream] Consumer ì¤‘ë‹¨ë¨ (KeyboardInterrupt)")
            break
        except Exception as e:
            logger.error(f"âŒ [Stream] Consumer ì˜¤ë¥˜: {e}")
            import time
            time.sleep(1)  # Backoff on error
    
    logger.info(f"âœ… [Stream] Consumer ì¢…ë£Œ: ì´ {total_processed}ê°œ ì²˜ë¦¬")
    return total_processed


# ==============================================================================
# Utility Functions
# ==============================================================================

def get_stream_length(stream_name: str = STREAM_NEWS_RAW) -> int:
    """ìŠ¤íŠ¸ë¦¼ì˜ í˜„ì¬ ê¸¸ì´ (ë°±ë¡œê·¸) ì¡°íšŒ"""
    client = get_redis_client()
    return client.xlen(stream_name)


def get_pending_count(
    stream_name: str,
    group_name: str
) -> int:
    """ì²˜ë¦¬ ëŒ€ê¸° ì¤‘ì¸ ë©”ì‹œì§€ ìˆ˜ ì¡°íšŒ"""
    client = get_redis_client()
    try:
        pending = client.xpending(stream_name, group_name)
        return pending.get("pending", 0) if pending else 0
    except Exception:
        return 0


def trim_stream(
    stream_name: str = STREAM_NEWS_RAW,
    maxlen: int = 100000
) -> int:
    """
    ìŠ¤íŠ¸ë¦¼ì„ ì§€ì •ëœ ê¸¸ì´ë¡œ íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤.
    
    Returns:
        ì‚­ì œëœ ë©”ì‹œì§€ ìˆ˜
    """
    client = get_redis_client()
    before = client.xlen(stream_name)
    client.xtrim(stream_name, maxlen=maxlen, approximate=True)
    after = client.xlen(stream_name)
    trimmed = before - after
    if trimmed > 0:
        logger.info(f"ğŸ§¹ [Stream] íŠ¸ë¦¬ë° ì™„ë£Œ: {trimmed}ê°œ ì‚­ì œ ({stream_name})")
    return trimmed


# ==============================================================================
# Module Init
# ==============================================================================

def init_streams():
    """ëª¨ë“  ìŠ¤íŠ¸ë¦¼ê³¼ Consumer Groupì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    ensure_consumer_group(STREAM_NEWS_RAW, GROUP_ARCHIVER)
    ensure_consumer_group(STREAM_NEWS_RAW, GROUP_ANALYZER)
    logger.info("âœ… [Stream] ëª¨ë“  ìŠ¤íŠ¸ë¦¼ ì´ˆê¸°í™” ì™„ë£Œ")


if __name__ == "__main__":
    # Quick test
    logging.basicConfig(level=logging.DEBUG)
    init_streams()
    print(f"Stream Length: {get_stream_length()}")
