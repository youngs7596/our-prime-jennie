
# Version: v1.0 (my-prime-jennie) - LLM ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ëª¨ë“ˆ
# Jennie Brain Module - LLM Orchestrator
#
# Roles:
# 1. Sentiment: News Sentiment Analysis (FAST Tier)
# 2. Hunter: Stock Analysis (REASONING Tier)
# 3. Judge: Final Decision (THINKING Tier)

import os
import logging
import json
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Union

# Factory & Enum
from shared.llm_factory import LLMFactory, LLMTier
import shared.database as database
import shared.auth as auth

# Imports from shared modules
from shared.llm_prompts import (
    build_buy_prompt_mean_reversion,
    build_buy_prompt_golden_cross, # Used as build_buy_prompt_trend_following
    build_sell_prompt, # Used as build_sell_decision_prompt
    build_news_sentiment_prompt,
    build_debate_prompt,
    build_judge_prompt, # V4 Judge
    build_hunter_prompt_v5,
    build_judge_prompt_v5
)

from shared.llm_constants import ANALYSIS_RESPONSE_SCHEMA
# Alias for compatibility if needed, or just use ANALYSIS_RESPONSE_SCHEMA
JUDGE_RESPONSE_SCHEMA = ANALYSIS_RESPONSE_SCHEMA

# "youngs75_jennie.llm" ì´ë¦„ìœ¼ë¡œ ë¡œê±° ìƒì„±
logger = logging.getLogger(__name__)

class JennieBrain:
    """
    LLMì„ ì‚¬ìš©í•˜ì—¬ 'BUY' ë˜ëŠ” 'SELL' ì‹ í˜¸ì— ëŒ€í•œ ìµœì¢… ê²°ì¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    LLM Factory Pattern ë„ì… - Hybrid Strategy (Local/Cloud)
    """
    
    def __init__(self, project_id=None, gemini_api_key_secret=None):
        # Factoryë¥¼ í†µí•´ ProviderëŠ” í•„ìš”í•  ë•Œ ë™ì ìœ¼ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤.
        # ê¸°ì¡´ __init__ì—ì„œì˜ ë³µì¡í•œ ì´ˆê¸°í™”ëŠ” ì œê±°í•˜ê³ , Factoryì— ìœ„ì„í•©ë‹ˆë‹¤.
        logger.info("--- [JennieBrain] v6.0 Initialized (Factory Pattern) ---")
        
        # ë ˆê±°ì‹œ í˜¸í™˜ì„±ì„ ìœ„í•´ í•„ë“œë§Œ ë‚¨ê²¨ë‘  (ì‹¤ì œë¡œëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
        self.provider_gemini = None 
        self.provider_claude = None
        self.provider_openai = None

    def _get_provider(self, tier: LLMTier):
        """Helper to get provider from Factory with error handling"""
        try:
            return LLMFactory.get_provider(tier)
        except Exception as e:
            logger.error(f"âŒ [JennieBrain] Provider ë¡œë“œ ì‹¤íŒ¨ ({tier}): {e}")
            return None

    def _calculate_grade(self, score: float) -> str:
        """
        ì •í™•í•œ ë“±ê¸‰ ì‚°ì •ì„ ìœ„í•œ ì½”ë“œ ë¡œì§
        LLMì˜ í• ë£¨ì‹œë„¤ì´ì…˜(ì ìˆ˜-ë“±ê¸‰ ë¶ˆì¼ì¹˜) ë°©ì§€
        """
        try:
            score = float(score)
        except (ValueError, TypeError):
            return 'D' # Default on error
            
        if score >= 80: return 'S'
        elif score >= 70: return 'A'
        elif score >= 60: return 'B'
        elif score >= 50: return 'C'
        elif score >= 40: return 'D'
        else: return 'F'

    # -----------------------------------------------------------------
    # 'ì œë‹ˆ' ê²°ì¬ ì‹¤í–‰
    # -----------------------------------------------------------------
    def get_jennies_decision(self, trade_type, stock_info, **kwargs):
        """
        LLMì„ í˜¸ì¶œí•˜ì—¬ ìµœì¢… ê²°ì¬ë¥¼ ë°›ìŠµë‹ˆë‹¤.
        Trade Decision = Critical Task -> THINKING Tier
        """
        provider = self._get_provider(LLMTier.THINKING)
        if provider is None:
            return {"decision": "REJECT", "reason": "JennieBrain ì´ˆê¸°í™” ì‹¤íŒ¨ (Thinking Tier)", "quantity": 0}

        try:
            if trade_type == 'BUY_MR':
                buy_signal_type = kwargs.get('buy_signal_type', 'UNKNOWN')
                prompt = build_buy_prompt_mean_reversion(stock_info, buy_signal_type)
            elif trade_type == 'BUY_TREND':
                buy_signal_type = kwargs.get('buy_signal_type', 'GOLDEN_CROSS')
                # Use aliased function
                prompt = build_buy_prompt_golden_cross(stock_info, buy_signal_type)
            elif trade_type == 'SELL':
                market_status = kwargs.get('market_status', 'N/A') # build_sell_prompt expects stock_info mainly
                # build_sell_prompt signature: (stock_info). market_status implies prompt builder change?
                # Assuming build_sell_prompt only takes stock_info for now based on outline.
                prompt = build_sell_prompt(stock_info)
            else:
                return {"decision": "REJECT", "reason": "ì•Œ ìˆ˜ ì—†ëŠ” ê±°ë˜ ìœ í˜•", "quantity": 0}

            logger.info(f"--- [JennieBrain] ê²°ì¬ ìš”ì²­ ({trade_type}) via {provider.name} ---")
            
            # JSON ì‘ë‹µ ìŠ¤í‚¤ë§ˆ
            DECISION_SCHEMA = {
                "type": "object",
                "properties": {
                    "decision": {"type": "string", "enum": ["APPROVE", "REJECT", "HOLD"]},
                    "reason": {"type": "string"},
                    "quantity": {"type": "integer"}
                },
                "required": ["decision", "reason", "quantity"]
            }

            result = provider.generate_json(
                prompt,
                DECISION_SCHEMA,
                temperature=0.1
            )
            
            logger.info(f"   ğŸ‘‘ ì œë‹ˆì˜ ê²°ì¬: {result.get('decision')} ({result.get('reason')})")
            return result

        except Exception as e:
            logger.error(f"âŒ [JennieBrain] ê²°ì¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return {"decision": "REJECT", "reason": f"System Error: {e}", "quantity": 0}

    # -----------------------------------------------------------------
    # ë‰´ìŠ¤ ê°ì„± ë¶„ì„
    # -----------------------------------------------------------------
    def analyze_news_sentiment(self, title, description):
        """
        ë‰´ìŠ¤ ì œëª©ê³¼ ìš”ì•½ì„ ë¶„ì„í•˜ì—¬ ê¸ì •/ë¶€ì • ì ìˆ˜ë¥¼ ë§¤ê¹ë‹ˆë‹¤.
        High Volume / Low Risk -> FAST Tier (Local LLM)
        [2026-01] Cloud Fallback ì œê±° - Local LLM ì „ìš© (ë¹„ìš© â‚©0)
        """
        provider = self._get_provider(LLMTier.FAST)
        if provider is None:
             return {'score': 50, 'reason': 'ëª¨ë¸ ë¯¸ì´ˆê¸°í™” (ê¸°ë³¸ê°’)'}

        try:
            # build_news_sentiment_prompt args: news_title, news_summary
            prompt = build_news_sentiment_prompt(title, description)
            
            # [Optimization] Use Flash model for FAST tier if available (e.g. Gemini 2.5 Flash)
            model_name = None
            if hasattr(provider, 'flash_model_name'):
                 model_name = provider.flash_model_name()

            result = provider.generate_json(
                prompt,
                ANALYSIS_RESPONSE_SCHEMA,
                temperature=0.0, # Deterministic
                model_name=model_name
            )
            return result
        except Exception as e:
            # [2026-01] Cloud Fallback ì œê±° - Local LLM ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
            logger.warning(f"âš ï¸ [News] Local LLM ë¶„ì„ ì‹¤íŒ¨ (Skip): {e}")
            return {'score': 50, 'reason': f'Local LLM ë¶„ì„ ì‹¤íŒ¨: {str(e)[:50]}'}

    def analyze_news_sentiment_batch(self, items: list) -> list:
        """
        ë‰´ìŠ¤ ë°°ì¹˜ ë¶„ì„ (ë‹¤ê±´ ì²˜ë¦¬ë¡œ ì†ë„ í–¥ìƒ)
        [2026-01] gpt-oss:20b + ë°°ì¹˜ ì²˜ë¦¬ ë„ì…
        
        Args:
            items: List of dicts with 'id', 'title', 'summary' keys
        Returns:
            List of dicts with 'id', 'score', 'reason' keys
        """
        provider = self._get_provider(LLMTier.FAST)
        if provider is None:
            return [{'id': item['id'], 'score': 50, 'reason': 'ëª¨ë¸ ë¯¸ì´ˆê¸°í™”'} for item in items]

        # Build batched prompt
        items_text = ""
        for item in items:
            items_text += f"""
        [ID: {item['id']}]
        - ì œëª©: {item['title']}
        - ë‚´ìš©: {item['summary']}
        """
        
        prompt = f"""
        [ê¸ˆìœµ ë‰´ìŠ¤ ë‹¤ê±´ ê°ì„± ë¶„ì„]
        ë‹¹ì‹ ì€ 'ê¸ˆìœµ ì „ë¬¸ê°€'ì…ë‹ˆë‹¤. ì•„ë˜ {len(items)}ê°œì˜ ë‰´ìŠ¤ë¥¼ ê°ê° ë¶„ì„í•˜ì—¬ í˜¸ì¬/ì•…ì¬ ì ìˆ˜ë¥¼ ë§¤ê¸°ì„¸ìš”.
        
        [ì¤‘ìš”] ë°˜ë“œì‹œ í•œêµ­ì–´(Korean)ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”. ì˜ì–´ë¡œ ì¶œë ¥í•˜ë©´ ì•ˆ ë©ë‹ˆë‹¤.
        
        {items_text}
        
        [ì±„ì  ê¸°ì¤€]
        - 80 ~ 100ì : ê°•ë ¥ í˜¸ì¬
        - 60 ~ 79ì : í˜¸ì¬
        - 40 ~ 59ì : ì¤‘ë¦½
        - 20 ~ 39ì : ì•…ì¬
        - 0 ~ 19ì : ê°•ë ¥ ì•…ì¬
        
        [ì¶œë ¥ í˜•ì‹]
        ë°˜ë“œì‹œ ì•„ë˜ì™€ ê°™ì€ JSON ê°ì²´ë¡œ ì‘ë‹µí•˜ì„¸ìš”. IDëŠ” ì…ë ¥ëœ ê²ƒê³¼ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.
        {{
            "results": [
                {{ "id": 0, "score": 85, "reason": "íŒë‹¨ ì´ìœ (í•œêµ­ì–´)" }},
                {{ "id": 1, "score": 30, "reason": "íŒë‹¨ ì´ìœ (í•œêµ­ì–´)" }},
                ...
            ]
        }}
        """
        
        BATCH_SCHEMA = {
            "type": "object",
            "properties": {
                "results": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "id": {"type": "integer"},
                            "score": {"type": "integer"},
                            "reason": {"type": "string"}
                        },
                        "required": ["id", "score", "reason"]
                    }
                }
            },
            "required": ["results"]
        }

        try:
            result = provider.generate_json(prompt, BATCH_SCHEMA, temperature=0.0)
            return result.get('results', [])
        except Exception as e:
            logger.warning(f"âš ï¸ [News Batch] ë°°ì¹˜ ë¶„ì„ ì‹¤íŒ¨: {e}")
            # Fallback: ê¸°ë³¸ê°’ ë°˜í™˜
            return [{'id': item['id'], 'score': 50, 'reason': 'ë°°ì¹˜ ë¶„ì„ ì‹¤íŒ¨'} for item in items]


    # -----------------------------------------------------------------
    # í† ë¡  (Bull vs Bear)
    # -----------------------------------------------------------------
    def run_debate_session(self, stock_info: dict, analysis_context: str = "", hunter_score: int = 0) -> str:
        """
        Bull vs Bear í† ë¡  ìƒì„± (Dynamic Role Allocation)
        Complex Creative Task -> REASONING Tier
        """
        provider = self._get_provider(LLMTier.REASONING)
        if provider is None:
             return "Debate Skipped (Model Error)"

        try:
            # Extract keywords for Dynamic Debate Context
            keywords = stock_info.get('dominant_keywords', [])
            
            # Pass hunter_score and keywords to build_debate_prompt
            prompt = build_debate_prompt(stock_info, hunter_score, keywords) 
            
            # generate_chat requires list of dicts, not str
            chat_history = [{"role": "user", "content": prompt}]
            
            logger.info(f"--- [JennieBrain/Debate] í† ë¡  ì‹œì‘ via {provider.name} (HunterScore: {hunter_score}, KW: {keywords}) ---")
            
            result = provider.generate_chat(chat_history, temperature=0.7)
            # If result is dict (e.g. from structured output), extracting text. 
            # generate_chat usually returns dict with 'text' or json. 
            # But the caller expects str. 
            if isinstance(result, dict):
                return result.get('text') or result.get('content') or str(result)
            return str(result)

        except Exception as e:
            logger.warning(f"âš ï¸ [Debate] Local LLM failed: {e}. Attempting Cloud Fallback (Tier-Adaptive)...")
            try:
                fallback_provider = LLMFactory.get_fallback_provider(LLMTier.REASONING)
                if fallback_provider is None:
                    raise ValueError("No fallback provider for REASONING tier")

                logger.info(f"--- [JennieBrain/Debate] Cloud Fallback via {fallback_provider.name} ---")
                chat_history = [{"role": "user", "content": prompt}]
                result = fallback_provider.generate_chat(chat_history, temperature=0.7)
                
                if isinstance(result, dict):
                    return result.get('text') or result.get('content') or str(result)
                return str(result)
            except Exception as fb_e:
                logger.error(f"âŒ [Debate] Fallback failed: {fb_e}")
                return f"Debate Error: {e}"

    # -----------------------------------------------------------------
    # Check if stock exists (Optional)
    # -----------------------------------------------------------------
    def verify_parameter_change(self, stock_info: dict, param_name: str, old_val, new_val) -> dict:
        # Simple task -> FAST Tier
        provider = self._get_provider(LLMTier.FAST)
        if not provider: return {"authorized": False}
        return {"authorized": True, "reason": "Auto-approved by FAST tier"}

    # -----------------------------------------------------------------
    # Judge (Supreme Jennie) ìµœì¢… íŒê²°
    # -----------------------------------------------------------------
    def run_judge_scoring(self, stock_info: dict, debate_log: str) -> dict:
        """
        Judge Scoring = Critical Decision -> THINKING Tier
        """
        provider = self._get_provider(LLMTier.THINKING)
        if provider is None:
             return {'score': 0, 'grade': 'D', 'reason': 'Provider Error'}

        try:
            prompt = build_judge_prompt(stock_info, debate_log)
            logger.info(f"--- [JennieBrain/Judge] íŒê²° via {provider.name} ---")
            
            result = provider.generate_json(
                prompt,
                JUDGE_RESPONSE_SCHEMA, 
                temperature=0.1
            )
            return result
        except Exception as e:
            logger.error(f"âŒ [Judge] íŒê²° ì‹¤íŒ¨: {e}")
            return {'score': 0, 'grade': 'D', 'reason': f"ì˜¤ë¥˜: {e}"}

    # -----------------------------------------------------------------
    # Scout Hybrid Scoring
    # -----------------------------------------------------------------
    def get_jennies_analysis_score_v5(self, stock_info: dict, quant_context: str = None, feedback_context: str = None) -> dict:
        """
        v5 Hunter = Reasoning Task -> REASONING Tier
        """
        provider = self._get_provider(LLMTier.REASONING)
        if provider is None:
            return {'score': 0, 'grade': 'D', 'reason': 'Provider Error'}
        
        try:
            prompt = build_hunter_prompt_v5(stock_info, quant_context, feedback_context)
            logger.info(f"--- [JennieBrain/v5-Hunter] ë¶„ì„ via {provider.name} ---")
            
            result = provider.generate_json(
                prompt,
                ANALYSIS_RESPONSE_SCHEMA,
                temperature=0.2
            )
            logger.info(f"   âœ… v5 Hunter ì™„ë£Œ: {stock_info.get('name')} - {result.get('score')}ì ")
            return result
        except Exception as e:
            logger.warning(f"âš ï¸ [Hunter] Local LLM failed: {e}. Attempting Cloud Fallback (Tier-Adaptive)...")
            try:
                fallback_provider = LLMFactory.get_fallback_provider(LLMTier.REASONING)
                if fallback_provider is None:
                    raise ValueError("No fallback provider for REASONING tier")

                logger.info(f"--- [JennieBrain/v5-Hunter] Cloud Fallback via {fallback_provider.name} ---")
                result = fallback_provider.generate_json(
                    prompt,
                    ANALYSIS_RESPONSE_SCHEMA,
                    temperature=0.2
                )
                return result
            except Exception as fb_e:
                logger.error(f"âŒ [Hunter] Fallback failed: {fb_e}")
                return {'score': 0, 'grade': 'D', 'reason': f"ì˜¤ë¥˜(Local+Cloud): {e}"}

    def run_judge_scoring_v5(self, stock_info: dict, debate_log: str, quant_context: str = None, feedback_context: str = None) -> dict:
        """
        v5 Judge = Critical Decision -> THINKING Tier
        [Strategy Gate]: Hunter score < 70 (Grade B) will be auto-rejected to save Cloud costs and avoid weak signals.
        """
        # 1. Strategy Gate Check (Junho's Condition)
        hunter_score = stock_info.get('hunter_score', 0)
        # Default Threshold: 70 (B Grade)
        JUDGE_THRESHOLD = 70 
        
        if hunter_score < JUDGE_THRESHOLD:
            logger.info(f"ğŸš« [Gatekeeper] Judge Skipped. Hunter Score {hunter_score} < {JUDGE_THRESHOLD}. Auto-Reject.")
            return {
                'score': hunter_score, 
                'grade': self._calculate_grade(hunter_score), 
                'reason': f"Hunter Score({hunter_score}) failed to meet Judge Threshold({JUDGE_THRESHOLD}). Auto-Rejected."
            }

        provider = self._get_provider(LLMTier.THINKING)
        if provider is None:
            return {'score': 0, 'grade': 'D', 'reason': 'Provider Error'}
            
        try:
            # 2. Structured Logging (Minji's Request)
            call_reason = "HighConviction_Verification"
            logger.info(json.dumps({
                "event": "ThinkingTier_Call",
                "tier": "THINKING",
                "task": "Judge_v5",
                "model": provider.client.__class__.__name__ if hasattr(provider, 'client') else provider.name,
                "reason": call_reason,
                "input_score": hunter_score
            }))
            
            prompt = build_judge_prompt_v5(stock_info, debate_log, quant_context, feedback_context)
            logger.info(f"--- [JennieBrain/v5-Judge] íŒê²° via {provider.name} (Why: {call_reason}) ---")
            
            # [Debug] Log critical inputs for Score Discrepancy Investigation
            logger.info(f"ğŸ” [Judge-Debug] Input Hunter Score: {hunter_score}")
            logger.debug(f"ğŸ” [Judge-Debug] Full Prompt Context: {prompt[:500]}...") # Log start of prompt securely
            
            result = provider.generate_json(
                prompt,
                ANALYSIS_RESPONSE_SCHEMA,
                temperature=0.1
            )
            
            # ë“±ê¸‰ ê°•ì œ ì‚°ì • (LLM ì¶œë ¥ ì˜¤ë²„ë¼ì´ë“œ)
            raw_score = result.get('score', 0)
            calculated_grade = self._calculate_grade(raw_score)
            
            # ë“±ê¸‰ ë¶ˆì¼ì¹˜ ë¡œê¹…
            llm_grade = result.get('grade', 'N/A')
            if llm_grade != calculated_grade and llm_grade != 'N/A':
                logger.warning(f"âš ï¸ [Judge] Grade Mismatch Corrected: LLM({llm_grade}) -> Code({calculated_grade}) for Score {raw_score}")
                
            result['grade'] = calculated_grade
            
            return result
        except Exception as e:
            logger.error(f"âŒ [Judge] ì˜¤ë¥˜: {e}")
            return {'score': 0, 'grade': 'D', 'reason': f"ì˜¤ë¥˜: {e}"}

    # -----------------------------------------------------------------
    # [New] Daily Briefing (Centralized from reporter.py)
    # -----------------------------------------------------------------
    def generate_daily_briefing(self, market_summary: str, execution_log: str) -> str:
        """
        Generate Daily Briefing Report.
        Task Type: REASONING or THINKING (depending on desired quality).
        Let's use THINKING for high quality report.
        """
        provider = self._get_provider(LLMTier.THINKING) 
        if provider is None:
            return "ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: ëª¨ë¸ ì´ˆê¸°í™” ì˜¤ë¥˜"

        # [Prompt Engineering]
        # Role: Jennie (Smart, Friendly, Professional AI Investment Assistant)
        # Goal: Provide a clear, structured, and actionable daily briefing.
        # Constraints: 
        # 1. Use Data: Do NOT make up market conditions. If data is missing (e.g. market_summary is empty), state it clearly.
        # 2. Tone: Friendly, reliable, and encouraging (like a trusted partner).
        # 3. Format: Clean Markdown for Telegram.

        prompt = f"""
        ì•ˆë…•í•˜ì„¸ìš”! ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ë˜‘ë˜‘í•˜ê³  ë‹¤ì •í•­ ì£¼ì‹ íˆ¬ì íŒŒíŠ¸ë„ˆ 'ì œë‹ˆ(Jennie)'ì…ë‹ˆë‹¤.
        ì˜¤ëŠ˜ì˜ ì‹œì¥ ìƒí™©ê³¼ ìë™ë§¤ë§¤ ìˆ˜í–‰ ê¸°ë¡ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìê°€ í‡´ê·¼ê¸¸(ë˜ëŠ” í•˜ë£¨ ë§ˆë¬´ë¦¬)ì— ë³´ê¸° ì¢‹ì€ 'ì¼ì¼ ë¸Œë¦¬í•‘ ë¦¬í¬íŠ¸'ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.

        ---
        [ì…ë ¥ ë°ì´í„°]
        1. ğŸ“Š ì‹œì¥ ìš”ì•½ (Market Summary):
        {market_summary if market_summary and market_summary.strip() else "ì •ë³´ ì—†ìŒ (ìˆ˜ì§‘ëœ ì‹œì¥ ì§€í‘œê°€ ì—†ìŠµë‹ˆë‹¤)"}

        2. ğŸ“œ ì˜¤ëŠ˜ ë§¤ë§¤ ë¡œê·¸ (Execution Log):
        {execution_log if execution_log and execution_log.strip() else "ê±°ë˜ ê¸°ë¡ ì—†ìŒ (ì˜¤ëŠ˜ì€ ë§¤ë§¤ê°€ ë°œìƒí•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤)"}
        ---

        [ì‘ì„± ê°€ì´ë“œë¼ì¸]
        1. **í†¤ì•¤ë§¤ë„ˆ**: 
           - ë‹¤ì •í•˜ê³  ë”°ëœ»í•œ ì–´ì¡° ("~í–ˆì–´ìš”", "~ë³´ì…ë‹ˆë‹¤" ë“±).
           - ì „ë¬¸ì„±ì€ ìœ ì§€í•˜ë˜, ë”±ë”±í•˜ì§€ ì•Šê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”.
           - ì‚¬ìš©ìë¥¼ ê²©ë ¤í•˜ëŠ” ë©˜íŠ¸ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨í•˜ì„¸ìš”.

        2. **êµ¬ì¡° (Telegram Markdown)**:
           # ğŸ“… [YYYY-MM-DD] ì œë‹ˆì˜ ì¼ì¼ ë¸Œë¦¬í•‘

           ## 1. ğŸŒ ì‹œì¥ í˜„í™© (Market Pulse)
           - ì‹œì¥ ìš”ì•½ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì£¼ìš” ì§€ìˆ˜(KOSPI, KOSDAC ë“±)ì˜ íë¦„ê³¼ íŠ¹ì§•ì„ ìš”ì•½í•´ì£¼ì„¸ìš”.
           - *ì£¼ì˜*: ë°ì´í„°ê°€ 'ì •ë³´ ì—†ìŒ'ì´ë¼ë©´, "ì˜¤ëŠ˜ì€ ì‹œì¥ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆì–´ìš”. ğŸ˜¢"ë¼ê³  ì†”ì§í•˜ê²Œ ì ì–´ì£¼ì„¸ìš”. 
           - *ì ˆëŒ€* "íŠ¹ì´ ì‚¬í•­ ì—†ì´ ì•ˆì •ì "ì´ë¼ëŠ” ë©˜íŠ¸ë¥¼ ë°ì´í„° ì—†ì´ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.

           ## 2. ğŸ’¼ í¬íŠ¸í´ë¦¬ì˜¤ ë° ë§¤ë§¤ ë¶„ì„
           - ì˜¤ëŠ˜ ë°œìƒí•œ ë§¤ìˆ˜/ë§¤ë„ ë‚´ì—­ì„ ê°„ëµíˆ ì •ë¦¬í•´ì£¼ì„¸ìš”.
           - ë§¤ë§¤ê°€ ì—†ì—ˆë‹¤ë©´ "ì˜¤ëŠ˜ì€ ì‰¬ì–´ê°€ëŠ” í•˜ë£¨ì˜€ì–´ìš”. í˜„ê¸ˆì„ ë³´ìœ í•˜ë©° ê¸°íšŒë¥¼ ì—¿ë³´ê³  ìˆìŠµë‹ˆë‹¤." ì²˜ëŸ¼ ê¸ì •ì ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
           
           ## 3. ğŸ“° ì£¼ìš” ë‰´ìŠ¤ ë° ì´ìŠˆ (Highlights)
           - ì‹œì¥ ìš”ì•½ì— í¬í•¨ëœ ë‰´ìŠ¤ë‚˜ íŠ¹ì§•ì£¼ê°€ ìˆë‹¤ë©´ 2~3ì¤„ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.
           - ì •ë³´ê°€ ì—†ë‹¤ë©´ ì´ ì„¹ì…˜ì€ ìƒëµí•´ë„ ì¢‹ìŠµë‹ˆë‹¤.

           ## 4. ğŸ’¡ ë‚´ì¼ì˜ íˆ¬ì ì „ëµ (Jennie's Note)
           - í˜„ì¬ ìƒí™©ì„ ë°”íƒ•ìœ¼ë¡œ ê°„ë‹¨í•œ ì¡°ì–¸ì´ë‚˜ ë‹¤ì§ì„ ì ì–´ì£¼ì„¸ìš”.
           - ì˜ˆ: "ë‚´ì¼ë„ ë³€ë™ì„±ì„ ì£¼ì‹œí•˜ë©° ì•ˆì „í•˜ê²Œ ëŒ€ì‘í• ê²Œìš”!"
           
           ## 5. ë§ˆë¬´ë¦¬ ì¸ì‚¬
           - ë”°ëœ»í•œ í•˜ë£¨ ë§ˆë¬´ë¦¬ë¥¼ ìœ„í•œ ì¸ì‚¬ë§.

        3. **ì£¼ì˜ì‚¬í•­**:
           - ì—†ëŠ” ë‚´ìš©ì„ ì§€ì–´ë‚´ì§€ ë§ˆì„¸ìš”.
           - ê¸ì •ì ì´ê³  í¬ë§ì ì¸ ì—ë„ˆì§€ë¥¼ ì „ë‹¬í•˜ì„¸ìš”.
           - ì´ëª¨ì§€(ğŸ“Š, ğŸ“ˆ, ğŸ’°, âœ¨)ë¥¼ ì ì ˆíˆ ì‚¬ìš©í•˜ì—¬ ê°€ë…ì„±ì„ ë†’ì—¬ì£¼ì„¸ìš”.
        """
        try:
            logger.info(f"--- [JennieBrain/Briefing] ë¦¬í¬íŠ¸ ìƒì„± via {provider.name} ---")
            # generate_chat requires list of dicts.
            # Gemini expects 'parts': [{'text': ...}] structure for 'user' role
            # However, BaseLLMProvider.generate_chat implementations (like GeminiLLMProvider) usually handle normalization from standard role/content.
            # Let's check GeminiLLMProvider.generate_chat. It uses history[-1]['parts'][0]['text'] implies it expects 'parts'.
            # But the Provider implementation should ideally handle 'content' -> 'parts' mapping.
            # Looking at logs: "provided dictionary has keys: ['role', 'content']".
            # It seems GeminiLLMProvider passes `history` directly to `model.start_chat(history=history)`?
            # Yes, `chat = model.start_chat(history=history)`.
            # Google GenAI `start_chat` expects standard Google format: role='user'|'model', parts=[{'text': '...'}]
            
            chat_history = [{"role": "user", "parts": [{"text": prompt}]}]
            result = provider.generate_chat(chat_history, temperature=0.7)
            
            if isinstance(result, dict):
                return result.get('text') or result.get('content') or str(result)
            return str(result)
        except Exception as e:
            logger.error(f"âŒ [Briefing] ì‹¤íŒ¨: {e}")
            return "ë³´ê³ ì„œ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    # -----------------------------------------------------------------
    # [New] Strategic Feedback Generator
    # -----------------------------------------------------------------
    def generate_strategic_feedback(self, performance_summary: str) -> str:
        """
        Analyst ì„±ê³¼ ë³´ê³ ì„œë¥¼ ë¶„ì„í•˜ì—¬ 'ì „ëµì  êµí›ˆ(Feedback)'ì„ ë„ì¶œí•©ë‹ˆë‹¤.
        Task Type: THINKING Tier (Deep Reflection)
        """
        provider = self._get_provider(LLMTier.THINKING)
        if provider is None:
            return ""

        prompt = f"""
        [ì‹œìŠ¤í…œ ì§€ì¹¨]
        ë‹¹ì‹ ì€ ì œë‹ˆì˜ 'AI ì „ëµ íšŒê³  ë‹´ë‹¹ê´€'ì…ë‹ˆë‹¤. 
        ìµœê·¼ AI Analystì˜ íˆ¬ì ì„±ê³¼ ë³´ê³ ì„œë¥¼ ê¹Šì´ ìˆê²Œ ë¶„ì„í•˜ì—¬, í–¥í›„ ì˜ì‚¬ê²°ì •ì„ ê°œì„ í•  ìˆ˜ ìˆëŠ” 'êµ¬ì²´ì ì´ê³  ì‹¤ì²œì ì¸ êµí›ˆ(Feedback)'ì„ ë„ì¶œí•´ì£¼ì„¸ìš”.

        ---
        [ì„±ê³¼ ë³´ê³ ì„œ ìš”ì•½]
        {performance_summary}
        ---

        [ë¶„ì„ ëª©í‘œ]
        ìœ„ ë³´ê³ ì„œì—ì„œ ë“œëŸ¬ë‚˜ëŠ” 'íŒ¨ë°° íŒ¨í„´(Failure Patterns)'ê³¼ 'ì„±ê³µ ìš”ì¸(Success Factors)'ì„ íŒŒì•…í•˜ê³ ,
        Scout AI(Hunter/Judge)ê°€ ë‹¤ìŒ ì¢…ëª© ì„ ì • ì‹œ **ë°˜ë“œì‹œ ì§€ì¼œì•¼ í•  ê°€ì´ë“œë¼ì¸**ì„ ì‘ì„±í•˜ì„¸ìš”.

        [ì‘ì„± ì›ì¹™]
        1. **êµ¬ì²´ì„±**: "ì¡°ì‹¬í•´ë¼" ê°™ì€ ëª¨í˜¸í•œ ì¡°ì–¸ ê¸ˆì§€. "í•˜ë½ì¥ì—ì„œ RSI > 70 ì¢…ëª© ë§¤ìˆ˜ ê¸ˆì§€" ì²˜ëŸ¼ êµ¬ì²´ì ìœ¼ë¡œ.
        2. **ê°„ê²°ì„±**: í•µì‹¬ë§Œ 3~5ê°œ í•­ëª©ìœ¼ë¡œ ìš”ì•½.
        3. **ì–´ì¡°**: ì—„ê²©í•˜ê³  ë¶„ì„ì ì¸ í†¤.

        [ì¶œë ¥ ì˜ˆì‹œ]
        1. [ì‹œì¥ êµ­ë©´] í•˜ë½ì¥(Bear Market)ì—ì„œëŠ” 'ëª¨ë©˜í…€' íŒ©í„°ì˜ ì‹ ë¢°ë„ê°€ ë‚®ìœ¼ë¯€ë¡œ, í€ë”ë©˜í„¸(PER<10) ë¹„ì¤‘ì„ ë†’ì¼ ê²ƒ.
        2. [ê¸°ìˆ ì  ì§€í‘œ] RSI 75 ì´ìƒ êµ¬ê°„ì—ì„œì˜ ì¶”ê²© ë§¤ìˆ˜ëŠ” ìŠ¹ë¥  20% ë¯¸ë§Œì´ë¯€ë¡œ ì ˆëŒ€ ê¸ˆì§€.
        3. [ì„¹í„°] ë°”ì´ì˜¤ ì„¹í„°ëŠ” ë‰´ìŠ¤ í˜¸ì¬ë§Œìœ¼ë¡œ ë§¤ìˆ˜ ì‹œ ì†ì‹¤ ë¦¬ìŠ¤í¬ê°€ í¼. ìˆ˜ê¸‰ ë™ë°˜ ì—¬ë¶€ í•„ìˆ˜ í™•ì¸.
        """
        
        try:
            logger.info(f"--- [JennieBrain/Feedback] ì „ëµ íšŒê³  via {provider.name} ---")
            
            # Use generate_chat for thinking models usually, or generate_json/text depending on provider capabilities.
            # Assuming generate_chat is robust for this.
            chat_history = [{"role": "user", "parts": [{"text": prompt}]}]
            result = provider.generate_chat(chat_history, temperature=0.2)
            
            if isinstance(result, dict):
                return result.get('text') or result.get('content') or str(result)
            return str(result)
            
        except Exception as e:
            logger.error(f"âŒ [Feedback] ìƒì„± ì‹¤íŒ¨: {e}")
            return ""



    # -----------------------------------------------------------------
    # Competitor Benefit Analysis (LLM-First)
    # -----------------------------------------------------------------
    def analyze_competitor_benefit(self, news_title: str) -> dict:
        """
        ë‰´ìŠ¤ ì œëª©ì„ ë¶„ì„í•˜ì—¬ ê²½ìŸì‚¬ ìˆ˜í˜œ(ì•…ì¬) ì—¬ë¶€ë¥¼ íŒë‹¨í•©ë‹ˆë‹¤.
        REASONING Tier (gpt-oss:20b ë“±)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë§¥ë½ì„ íŒŒì•…í•˜ê³  ìŠ¤í¬ì¸  ë‰´ìŠ¤ ë“±ì„ í•„í„°ë§í•©ë‹ˆë‹¤.
        """
        provider = self._get_provider(LLMTier.FAST)
        if provider is None:
            return {'is_risk': False, 'reason': 'Provider Error'}

        try:
            # Import prompt builder locally to avoid circular imports if any
            from shared.llm_prompts import build_competitor_benefit_prompt
            
            prompt = build_competitor_benefit_prompt(news_title)
            
            # Define Schema
            SCHEMA = {
                "type": "object",
                "properties": {
                    "is_risk": {"type": "boolean"},
                    "event_type": {"type": "string"},
                    "competitor_benefit_score": {"type": "integer"},
                    "reason": {"type": "string"}
                },
                "required": ["is_risk", "event_type", "competitor_benefit_score", "reason"]
            }

            logger.info(f"--- [JennieBrain/Competitor] ë‰´ìŠ¤ ë¶„ì„ via {provider.name} ---")
            logger.debug(f"   Target: {news_title[:50]}...")

            # [Optimization] Use Flash model for FAST tier if available (e.g. Gemini 2.5 Flash)
            model_name = None
            if hasattr(provider, 'flash_model_name'):
                 model_name = provider.flash_model_name()

            result = provider.generate_json(
                prompt,
                SCHEMA,
                temperature=0.1, # Deterministic for classification
                model_name=model_name
            )
            return result
            
        except Exception as e:
            logger.error(f"âŒ [Competitor] ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {'is_risk': False, 'reason': f"Error: {e}"}

