#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
services/telegram-collector/collector.py
----------------------------------------
í…”ë ˆê·¸ë¨ ì¦ê¶Œì‚¬ ë¦¬ì„œì¹˜ ì±„ë„ì—ì„œ ë©”ì‹œì§€ë¥¼ ìˆ˜ì§‘í•˜ëŠ” ì„œë¹„ìŠ¤.

3í˜„ì Council ê¶Œê³ ì‚¬í•­:
- ì™¸ë¶€ ì •ë³´ëŠ” ë³´ì¡° ì—­í• ì— ë¨¸ë¬¼ëŸ¬ì•¼ í•¨ (ê°€ì¤‘ì¹˜ â‰¤10%)
- RISK_OFF ë‹¨ë… ë°œë™ ê¸ˆì§€
- í¸í–¥ í•„í„°ë§ ë° ë‹¤ì¤‘ ê²€ì¦ í•„ìˆ˜

v1.0: Telethon ê¸°ë°˜ ì±„ë„ ë©”ì‹œì§€ ìˆ˜ì§‘
"""

import os
import sys
import json
import asyncio
import logging
import hashlib
import re
from datetime import datetime, timezone, timedelta
from typing import List, Dict, Any, Optional
from dataclasses import dataclass, asdict

from dotenv import load_dotenv

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì„¤ì •
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, PROJECT_ROOT)

from channels import (
    TELEGRAM_CHANNELS,
    TelegramChannel,
    ChannelRole,
    get_channel_by_username,
)

# ==============================================================================
# Logging
# ==============================================================================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
)
logger = logging.getLogger(__name__)

# ==============================================================================
# Configuration
# ==============================================================================
load_dotenv()


def load_secrets() -> dict:
    """secrets.jsonì—ì„œ ì„¤ì • ë¡œë“œ"""
    secrets_paths = [
        "/app/secrets.json",  # Docker í™˜ê²½ (ë³¼ë¥¨ ë§ˆìš´íŠ¸)
        os.path.join(PROJECT_ROOT, "secrets.json"),
        "/app/config/secrets.json",
    ]
    for path in secrets_paths:
        if os.path.exists(path):
            try:
                with open(path, "r") as f:
                    import json
                    logger.info(f"secrets.json ë¡œë“œ: {path}")
                    return json.load(f)
            except Exception as e:
                logger.warning(f"secrets.json ë¡œë“œ ì‹¤íŒ¨ ({path}): {e}")
    logger.warning("secrets.jsonì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
    return {}


_secrets = load_secrets()

# Telegram API ì„¤ì • (secrets.json ìš°ì„ , í™˜ê²½ë³€ìˆ˜ fallback)
TELEGRAM_API_ID = _secrets.get("telegram_api_id") or os.getenv("TELEGRAM_API_ID")
TELEGRAM_API_HASH = _secrets.get("telegram_api_hash") or os.getenv("TELEGRAM_API_HASH")
TELEGRAM_SESSION_NAME = os.getenv("TELEGRAM_SESSION_NAME", "telegram_collector")

# ìˆ˜ì§‘ ì„¤ì •
MAX_MESSAGES_PER_CHANNEL = int(os.getenv("TELEGRAM_MAX_MESSAGES", "50"))
MESSAGE_AGE_HOURS = int(os.getenv("TELEGRAM_MESSAGE_AGE_HOURS", "24"))

# ì¤‘ë³µ ì²´í¬ ìºì‹œ
_seen_message_hashes: set = set()


# ==============================================================================
# Data Classes
# ==============================================================================

@dataclass
class CollectedMessage:
    """ìˆ˜ì§‘ëœ ë©”ì‹œì§€ ë°ì´í„° êµ¬ì¡°"""
    message_id: int
    channel_username: str
    channel_name: str
    channel_role: str
    content: str
    published_at: datetime
    collected_at: datetime
    content_hash: str
    weight: float
    metadata: Dict[str, Any]

    def to_stream_format(self) -> Dict[str, Any]:
        """Redis Stream ë°œí–‰ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        return {
            "page_content": self.content,
            "metadata": {
                "message_id": self.message_id,
                "channel_username": self.channel_username,
                "channel_name": self.channel_name,
                "channel_role": self.channel_role,
                "published_at_utc": int(self.published_at.timestamp()),
                "collected_at_utc": int(self.collected_at.timestamp()),
                "content_hash": self.content_hash,
                "weight": self.weight,
                "source": "telegram",
                **self.metadata,
            }
        }


# ==============================================================================
# Helper Functions
# ==============================================================================

def compute_content_hash(content: str) -> str:
    """ì½˜í…ì¸  í•´ì‹œ ê³„ì‚° (ì¤‘ë³µ ì²´í¬ìš©)"""
    normalized = re.sub(r'\s+', ' ', content.lower().strip())
    return hashlib.md5(normalized.encode()).hexdigest()[:16]


def is_duplicate(content_hash: str) -> bool:
    """ì¤‘ë³µ ë©”ì‹œì§€ ì²´í¬"""
    if content_hash in _seen_message_hashes:
        return True
    _seen_message_hashes.add(content_hash)
    return False


def clear_hash_cache():
    """í•´ì‹œ ìºì‹œ ì´ˆê¸°í™”"""
    global _seen_message_hashes
    _seen_message_hashes = set()


def extract_metadata_from_content(
    content: str,
    channel: TelegramChannel
) -> Dict[str, Any]:
    """ì½˜í…ì¸ ì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
    metadata = {
        "has_link": bool(re.search(r'https?://', content)),
        "has_stock_code": bool(re.search(r'\d{6}\.KS', content)),
        "content_length": len(content),
        "line_count": content.count('\n') + 1,
    }

    # ì±„ë„ë³„ ì œëª© íŒ¨í„´ ì¶”ì¶œ
    if channel.title_pattern:
        match = re.search(channel.title_pattern, content)
        if match:
            metadata["parsed_title"] = match.groups()

    # ì„¹í„°/ì¢…ëª© í‚¤ì›Œë“œ ê°ì§€
    sector_keywords = ['ë°˜ë„ì²´', 'ìë™ì°¨', 'ë°”ì´ì˜¤', '2ì°¨ì „ì§€', 'ì¡°ì„ ', 'ì€í–‰', 'ì¦ê¶Œ']
    detected_sectors = [kw for kw in sector_keywords if kw in content]
    if detected_sectors:
        metadata["detected_sectors"] = detected_sectors

    # ë§¤í¬ë¡œ í‚¤ì›Œë“œ ê°ì§€
    macro_keywords = ['ê¸ˆë¦¬', 'í™˜ìœ¨', 'FOMC', 'ë¬¼ê°€', 'CPI', 'GDP', 'ê²½ê¸°', 'ì¹¨ì²´', 'ì¸í”Œë ˆì´ì…˜', 'ê´€ì„¸', 'íŠ¸ëŸ¼í”„', 'ë‹¬ëŸ¬']
    detected_macro = [kw for kw in macro_keywords if kw in content]
    if detected_macro:
        metadata["detected_macro_keywords"] = detected_macro
        metadata["is_macro_related"] = True
    else:
        metadata["is_macro_related"] = False

    return metadata


# ==============================================================================
# Telegram Client (Telethon)
# ==============================================================================

async def collect_channel_messages(
    channel_username: str,
    max_messages: int = MAX_MESSAGES_PER_CHANNEL,
    hours_ago: int = MESSAGE_AGE_HOURS,
) -> List[CollectedMessage]:
    """
    í…”ë ˆê·¸ë¨ ì±„ë„ì—ì„œ ë©”ì‹œì§€ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.

    Args:
        channel_username: ì±„ë„ username (@ ì œì™¸)
        max_messages: ìµœëŒ€ ìˆ˜ì§‘ ë©”ì‹œì§€ ìˆ˜
        hours_ago: ëª‡ ì‹œê°„ ì´ë‚´ ë©”ì‹œì§€ë§Œ ìˆ˜ì§‘

    Returns:
        ìˆ˜ì§‘ëœ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸
    """
    try:
        from telethon import TelegramClient
        from telethon.tl.functions.messages import GetHistoryRequest
    except ImportError:
        logger.error("Telethon not installed. Run: pip install telethon")
        return []

    if not TELEGRAM_API_ID or not TELEGRAM_API_HASH:
        logger.error("TELEGRAM_API_ID and TELEGRAM_API_HASH must be set")
        return []

    channel = get_channel_by_username(channel_username)
    if not channel:
        logger.warning(f"Unknown channel: {channel_username}")
        return []

    collected = []
    cutoff_time = datetime.now(timezone.utc) - timedelta(hours=hours_ago)

    # Docker í™˜ê²½ì—ì„œ ì„¸ì…˜ ë””ë ‰í† ë¦¬ íƒìƒ‰ (ìš°ì„ ìˆœìœ„ ìˆœ)
    session_dir = None
    for candidate in [
        "/opt/airflow/.telegram_sessions",  # Airflow container
        "/app/.telegram_sessions",           # telegram-collector container
        os.path.join(PROJECT_ROOT, ".telegram_sessions"),  # Local development
    ]:
        if os.path.exists(candidate):
            session_dir = candidate
            break
    if session_dir is None:
        session_dir = os.path.join(PROJECT_ROOT, ".telegram_sessions")
    os.makedirs(session_dir, exist_ok=True)
    session_path = os.path.join(session_dir, TELEGRAM_SESSION_NAME)

    async with TelegramClient(session_path, int(TELEGRAM_API_ID), TELEGRAM_API_HASH) as client:
        try:
            entity = await client.get_entity(channel_username)

            messages = await client(GetHistoryRequest(
                peer=entity,
                limit=max_messages,
                offset_id=0,
                offset_date=None,
                add_offset=0,
                max_id=0,
                min_id=0,
                hash=0
            ))

            for msg in messages.messages:
                if not msg.message:
                    continue

                # ì‹œê°„ í•„í„°
                msg_time = msg.date.replace(tzinfo=timezone.utc)
                if msg_time < cutoff_time:
                    continue

                content = msg.message.strip()
                content_hash = compute_content_hash(content)

                # ì¤‘ë³µ ì²´í¬
                if is_duplicate(content_hash):
                    logger.debug(f"Duplicate message skipped: {content_hash}")
                    continue

                # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
                metadata = extract_metadata_from_content(content, channel)

                collected.append(CollectedMessage(
                    message_id=msg.id,
                    channel_username=channel_username,
                    channel_name=channel.name,
                    channel_role=channel.role.value,
                    content=content,
                    published_at=msg_time,
                    collected_at=datetime.now(timezone.utc),
                    content_hash=content_hash,
                    weight=channel.weight,
                    metadata=metadata,
                ))

            logger.info(f"Collected {len(collected)} messages from @{channel_username}")

        except Exception as e:
            logger.error(f"Error collecting from @{channel_username}: {e}")

    return collected


async def collect_all_channels(
    max_messages: int = MAX_MESSAGES_PER_CHANNEL,
    hours_ago: int = MESSAGE_AGE_HOURS,
) -> List[CollectedMessage]:
    """ëª¨ë“  ì„¤ì •ëœ ì±„ë„ì—ì„œ ë©”ì‹œì§€ ìˆ˜ì§‘"""
    all_messages = []

    for channel in TELEGRAM_CHANNELS:
        messages = await collect_channel_messages(
            channel.username,
            max_messages=max_messages,
            hours_ago=hours_ago,
        )
        all_messages.extend(messages)

    logger.info(f"Total collected: {len(all_messages)} messages from {len(TELEGRAM_CHANNELS)} channels")
    return all_messages


# ==============================================================================
# Redis Stream Publisher
# ==============================================================================

def publish_to_stream(messages: List[CollectedMessage]) -> int:
    """
    ìˆ˜ì§‘ëœ ë©”ì‹œì§€ë¥¼ Redis Streamì— ë°œí–‰í•©ë‹ˆë‹¤.

    Returns:
        ë°œí–‰ëœ ë©”ì‹œì§€ ìˆ˜
    """
    try:
        from shared.messaging.stream_client import publish_news_batch, STREAM_MACRO_RAW
    except ImportError:
        logger.error("Failed to import stream_client")
        return 0

    if not messages:
        return 0

    documents = [msg.to_stream_format() for msg in messages]
    published = publish_news_batch(documents, STREAM_MACRO_RAW)

    logger.info(f"Published {published} messages to {STREAM_MACRO_RAW}")
    return published


# ==============================================================================
# Main Entry Points
# ==============================================================================

def run_collector_job():
    """
    ìˆ˜ì§‘ ì‘ì—… ì‹¤í–‰ (1íšŒ)
    - ëª¨ë“  ì±„ë„ì—ì„œ ë©”ì‹œì§€ ìˆ˜ì§‘
    - Redis Streamì— ë°œí–‰
    """
    logger.info("=" * 60)
    logger.info("[TelegramCollector] Starting collection job")
    logger.info("=" * 60)

    # Async ì‹¤í–‰
    messages = asyncio.run(collect_all_channels())

    if messages:
        published = publish_to_stream(messages)
        logger.info(f"[TelegramCollector] Job complete: {published} messages published")
    else:
        logger.info("[TelegramCollector] No new messages to publish")

    return len(messages)


def run_collector_daemon(interval_seconds: int = 600):
    """
    ìˆ˜ì§‘ ë°ëª¬ (ë¬´í•œ ë£¨í”„)
    - ì§€ì •ëœ ê°„ê²©ìœ¼ë¡œ ë©”ì‹œì§€ ìˆ˜ì§‘
    """
    import time

    logger.info(f"[TelegramCollector Daemon] Starting (interval={interval_seconds}s)")

    while True:
        try:
            run_collector_job()
        except Exception as e:
            logger.error(f"[TelegramCollector] Job failed: {e}", exc_info=True)

        logger.info(f"Sleeping for {interval_seconds} seconds...")
        time.sleep(interval_seconds)


# ==============================================================================
# Mock Mode (í…ŒìŠ¤íŠ¸ìš©)
# ==============================================================================

def generate_mock_messages() -> List[CollectedMessage]:
    """í…ŒìŠ¤íŠ¸ìš© ëª¨ì˜ ë©”ì‹œì§€ ìƒì„±"""
    now = datetime.now(timezone.utc)

    mock_data = [
        {
            "channel_username": "hedgecat0301",
            "content": """[1/30, ì¥ ì‹œì‘ ì „ ìƒê°: ë¯¸ì¤‘ ê´€ì„¸ ìš°ë ¤ vs ì‹¤ì  ê¸°ëŒ€, í‚¤ì›€ í•œì§€ì˜]

1. ë‹¤ìš° +0.3%, S&P500 +0.5%, ë‚˜ìŠ¤ë‹¥ +0.8%
2. ë¯¸ì¤‘ ê´€ì„¸ ì´ìŠˆ ì§€ì†. íŠ¸ëŸ¼í”„ ëŒ€í†µë ¹ ê´€ì„¸ ë¶€ê³¼ ì‹œì‚¬
3. ë°˜ë„ì²´ ì—…ì¢… ê°•ì„¸. AI íˆ¬ì ê¸°ëŒ€ê° ìœ ì§€
4. ì›/ë‹¬ëŸ¬ 1,450ì›ëŒ€. ì™¸êµ­ì¸ ìˆœë§¤ìˆ˜ ì „í™˜ ê¸°ëŒ€

ì˜¤ëŠ˜ë„ ì¢‹ì€ í•˜ë£¨ ë˜ì„¸ìš”!""",
        },
        {
            "channel_username": "HanaResearch",
            "content": """ì‚¼ì„±ì „ì (005930.KS/ë§¤ìˆ˜): 4Q24 ì‹¤ì  ë¦¬ë·°

â–  ì‹¤ì  í•˜ì´ë¼ì´íŠ¸
- ë§¤ì¶œ 75ì¡°ì› (YoY +10%)
- ì˜ì—…ì´ìµ 8ì¡°ì› (YoY +15%)

â–  ì• ë„ë¦¬ìŠ¤íŠ¸ Comment
ë©”ëª¨ë¦¬ ê°€ê²© ìƒìŠ¹ìœ¼ë¡œ ì‹¤ì  ê°œì„ . ëª©í‘œì£¼ê°€ 90,000ì› ìœ ì§€.

í•˜ë‚˜ì¦ê¶Œ ë°˜ë„ì²´ ë‹´ë‹¹
https://vo.la/xxx""",
        },
        {
            "channel_username": "meritz_research",
            "content": """ğŸ“® [ë°˜ë„ì²´/ë””ìŠ¤í”Œë ˆì´ ê¹€ì„ ìš°]

í˜„ëŒ€ì°¨ (005380) 4Q24 Review

â–¶ ì‹¤ì  ìš”ì•½
- ë§¤ì¶œ 42ì¡°ì› (ì»¨ì„¼ì„œìŠ¤ ìƒíšŒ)
- ì˜ì—…ì´ìµë¥  9.5%

â–¶ íˆ¬ìì˜ê²¬
ì ì •ì£¼ê°€ 320,000ì›ìœ¼ë¡œ ìƒí–¥

*ë™ ìë£ŒëŠ” Compliance ê·œì •ì„ ì¤€ìˆ˜í•˜ì—¬ ì‚¬ì „ ê³µí‘œëœ ìë£Œì…ë‹ˆë‹¤.""",
        },
    ]

    messages = []
    for i, data in enumerate(mock_data):
        channel = get_channel_by_username(data["channel_username"])
        content = data["content"]
        content_hash = compute_content_hash(content)

        messages.append(CollectedMessage(
            message_id=1000 + i,
            channel_username=data["channel_username"],
            channel_name=channel.name if channel else "Unknown",
            channel_role=channel.role.value if channel else "unknown",
            content=content,
            published_at=now - timedelta(hours=i),
            collected_at=now,
            content_hash=content_hash,
            weight=channel.weight if channel else 0.0,
            metadata=extract_metadata_from_content(content, channel) if channel else {},
        ))

    return messages


# ==============================================================================
# CLI Entry Point
# ==============================================================================

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Telegram Collector Service")
    parser.add_argument("--daemon", action="store_true", help="Run as daemon")
    parser.add_argument("--interval", type=int, default=600, help="Daemon interval (seconds)")
    parser.add_argument("--mock", action="store_true", help="Use mock data (no Telegram API)")
    parser.add_argument("--max-messages", type=int, default=50, help="Max messages per channel")
    args = parser.parse_args()

    if args.mock:
        logger.info("[Mock Mode] Generating mock messages...")
        messages = generate_mock_messages()
        for msg in messages:
            print(f"\n--- {msg.channel_name} ({msg.channel_role}) ---")
            print(f"Weight: {msg.weight}")
            print(f"Macro related: {msg.metadata.get('is_macro_related', False)}")
            print(f"Content preview: {msg.content[:200]}...")

        # Redisì— ë°œí–‰
        published = publish_to_stream(messages)
        print(f"\nPublished {published} mock messages")
    elif args.daemon:
        run_collector_daemon(args.interval)
    else:
        run_collector_job()
