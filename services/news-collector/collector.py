#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
services/news-collector/collector.py
--------------------------------------
ë‰´ìŠ¤ ìˆ˜ì§‘ ì „ìš© ì„œë¹„ìŠ¤ (Producer).
Redis Streamsì— ë‰´ìŠ¤ë¥¼ ë°œí–‰í•˜ë©°, ë¶„ì„ì€ ìˆ˜í–‰í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

v2.0: shared/crawlers/naver.py ê³µí†µ ëª¨ë“ˆ ì‚¬ìš©
"""

import os
import sys
import time
import logging
import calendar
from datetime import datetime, timezone
from concurrent.futures import ThreadPoolExecutor, as_completed

import feedparser
from dotenv import load_dotenv

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì„¤ì •
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, PROJECT_ROOT)

from shared.messaging.stream_client import (
    publish_news_batch, get_stream_length, trim_stream,
    STREAM_NEWS_RAW, NewsDeduplicator, compute_news_hash,
)
from shared.crawlers.naver import (
    crawl_stock_news,
    get_kospi_top_stocks,
    clear_news_hash_cache,
    NOISE_KEYWORDS,
)

# ==============================================================================
# Logging
# ==============================================================================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
)
logger = logging.getLogger(__name__)

# ==============================================================================
# Configuration
# ==============================================================================
load_dotenv()

# Universe
UNIVERSE_SIZE = int(os.getenv("COLLECTOR_UNIVERSE_SIZE", "200"))

# Naver Finance
NAVER_NEWS_MAX_PAGES = int(os.getenv("NAVER_NEWS_MAX_PAGES", "2"))
NAVER_NEWS_REQUEST_DELAY = float(os.getenv("NAVER_NEWS_REQUEST_DELAY", "0.3"))

# General RSS
GENERAL_RSS_FEEDS = [
    {"source_name": "Hankyung (Finance)", "url": "https://www.hankyung.com/feed/finance"},
    {"source_name": "Hankyung (Economy)", "url": "https://www.hankyung.com/feed/economy"},
    {"source_name": "Maeil Business (Economy)", "url": "https://www.mk.co.kr/rss/50000001/"},
    {"source_name": "Maeil Business (Stock)", "url": "https://www.mk.co.kr/rss/50100001/"},
]

# Redis ê¸°ë°˜ ì˜ì† ì¤‘ë³µ ì²´í¬ (ì»¨í…Œì´ë„ˆ ì¬ì‹œì‘ì—ë„ ìœ ì§€)
_deduplicator = NewsDeduplicator()

# ==============================================================================
# Helper Functions
# ==============================================================================

def _compute_hash(title: str) -> str:
    import hashlib
    import re
    normalized = re.sub(r'[^\w]', '', title.lower())
    return hashlib.md5(normalized.encode()).hexdigest()[:12]


def _is_noise_title(title: str) -> bool:
    for noise in NOISE_KEYWORDS:
        if noise in title:
            return True
    return False


# ==============================================================================
# RSS News Crawling
# ==============================================================================

def crawl_general_rss_news() -> list:
    """ì¼ë°˜ ê²½ì œ/ê¸ˆìœµ RSS ë‰´ìŠ¤ ìˆ˜ì§‘"""
    logger.info("ğŸ“° ì¼ë°˜ ê²½ì œ ë‰´ìŠ¤ RSS ìˆ˜ì§‘ ì¤‘...")
    documents = []

    for feed_info in GENERAL_RSS_FEEDS:
        try:
            feed = feedparser.parse(feed_info["url"])
            for entry in feed.entries[:20]:
                title = entry.get('title', '').strip()
                link = entry.get('link', '')

                if not title or not link:
                    continue

                # Filter
                if _is_noise_title(title):
                    continue

                news_hash = _compute_hash(title)
                if not _deduplicator.check_and_mark(news_hash):
                    continue
                
                # Timestamp
                if hasattr(entry, 'published_parsed') and entry.published_parsed:
                    pub_timestamp = int(calendar.timegm(entry.published_parsed))
                else:
                    pub_timestamp = int(datetime.now(timezone.utc).timestamp())
                
                # Phase 1C: ë©”íƒ€ë°ì´í„° ê°•í™” - ì¶œì²˜/ë‚ ì§œë¥¼ page_contentì— í¬í•¨
                pub_date_str = datetime.fromtimestamp(pub_timestamp, tz=timezone.utc).strftime('%Y-%m-%d')
                documents.append({
                    "page_content": f"[ì‹œì¥ë‰´ìŠ¤] {title} | ì¶œì²˜: {feed_info['source_name']} | ë‚ ì§œ: {pub_date_str}",
                    "metadata": {
                        "stock_code": None,
                        "stock_name": None,
                        "source": feed_info["source_name"],
                        "source_url": link,
                        "created_at_utc": pub_timestamp,
                    }
                })
        except Exception as e:
            logger.warning(f"RSS ìˆ˜ì§‘ ì˜¤ë¥˜ ({feed_info['source_name']}): {e}")
    
    logger.info(f"âœ… ì¼ë°˜ ë‰´ìŠ¤ {len(documents)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
    return documents


# ==============================================================================
# Main Collector Job
# ==============================================================================

def run_collector_job():
    """
    ìˆ˜ì§‘ ì‘ì—… ì‹¤í–‰ (1íšŒ)
    - KOSPI 200 ì¢…ëª© ë‰´ìŠ¤ ìˆ˜ì§‘
    - ì¼ë°˜ ê²½ì œ ë‰´ìŠ¤ ìˆ˜ì§‘
    - Redis Streamì— ë°œí–‰
    """
    start_time = time.time()
    logger.info("=" * 60)
    logger.info("ğŸš€ [Collector] ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘")
    logger.info("=" * 60)
    
    all_documents = []
    
    # 1. General News
    general_docs = crawl_general_rss_news()
    all_documents.extend(general_docs)
    
    # 2. Stock-specific News (Parallel) - shared ëª¨ë“ˆ ì‚¬ìš©
    logger.info(f"ğŸ“Š KOSPI ìƒìœ„ {UNIVERSE_SIZE}ê°œ ì¢…ëª© ë¡œë“œ ì¤‘...")
    universe = get_kospi_top_stocks(UNIVERSE_SIZE)
    
    if universe:
        logger.info(f"âœ… {len(universe)}ê°œ ì¢…ëª© ë¡œë“œ ì™„ë£Œ")
        logger.info(f"ğŸ“Š {len(universe)}ê°œ ì¢…ëª© ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘...")
        
        with ThreadPoolExecutor(max_workers=10) as executor:
            futures = {
                executor.submit(
                    crawl_stock_news, 
                    s["code"], 
                    s["name"], 
                    max_pages=NAVER_NEWS_MAX_PAGES,
                    request_delay=NAVER_NEWS_REQUEST_DELAY
                ): s
                for s in universe
            }
            for future in as_completed(futures):
                stock = futures[future]
                try:
                    docs = future.result()
                    if docs:
                        all_documents.extend(docs)
                except Exception as e:
                    logger.warning(f"ì¢…ëª© ìˆ˜ì§‘ ì‹¤íŒ¨ ({stock['name']}): {e}")
    
    # 3. Redis ì˜ì† ì¤‘ë³µ ì²´í¬ (naver.py ì¸ë©”ëª¨ë¦¬ dedup ë³´ì™„ â€” ì»¨í…Œì´ë„ˆ ì¬ì‹œì‘ ëŒ€ì‘)
    if all_documents:
        doc_hashes = [compute_news_hash(doc["page_content"]) for doc in all_documents]
        new_hashes = _deduplicator.filter_new_batch(doc_hashes)

        before_count = len(all_documents)
        all_documents = [
            doc for doc, h in zip(all_documents, doc_hashes)
            if h in new_hashes
        ]
        dedup_removed = before_count - len(all_documents)
        if dedup_removed > 0:
            logger.info(f"ğŸ”„ [Dedup] ì˜ì† ì¤‘ë³µ ì œê±°: {dedup_removed}ê°œ ({before_count} â†’ {len(all_documents)})")

    # 4. Publish to Redis Stream
    if all_documents:
        logger.info(f"ğŸ“¤ Redis Streamì— {len(all_documents)}ê°œ ë‰´ìŠ¤ ë°œí–‰ ì¤‘...")
        published = publish_news_batch(all_documents, STREAM_NEWS_RAW)
        logger.info(f"âœ… {published}ê°œ ë°œí–‰ ì™„ë£Œ")
        
        # Trim old messages (keep last 100k)
        trim_stream(STREAM_NEWS_RAW, maxlen=100000)
    else:
        logger.info("â„¹ï¸ ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ì—†ìŒ")
    
    elapsed = time.time() - start_time
    logger.info("=" * 60)
    logger.info(f"âœ… [Collector] ì™„ë£Œ - ì´ {len(all_documents)}ê°œ ìˆ˜ì§‘, {elapsed:.1f}ì´ˆ ì†Œìš”")
    logger.info(f"ğŸ“Š Stream ê¸¸ì´: {get_stream_length(STREAM_NEWS_RAW)}")
    logger.info("=" * 60)


def run_collector_daemon(interval_seconds: int = 600):
    """
    ìˆ˜ì§‘ ë°ëª¬ (ë¬´í•œ ë£¨í”„)
    - ì§€ì •ëœ ê°„ê²©ìœ¼ë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘
    """
    logger.info(f"ğŸ”„ [Collector Daemon] ì‹œì‘ (interval={interval_seconds}s)")
    
    while True:
        try:
            run_collector_job()
        except Exception as e:
            logger.error(f"âŒ [Collector] ì‘ì—… ì‹¤íŒ¨: {e}", exc_info=True)
        
        logger.info(f"ğŸ’¤ {interval_seconds}ì´ˆ ëŒ€ê¸°...")
        time.sleep(interval_seconds)


if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--daemon", action="store_true", help="Run as daemon (infinite loop)")
    parser.add_argument("--interval", type=int, default=600, help="Interval seconds for daemon mode")
    args = parser.parse_args()
    
    if args.daemon:
        run_collector_daemon(args.interval)
    else:
        run_collector_job()
