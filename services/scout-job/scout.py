#!/usr/bin/env python3
# Version: v1.0
# ì‘ì—… LLM: Claude Sonnet 4.5, Claude Opus 4.5
"""
Scout Job - ì¢…ëª© ë°œêµ´ íŒŒì´í”„ë¼ì¸

ê¸°ëŠ¥:
- ê¹ê¹í•œ í•„í„°ë§ (ê¸°ë³¸ì ìˆ˜ 20, Hunter í†µê³¼ 60ì , Judge ìŠ¹ì¸ 75ì )
- ì¿¼í„°ì œ: ìµœì¢… Watchlist ìƒìœ„ 15ê°œë§Œ ì €ì¥
- Debate í”„ë¡¬í”„íŠ¸: Bull/Bear ìºë¦­í„° ê·¹ë‹¨ì ìœ¼ë¡œ ì„¤ì •
- Redis ìƒíƒœ ì €ì¥: Dashboardì—ì„œ ì‹¤ì‹œê°„ íŒŒì´í”„ë¼ì¸ ì§„í–‰ ìƒí™© í™•ì¸ ê°€ëŠ¥
- ê²½ìŸì‚¬ ìˆ˜í˜œ ì ìˆ˜ ë°˜ì˜: ê²½ìŸì‚¬ ì•…ì¬ ì‹œ Hunter ì ìˆ˜ì— ê°€ì‚°
"""

import logging
import os
import sys
import time
import re
import threading
import json
import hashlib
import warnings
from typing import Dict, Tuple, List, Optional
from datetime import datetime, timezone, timedelta
from dotenv import load_dotenv
import requests
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed
import redis

# ë¡œê¹… ì„¤ì •ì„ ëª¨ë“  import ë³´ë‹¤ ë¨¼ì € ìˆ˜í–‰
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - [%(funcName)s] - %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)

logger = logging.getLogger(__name__)

# ê³µìš© ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ë¥¼ ìœ„í•œ ê²½ë¡œ ì„¤ì •
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))  # /app
try:
    import shared
except ImportError:
    sys.path.insert(0, PROJECT_ROOT)

import shared.auth as auth
import shared.database as database
from shared.db.connection import session_scope, ensure_engine_initialized
from shared.kis import KISClient as KIS_API
from shared.kis.gateway_client import KISGatewayClient
from shared.llm import JennieBrain
from shared.financial_data_collector import batch_update_watchlist_financial_data

from shared.archivist import Archivist

import chromadb
from langchain_chroma import Chroma

# langchain_google_genai ë‚´ë¶€ google.generativeai FutureWarning ë¬´ì‹œ
warnings.filterwarnings("ignore", category=FutureWarning, module="langchain_google_genai")
warnings.filterwarnings("ignore", category=FutureWarning, module="google.generativeai")
from langchain_google_genai import GoogleGenerativeAIEmbeddings



# Backtest ëª¨ë“ˆ (ì„ íƒì )
try:
    from utilities.backtest import Backtester
    logger.info("âœ… Backtester ëª¨ë“ˆ ì„í¬íŠ¸ ì„±ê³µ")
except ImportError:
    logger.info("â„¹ï¸ Backtester ëª¨ë“ˆ ì—†ìŒ - ë°±í…ŒìŠ¤íŠ¸ ê¸°ëŠ¥ ë¹„í™œì„±í™”")
    Backtester = None

# Chroma ì„œë²„
CHROMA_SERVER_HOST = os.getenv("CHROMA_SERVER_HOST", "10.178.0.2") 
CHROMA_SERVER_PORT = 8000

# ìºì‹œ/ìƒíƒœ ê´€ë¦¬ í•¨ìˆ˜ (scout_cache.py)
from scout_cache import (
    # ìƒìˆ˜
    STATE_PREFIX, CANDIDATE_DIGEST_SUFFIX, CANDIDATE_HASHES_SUFFIX,
    LLM_CACHE_SUFFIX, LLM_LAST_RUN_SUFFIX, ISO_FORMAT_Z,
    REDIS_URL,
    # Redis í•¨ìˆ˜
    _get_redis, _utcnow, update_pipeline_status, save_pipeline_results,
    # save_hot_watchlist removed from here
    # CONFIG í…Œì´ë¸” í•¨ìˆ˜
    _get_scope, _make_state_key, _load_json_config, _save_json_config,
    _get_last_llm_run_at, _save_last_llm_run_at,
    _load_candidate_state, _save_candidate_state,
    # LLM_EVAL_CACHE í…Œì´ë¸” í•¨ìˆ˜
    _load_llm_cache_from_db, _save_llm_cache_to_db, _save_llm_cache_batch,
    # ìºì‹œ ìœ íš¨ì„± ê²€ì‚¬ ë° í•´ì‹œ ê³„ì‚°
    _is_cache_valid_direct, _get_price_bucket, _get_volume_bucket, _get_foreign_direction,
    _hash_candidate_payload, _compute_candidate_hashes,
    _minutes_since, _parse_int_env, _is_cache_entry_valid,
    _record_to_watchlist_entry, _record_to_cache_payload, _cache_payload_to_record,
)

# ì¢…ëª© ìœ ë‹ˆë²„ìŠ¤ ê´€ë¦¬ (scout_universe.py)
from scout_universe import (
    SECTOR_MAPPING, BLUE_CHIP_STOCKS,

    analyze_sector_momentum, get_hot_sector_stocks,
    get_dynamic_blue_chips, get_momentum_stocks,
    filter_valid_stocks
)
import scout_cache
from shared.watchlist import save_hot_watchlist
# íŒŒì´í”„ë¼ì¸ íƒœìŠ¤í¬ (scout_pipeline.py)
from scout_pipeline import (
    is_hybrid_scoring_enabled,
    process_quant_scoring_task,
    process_phase1_hunter_v5_task, process_phase23_judge_v5_task,
    fetch_kis_data_task,
)

_redis_client = None  # scout_cacheì—ì„œ ê´€ë¦¬í•˜ì§€ë§Œ í˜¸í™˜ì„± ìœ ì§€




def prefetch_all_data(candidate_stocks: Dict[str, Dict], kis_api, vectorstore) -> Tuple[Dict[str, Dict], Dict[str, str]]:
    """
    Phase 1 ì‹œì‘ ì „ì— ëª¨ë“  ë°ì´í„°ë¥¼ ì¼ê´„ ì¡°íšŒí•˜ì—¬ ìºì‹œ
    
    Returns:
        (snapshot_cache, news_cache) - ì¢…ëª©ì½”ë“œë¥¼ í‚¤ë¡œ í•˜ëŠ” dict
    
    íš¨ê³¼: ë³‘ë ¬ ìŠ¤ë ˆë“œ ì•ˆì—ì„œ API í˜¸ì¶œ ì œê±° â†’ Rate Limit íšŒí”¼ + ì†ë„ í–¥ìƒ
    """
    stock_codes = list(candidate_stocks.keys())
    logger.info(f"   (Prefetch) {len(stock_codes)}ê°œ ì¢…ëª© ë°ì´í„° ì‚¬ì „ ì¡°íšŒ ì‹œì‘...")
    
    snapshot_cache: Dict[str, Dict] = {}
    news_cache: Dict[str, str] = {}
    
    prefetch_start = time.time()
    
    # 1. KIS API ìŠ¤ëƒ…ìƒ· ë³‘ë ¬ ì¡°íšŒ (4ê°œ ì›Œì»¤)
    logger.info(f"   (Prefetch) KIS ìŠ¤ëƒ…ìƒ· ì¡°íšŒ ì¤‘...")
    snapshot_start = time.time()
    
    def fetch_snapshot(code):
        try:
            if hasattr(kis_api, 'API_CALL_DELAY'):
                time.sleep(kis_api.API_CALL_DELAY * 0.3)  # ì•½ê°„ì˜ ë”œë ˆì´
            return code, kis_api.get_stock_snapshot(code)
        except Exception as e:
            logger.debug(f"   âš ï¸ [{code}] Snapshot ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return code, None
    
    with ThreadPoolExecutor(max_workers=8) as executor:
        futures = [executor.submit(fetch_snapshot, code) for code in stock_codes]
        for future in as_completed(futures):
            code, snapshot = future.result()
            if snapshot:
                snapshot_cache[code] = snapshot
    
    snapshot_time = time.time() - snapshot_start
    logger.info(f"   (Prefetch) âœ… KIS ìŠ¤ëƒ…ìƒ· {len(snapshot_cache)}/{len(stock_codes)}ê°œ ì¡°íšŒ ì™„ë£Œ ({snapshot_time:.1f}ì´ˆ)")
    
    # 2. ChromaDB ë‰´ìŠ¤ ë³‘ë ¬ ì¡°íšŒ (8ê°œ ì›Œì»¤)
    if vectorstore:
        logger.info(f"   (Prefetch) ChromaDB ë‰´ìŠ¤ ì¡°íšŒ ì¤‘...")
        news_start = time.time()
        
        def fetch_news(code_name):
            code, name = code_name
            try:
                news = fetch_stock_news_from_chroma(vectorstore, code, name, k=3)
                return code, news
            except Exception as e:
                logger.debug(f"   âš ï¸ [{code}] ë‰´ìŠ¤ ì¡°íšŒ ì‹¤íŒ¨: {e}")
                return code, "ë‰´ìŠ¤ ì¡°íšŒ ì‹¤íŒ¨"
        
        code_name_pairs = [(code, info.get('name', '')) for code, info in candidate_stocks.items()]
        
        with ThreadPoolExecutor(max_workers=8) as executor:
            futures = [executor.submit(fetch_news, pair) for pair in code_name_pairs]
            for future in as_completed(futures):
                code, news = future.result()
                news_cache[code] = news
        
        news_time = time.time() - news_start
        valid_news = sum(1 for n in news_cache.values() if n and n not in ["ë‰´ìŠ¤ DB ë¯¸ì—°ê²°", "ìµœê·¼ ê´€ë ¨ ë‰´ìŠ¤ ì—†ìŒ", "ë‰´ìŠ¤ ê²€ìƒ‰ ì˜¤ë¥˜", "ë‰´ìŠ¤ ì¡°íšŒ ì‹¤íŒ¨"])
        logger.info(f"   (Prefetch) âœ… ChromaDB ë‰´ìŠ¤ {valid_news}/{len(stock_codes)}ê°œ ì¡°íšŒ ì™„ë£Œ ({news_time:.1f}ì´ˆ)")
    
    total_time = time.time() - prefetch_start
    logger.info(f"   (Prefetch) âœ… ì „ì²´ ì‚¬ì „ ì¡°íšŒ ì™„ë£Œ ({total_time:.1f}ì´ˆ)")
    
    return snapshot_cache, news_cache


def enrich_candidates_with_market_data(candidate_stocks: Dict[str, Dict], session, vectorstore) -> None:
    """
    í›„ë³´êµ°ì— ì‹œì¥ ë°ì´í„° ì¶”ê°€ (í•´ì‹œ ê³„ì‚°ìš©)
    
    í•´ì‹œì— í¬í•¨ë  ë°ì´í„°:
    - price: ìµœì‹  ì¢…ê°€ (5% ë²„í‚·í™”ë¨)
    - volume: ìµœì‹  ê±°ë˜ëŸ‰ (10ë§Œì£¼ ë²„í‚·í™”ë¨)
    - foreign_net: ì™¸êµ­ì¸ ìˆœë§¤ìˆ˜ (ë°©í–¥ë§Œ - buy/sell/neutral)
    - news_date: ìµœì‹  ë‰´ìŠ¤ ë‚ ì§œ (YYYY-MM-DD)
    """
    if not candidate_stocks:
        return
    
    stock_codes = list(candidate_stocks.keys())
    logger.info(f"   (Hash) {len(stock_codes)}ê°œ ì¢…ëª© ì‹œì¥ ë°ì´í„° ì¡°íšŒ ì¤‘...")
    
    # 1. DBì—ì„œ ìµœì‹  ê°€ê²©/ê±°ë˜ëŸ‰ ë°ì´í„° ì¼ê´„ ì¡°íšŒ
    try:
        from sqlalchemy import text
        
        placeholders = ','.join([f"'{code}'" for code in stock_codes])
        
        # ìµœì‹  ë‚ ì§œì˜ ë°ì´í„°ë§Œ ì¡°íšŒ (ê°€ê²©, ê±°ë˜ëŸ‰)
        query = text(f"""
            SELECT STOCK_CODE, CLOSE_PRICE, VOLUME, PRICE_DATE
            FROM STOCK_DAILY_PRICES_3Y
            WHERE STOCK_CODE IN ({placeholders})
            AND (STOCK_CODE, PRICE_DATE) IN (
                SELECT STOCK_CODE, MAX(PRICE_DATE) 
                FROM STOCK_DAILY_PRICES_3Y
                WHERE STOCK_CODE IN ({placeholders})
                GROUP BY STOCK_CODE
            )
        """)
        rows = session.execute(query).fetchall()
        
        for row in rows:
            code = row[0]
            price = row[1]
            volume = row[2]
            
            if code in candidate_stocks:
                candidate_stocks[code]['price'] = float(price) if price else 0
                candidate_stocks[code]['volume'] = int(volume) if volume else 0
        
        logger.info(f"   (Hash) âœ… DBì—ì„œ {len(rows)}ê°œ ì¢…ëª© ì‹œì¥ ë°ì´í„° ë¡œë“œ")
        
        # [Fix] ìµœì‹  ë‰´ìŠ¤ ê°ì„± ì ìˆ˜ ì¡°íšŒ (NEWS_SENTIMENT - Active Table)
        # QuantScorerì— ì „ë‹¬í•˜ê¸° ìœ„í•´ ì—¬ê¸°ì„œ ì¡°íšŒ
        sent_query = text(f"""
            SELECT STOCK_CODE, SENTIMENT_SCORE 
            FROM NEWS_SENTIMENT
            WHERE STOCK_CODE IN ({placeholders})
            AND PUBLISHED_AT >= DATE_SUB(NOW(), INTERVAL 3 DAY)
            AND (STOCK_CODE, PUBLISHED_AT) IN (
                SELECT STOCK_CODE, MAX(PUBLISHED_AT)
                FROM NEWS_SENTIMENT
                WHERE STOCK_CODE IN ({placeholders})
                GROUP BY STOCK_CODE
            )
        """)
        sent_rows = session.execute(sent_query).fetchall()
        for row in sent_rows:
            code = row[0]
            score = row[1]
            if code in candidate_stocks and score is not None:
                candidate_stocks[code]['sentiment_score'] = float(score)
        
        logger.info(f"   (Hash) âœ… DBì—ì„œ {len(sent_rows)}ê°œ ì¢…ëª© ë‰´ìŠ¤ ê°ì„± ì ìˆ˜ ë¡œë“œ")
    except Exception as e:
        logger.warning(f"   (Hash) âš ï¸ DB ì‹œì¥ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}")
    
    # 2. ChromaDB ë‰´ìŠ¤ ì¡°íšŒ ìƒëµ (ì†ë„ ìµœì í™”)
    # ì´ìœ : í•´ì‹œì— ì˜¤ëŠ˜ ë‚ ì§œê°€ í¬í•¨ë˜ì–´ ìˆì–´ì„œ ë§¤ì¼ ì¬í‰ê°€ ë³´ì¥ë¨
    # ë‰´ìŠ¤ ë°ì´í„°ëŠ” Phase 1 Hunterì—ì„œ ê°œë³„ ì¢…ëª© í‰ê°€ ì‹œ ì¡°íšŒí•¨
    logger.info(f"   (Hash) âœ… ë‰´ìŠ¤ ë‚ ì§œ ì¡°íšŒ ìƒëµ (ë‚ ì§œ ê¸°ë°˜ ìºì‹œ ë¬´íš¨í™”ë¡œ ëŒ€ì²´)")


def _get_latest_news_date(vectorstore, stock_code: str, stock_name: str) -> Optional[str]:
    """ChromaDBì—ì„œ ì¢…ëª©ì˜ ìµœì‹  ë‰´ìŠ¤ ë‚ ì§œ ì¡°íšŒ"""
    try:
        from datetime import datetime, timezone
        
        docs = vectorstore.similarity_search(
            query=f"{stock_name}",
            k=1,
            filter={"stock_code": stock_code}
        )
        if docs and docs[0].metadata:
            # crawler.pyëŠ” 'created_at_utc' (int timestamp)ë¥¼ ì €ì¥í•¨
            # 'date'ë‚˜ 'published_at'ì€ legacy
            timestamp = docs[0].metadata.get('created_at_utc')
            if timestamp:
                return datetime.fromtimestamp(int(timestamp), tz=timezone.utc).strftime('%Y-%m-%d')

            # Legacy fields fallback
            news_date = docs[0].metadata.get('date') or docs[0].metadata.get('published_at')
            if news_date:
                return str(news_date)[:10]
    except Exception:
        pass
    return None

# ... (skip _record_to_cache_payload, _cache_payload_to_record, etc.)

def fetch_stock_news_from_chroma(vectorstore, stock_code: str, stock_name: str, k: int = 3) -> str:
    """
    ChromaDBì—ì„œ ì¢…ëª©ë³„ ìµœì‹  ë‰´ìŠ¤ ê²€ìƒ‰
    
    Args:
        vectorstore: ChromaDB vectorstore ì¸ìŠ¤í„´ìŠ¤
        stock_code: ì¢…ëª© ì½”ë“œ
        stock_name: ì¢…ëª©ëª…
        k: ê°€ì ¸ì˜¬ ë‰´ìŠ¤ ê°œìˆ˜
        
    Returns:
        ë‰´ìŠ¤ ìš”ì•½ ë¬¸ìì—´ (ì—†ìœ¼ë©´ "ìµœê·¼ ê´€ë ¨ ë‰´ìŠ¤ ì—†ìŒ")
    """
    if not vectorstore:
        return "ë‰´ìŠ¤ DB ë¯¸ì—°ê²°"
    
    try:
        from datetime import datetime, timedelta, timezone
        
        # ìµœì‹  7ì¼ ì´ë‚´ ë‰´ìŠ¤ í•„í„°
        recency_timestamp = int((datetime.now(timezone.utc) - timedelta(days=7)).timestamp())
        
        # ì¢…ëª© ì½”ë“œë¡œ í•„í„°ë§ëœ ë‰´ìŠ¤ ê²€ìƒ‰ ì‹œë„
        try:
            # ë‚ ì§œ í•„í„°($gte) ì¶”ê°€í•˜ì—¬ ì˜¤ë˜ëœ ë‰´ìŠ¤(2020ë…„ ë“±) ë°©ì§€
            docs = vectorstore.similarity_search(
                query=f"{stock_name} ì‹¤ì  ìˆ˜ì£¼ í˜¸ì¬",
                k=k,
                filter={
                    "$and": [
                        {"stock_code": stock_code},
                        {"created_at_utc": {"$gte": recency_timestamp}}
                    ]
                }
            )
            # logger.debug(f"   (D) [{stock_code}] í•„í„° ê²€ìƒ‰ ê²°ê³¼: {len(docs)}ê±´")
        except Exception:
            # í•„í„° ì‹¤íŒ¨ì‹œ ì¢…ëª©ëª…ìœ¼ë¡œ ê²€ìƒ‰
            docs = vectorstore.similarity_search(
                query=f"{stock_name} ì£¼ì‹ ë‰´ìŠ¤",
                k=k
            )
            logger.debug(f"   (D) [{stock_code}] ì¢…ëª©ëª… ê²€ìƒ‰(Fallback): {len(docs)}ê±´")
            # ì¢…ëª© ê´€ë ¨ ë‰´ìŠ¤ë§Œ í•„í„°ë§
            docs = [d for d in docs if stock_name in d.page_content or stock_code in str(d.metadata)]
        
        if docs:
            news_items = []
            for i, doc in enumerate(docs[:k], 1):
                content = doc.page_content[:100].strip()
                if content:
                    news_items.append(f"[ë‰´ìŠ¤{i}] {content}")
            
            if news_items:
                return " | ".join(news_items)
        
        return "ìµœê·¼ ê´€ë ¨ ë‰´ìŠ¤ ì—†ìŒ"
        
    except Exception as e:
        logger.debug(f"   âš ï¸ [{stock_code}] ChromaDB ë‰´ìŠ¤ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return "ë‰´ìŠ¤ ê²€ìƒ‰ ì˜¤ë¥˜"


# íŒŒì´í”„ë¼ì¸ íƒœìŠ¤í¬ í•¨ìˆ˜ (scout_pipeline.py)
# - is_hybrid_scoring_enabled, process_quant_scoring_task
# - process_phase1_hunter_v5_task, process_phase23_judge_v5_task
# - process_phase1_hunter_task, process_phase23_debate_judge_task
# - process_llm_decision_task, fetch_kis_data_task

def main():
    start_time = time.time()
    
    logger.info("--- ğŸ¤– 'Scout Job' ì‹¤í–‰ ì‹œì‘ ---")
    
    kis_api = None
    brain = None

    try:
        logger.info("--- [Init] í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ë° KIS API ì—°ê²° ì‹œì‘ ---")
        load_dotenv(override=True)
        
        trading_mode = os.getenv("TRADING_MODE", "REAL")
        use_gateway = os.getenv("USE_KIS_GATEWAY", "true").lower() == "true"
        
        if use_gateway:
            kis_api = KISGatewayClient()
            logger.info("âœ… KIS Gateway Client ì´ˆê¸°í™” ì™„ë£Œ")
        else:
            kis_api = KIS_API(
                app_key=auth.get_secret(os.getenv(f"{trading_mode}_SECRET_ID_APP_KEY")),
                app_secret=auth.get_secret(os.getenv(f"{trading_mode}_SECRET_ID_APP_SECRET")),
                base_url=os.getenv(f"KIS_BASE_URL_{trading_mode}"),
                account_prefix=auth.get_secret(os.getenv(f"{trading_mode}_SECRET_ID_ACCOUNT_PREFIX")),
                account_suffix=os.getenv("KIS_ACCOUNT_SUFFIX"),
                token_file_path="/app/tokens/kis_token_scout.json",
                trading_mode=trading_mode
            )
            if not kis_api.authenticate():
                raise Exception("KIS API ì¸ì¦ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        
        # [Check] ì‹¤í–‰ ì‹œê°„ ì œí•œ (07:00 ~ 16:00)
        # í…ŒìŠ¤íŠ¸/Mock ëª¨ë“œì´ê±°ë‚˜ ê°•ì œ ì‹¤í–‰ ì„¤ì •ì´ ì•„ë‹ˆë©´ ì‹œê°„ ì²´í¬ ìˆ˜í–‰
        disable_check = os.getenv("DISABLE_MARKET_OPEN_CHECK", "false").lower() in {"1", "true", "yes", "on"}
        
        if not disable_check and trading_mode.lower() != "mock":
            import pytz
            kst = pytz.timezone('Asia/Seoul')
            now_kst = datetime.now(kst)

            # ì£¼ë§ ì²´í¬ (í† =5, ì¼=6)
            if now_kst.weekday() >= 5:
                logger.info(f"ğŸ›‘ [Check] ì˜¤ëŠ˜ì€ ì£¼ë§({now_kst.strftime('%A')})ì´ë¯€ë¡œ ì‹¤í–‰í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. (Scout ì¢…ë£Œ)")
                return
            
            if 7 <= now_kst.hour < 16:
                logger.info(f"ğŸ“… [Check] í˜„ì¬ ì‹œê°„({now_kst.strftime('%H:%M')})ì€ ì‹¤í–‰ í—ˆìš© ì‹œê°„(07:00~16:00)ì…ë‹ˆë‹¤.")
            else:
                logger.info(f"ğŸ›‘ [Check] í˜„ì¬ ì‹œê°„({now_kst.strftime('%H:%M')})ì€ ì‹¤í–‰ í—ˆìš© ì‹œê°„ì´ ì•„ë‹™ë‹ˆë‹¤. (Scout ì¢…ë£Œ)")
                return
        else:
            logger.info("â© ì‹œê°„/ì¥ìš´ì˜ ì²´í¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤ (mock/test ëª¨ë“œ ë˜ëŠ” DISABLE_MARKET_OPEN_CHECK=true).")
        
        brain = JennieBrain(
            project_id=os.getenv("GCP_PROJECT_ID", "local"),
            gemini_api_key_secret=os.getenv("SECRET_ID_GEMINI_API_KEY")
        )
        
        # SQLAlchemy ì„¸ì…˜ ì´ˆê¸°í™” (session_scope ì‚¬ìš© ì „ì— í˜¸ì¶œ í•„ìˆ˜)
        ensure_engine_initialized()
        
        # SQLAlchemy ì„¸ì…˜ ì‚¬ìš©
        with session_scope() as session:
            watchlist_snapshot = database.get_active_watchlist(session)
            
            vectorstore = None
            # RAG í™œì„±í™” ì—¬ë¶€ í™•ì¸ (ê¸°ë³¸ê°’: True)
            enable_rag = os.getenv("ENABLE_RAG", "true").lower() == "true"
            rag_provider = os.getenv("RAG_EMBEDDING_PROVIDER", "local").lower()  # ê¸°ë³¸ê°’ local (ë¹„ìš© ì ˆê°)

            if not enable_rag:
                logger.info("â© [Config] RAG ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤ (ENABLE_RAG=false).")
                vectorstore = None
            else:
                try:
                    embeddings = None
                    if rag_provider == "local":
                        # Local Embedding (HuggingFace)
                        logger.info("   ... ChromaDB í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì‹œë„ (Local Embeddings: jhgan/ko-sroberta-multitask) ...")
                        try:
                            from langchain_huggingface import HuggingFaceEmbeddings
                            embeddings = HuggingFaceEmbeddings(
                                model_name="jhgan/ko-sroberta-multitask",
                                model_kwargs={"device": "cpu"},
                                encode_kwargs={"normalize_embeddings": True}
                            )
                        except ImportError:
                            logger.error("ğŸš¨ langchain_huggingface ëª¨ë“ˆì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. RAGë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                            raise

                    else:
                        # Cloud Embedding (Gemini)
                        logger.info("   ... ChromaDB í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì‹œë„ (Gemini Embeddings) ...")
                        api_key = auth.get_secret("gemini-api-key")
                        if not api_key:
                             raise ValueError("Gemini API Key not found")
                        embeddings = GoogleGenerativeAIEmbeddings(
                            model="models/gemini-embedding-001", 
                            google_api_key=api_key
                        )
                    
                    chroma_client = chromadb.HttpClient( # noqa
                        host=CHROMA_SERVER_HOST, 
                        port=CHROMA_SERVER_PORT
                    )
                    vectorstore = Chroma(
                        client=chroma_client, 
                        collection_name="rag_stock_data", 
                        embedding_function=embeddings
                    )
                    logger.info(f"âœ… LLM ë° ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ (Provider: {rag_provider}).")
                except Exception as e:
                    logger.warning(f"âš ï¸ ChromaDB ì´ˆê¸°í™” ì‹¤íŒ¨ (RAG ê¸°ëŠ¥ ë¹„í™œì„±í™”): {e}")
                    vectorstore = None

            # Phase 1: íŠ¸ë¦¬í”Œ ì†ŒìŠ¤ í›„ë³´ ë°œêµ´ (v3.8: ì„¹í„° ë¶„ì„ ì¶”ê°€)
            logger.info("--- [Phase 1] íŠ¸ë¦¬í”Œ ì†ŒìŠ¤ í›„ë³´ ë°œêµ´ ì‹œì‘ ---")
            update_pipeline_status(phase=1, phase_name="Hunter Scout", status="running", progress=0)
            candidate_stocks = {}

            # A: ë™ì  ìš°ëŸ‰ì£¼ (KOSPI 200 ê¸°ì¤€)
            universe_size = int(os.getenv("SCOUT_UNIVERSE_SIZE", "200"))
            for stock in get_dynamic_blue_chips(limit=universe_size):
                candidate_stocks[stock['code']] = {
                    'name': stock['name'], 
                    'sector': stock.get('sector'),
                    'reasons': ['KOSPI ì‹œì´ ìƒìœ„']
                }
            
            # E: ì„¹í„° ëª¨ë©˜í…€ ë¶„ì„ (v3.8 ì‹ ê·œ)
            sector_analysis = analyze_sector_momentum(kis_api, session, watchlist_snapshot)
            hot_sector_stocks = get_hot_sector_stocks(sector_analysis, top_n=30)
            for stock in hot_sector_stocks:
                if stock['code'] not in candidate_stocks:
                    candidate_stocks[stock['code']] = {
                        'name': stock['name'], 
                        'sector': stock.get('sector'),
                        'reasons': [f"í•« ì„¹í„° ({stock['sector']}, +{stock['sector_momentum']:.1f}%)"]
                    }
                else:
                    candidate_stocks[stock['code']]['reasons'].append(
                        f"í•« ì„¹í„° ({stock['sector']}, +{stock['sector_momentum']:.1f}%)"
                    )

            # B: ì •ì  ìš°ëŸ‰ì£¼
            for stock in BLUE_CHIP_STOCKS:
                if stock['code'] not in candidate_stocks:
                    candidate_stocks[stock['code']] = {'name': stock['name'], 'reasons': ['ì •ì  ìš°ëŸ‰ì£¼']}

            # C: RAG
            if vectorstore:
                try:
                    logger.info("   (C) RAG ê¸°ë°˜ í›„ë³´ ë°œêµ´ ì¤‘...")
                    rag_results = vectorstore.similarity_search(query="ì‹¤ì  í˜¸ì¬ ê³„ì•½ ìˆ˜ì£¼", k=50)
                    for doc in rag_results:
                        stock_code = doc.metadata.get('stock_code')
                        stock_name = doc.metadata.get('stock_name')
                        if stock_code and stock_name:
                            if stock_code not in candidate_stocks:
                                candidate_stocks[stock_code] = {'name': stock_name, 'reasons': []}
                            candidate_stocks[stock_code]['reasons'].append("RAG ê¸°ë°˜ í˜¸ì¬ ê²€ìƒ‰")
                except Exception as e:
                    logger.warning(f"   (C) RAG í›„ë³´ ë°œêµ´ ì‹¤íŒ¨: {e}")

            # NEW: Filter against STOCK_MASTER (Remove ETFs and unregistered stocks)
            candidate_stocks = filter_valid_stocks(candidate_stocks, session)

            # D: ëª¨ë©˜í…€
            logger.info("   (D) ëª¨ë©˜í…€ íŒ©í„° ê¸°ë°˜ ì¢…ëª© ë°œêµ´ ì¤‘...")
            momentum_stocks = get_momentum_stocks(
                    kis_api,
                    session,
                period_months=6,
                top_n=30,
                watchlist_snapshot=watchlist_snapshot
            )
            for stock in momentum_stocks:
                if stock['code'] not in candidate_stocks:
                    candidate_stocks[stock['code']] = {
                        'name': stock['name'], 
                        'reasons': [f'ëª¨ë©˜í…€ ({stock["momentum"]:.1f}%)']
                    }
            
            logger.info(f"   âœ… í›„ë³´êµ° {len(candidate_stocks)}ê°œ ë°œêµ´ ì™„ë£Œ.")
            
            # [Filter] ì œì™¸ ì¢…ëª© í•„í„°ë§ (v1.1)
            excluded_stocks = [s.strip() for s in os.getenv("EXCLUDED_STOCKS", "").split(",") if s.strip()]
            if excluded_stocks:
                logger.info(f"   ğŸš« ì œì™¸ ì¢…ëª© í•„í„° ì ìš©: {excluded_stocks}")
                for ex_code in excluded_stocks:
                    if ex_code in candidate_stocks:
                        del candidate_stocks[ex_code]
                        logger.info(f"      - {ex_code} ì œì™¸ë¨ (ì‚¬ìš©ì ì„¤ì •)")
            
            # [DEBUG] Truncate for Judge Phase Verification - Removed
            # candidate_stocks = dict(list(candidate_stocks.items())[:3])

            # í•´ì‹œ ê³„ì‚° ì „ì— ì‹œì¥ ë°ì´í„° ì¶”ê°€ (ê°€ê²©, ê±°ë˜ëŸ‰)
            logger.info("--- [Phase 1.5] ì‹œì¥ ë°ì´í„° ê¸°ë°˜ í•´ì‹œ ê³„ì‚° ---")
            enrich_candidates_with_market_data(candidate_stocks, session, vectorstore)
            
    # Phase 1 ì‹œì‘ ì „ì— ëª¨ë“  ë°ì´í„° ì¼ê´„ ì¡°íšŒ (ë³‘ë ¬ ìŠ¤ë ˆë“œ ì•ˆ API í˜¸ì¶œ ì œê±°)
            logger.info("--- [Phase 1.6] ë°ì´í„° ì‚¬ì „ ì¡°íšŒ (ìŠ¤ëƒ…ìƒ·/ë‰´ìŠ¤) ---")
            snapshot_cache, news_cache = prefetch_all_data(candidate_stocks, kis_api, vectorstore)

            # [Filter] ì¡ì£¼ í•„í„°ë§ (Junk Stock Filter) - ì‹œì´/ì£¼ê°€ ê¸°ì¤€
            # Config: JUNK_FILTER_MIN_CAP_BILLION (ê¸°ë³¸ 500ì–µ), JUNK_FILTER_MIN_PRICE (ê¸°ë³¸ 1000ì›)
            min_cap_billion = _parse_int_env(os.getenv("JUNK_FILTER_MIN_CAP_BILLION"), 50) # 500ì–µ (ë‹¨ìœ„: ì–µ ì•„ë‹˜. input int 50 -> 500ì–µ?)
            # Wait, DB unit is Million. 50B = 50,000 Million.
            # User expectation: 500ì–µ. 
            # Let's align with ENV var naming. 
            # If ENV is "50", it might mean 50 Billion?
            # Let's set default code constant to 50000 (Million KRW) for safety and clarity.
            
            junk_min_cap_unit = _parse_int_env(os.getenv("MIN_MARKET_CAP_INT"), 50000) # Default 500ì–µ (50000 ë°±ë§Œ)
            junk_min_price = _parse_int_env(os.getenv("MIN_PRICE_INT"), 1000)

            junk_dropped = 0
            junk_codes = []
            
            for code in list(candidate_stocks.keys()):
                if code == '0001': continue # ì§€ìˆ˜ëŠ” ì œì™¸
                
                # Check 1: Penny Stock
                price = candidate_stocks[code].get('price', 0)
                # Check 2: Small Cap (Use Snapshot)
                snapshot = snapshot_cache.get(code)
                market_cap = snapshot.get('market_cap', 0) if snapshot else 0
                
                is_penny = price < junk_min_price
                is_small_cap = market_cap < junk_min_cap_unit
                
                if is_penny or is_small_cap:
                    reason = []
                    if is_penny: reason.append(f"ë™ì „ì£¼({price:,.0f}ì›)")
                    if is_small_cap: reason.append(f"ì´ˆì†Œí˜•ì£¼({market_cap//100:,.0f}ì–µ)")
                    
                    logger.info(f"      ğŸ—‘ï¸ [JunkFilter] {candidate_stocks[code]['name']}({code}) ì œì™¸: {', '.join(reason)}")
                    del candidate_stocks[code]
                    if snapshot_cache.get(code): del snapshot_cache[code] # Clean cache too
                    if news_cache.get(code): del news_cache[code]
                    junk_dropped += 1
                    junk_codes.append(code)
            
            if junk_dropped > 0:
                logger.info(f"   (Filter) ğŸš« ì¡ì£¼ í•„í„°ë§: {junk_dropped}ê°œ ì¢…ëª© ì œì™¸ ì™„ë£Œ")

            # [NEW] Phase 1.7: ìŠ¤ëƒ…ìƒ·ì—ì„œ ì¬ë¬´ì§€í‘œ(PER/PBR) ì¶”ì¶œ â†’ STOCK_FUNDAMENTALS ì €ì¥
            # ì´ìœ : ì „ì²´ 200ê°œ ì¢…ëª©ì˜ ì¬ë¬´ ë°ì´í„°ë¥¼ ì¼ì¼ ë‹¨ìœ„ë¡œ ì¶•ì í•˜ì—¬ ë°±í…ŒìŠ¤íŠ¸ ì •í™•ë„ í–¥ìƒ
            logger.info("--- [Phase 1.7] ì¬ë¬´ì§€í‘œ ì €ì¥ (STOCK_FUNDAMENTALS) ---")
            from datetime import date
            fundamentals_to_save = []
            today = date.today()
            for code, snapshot in snapshot_cache.items():
                if snapshot and (snapshot.get('per') or snapshot.get('pbr')):
                    fundamentals_to_save.append({
                        'stock_code': code,
                        'trade_date': today,
                        'per': snapshot.get('per'),
                        'pbr': snapshot.get('pbr'),
                        'roe': None,  # KIS API ìŠ¤ëƒ…ìƒ·ì—ëŠ” ROEê°€ ì—†ìŒ
                        'market_cap': snapshot.get('market_cap')
                    })
            if fundamentals_to_save:
                database.update_all_stock_fundamentals(session, fundamentals_to_save)
                logger.info(f"   (DB) âœ… ì¬ë¬´ì§€í‘œ {len(fundamentals_to_save)}ê°œ ì¢…ëª© ì €ì¥ ì™„ë£Œ")

            # ë‰´ìŠ¤ í•´ì‹œë¥¼ candidate_stocksì— ë°˜ì˜ (í•´ì‹œ ê³„ì‚°ì— í¬í•¨)
            # ë‰´ìŠ¤ ë‚´ìš©ì´ ë°”ë€Œë©´ í•´ì‹œê°€ ë‹¬ë¼ì ¸ LLM ì¬í˜¸ì¶œë¨
            news_hash_count = 0
            for code, news in news_cache.items():
                if code in candidate_stocks and news and news not in [
                    "ë‰´ìŠ¤ DB ë¯¸ì—°ê²°", "ìµœê·¼ ê´€ë ¨ ë‰´ìŠ¤ ì—†ìŒ", "ë‰´ìŠ¤ ê²€ìƒ‰ ì˜¤ë¥˜", 
                    "ë‰´ìŠ¤ ì¡°íšŒ ì‹¤íŒ¨", "ë‰´ìŠ¤ ìºì‹œ ì—†ìŒ"
                ]:
                    # ë‰´ìŠ¤ ë‚´ìš©ì˜ MD5 í•´ì‹œ (ì‹œê°„ ì •ë³´ í¬í•¨ë˜ì–´ ìˆìŒ)
                    candidate_stocks[code]['news_hash'] = hashlib.md5(news.encode()).hexdigest()[:16]
                    news_hash_count += 1
            logger.info(f"   (Hash) âœ… ë‰´ìŠ¤ í•´ì‹œ {news_hash_count}ê°œ ë°˜ì˜ ì™„ë£Œ")

            # Phase 1.8: ìˆ˜ê¸‰ ë°ì´í„°(Market Flow) ë¶„ì„ ë° ê¸°ë¡
            logger.info("--- [Phase 1.8] ìˆ˜ê¸‰ ë°ì´í„°(Market Flow) ë¶„ì„ (Foreign/Institution) ---")
            
            # [Optimization] ë³‘ë ¬ë¡œ íˆ¬ìì ë™í–¥ ì¡°íšŒ
            investor_flow_cache = {}
            
            # Archivist ì´ˆê¸°í™” (ì—¬ê¸°ì„œë„ ì‚¬ìš©)
            if 'archivist' not in locals():
                archivist = Archivist(session_scope)
                
            def process_flow_data(code):
                try:
                    # [Tier 1] KIS API via gateway.market_data
                    try:
                        trends = kis_api.get_market_data().get_investor_trend(code, start_date=None, end_date=None)
                    except (AttributeError, Exception):
                        # [Tier 2] KIS API Direct (If method is missing or fails)
                        try:
                            trends = kis_api.get_investor_trend(code, start_date=None, end_date=None)
                        except Exception:
                            trends = None
                    
                    if trends:
                        # ê°€ì¥ ìµœê·¼ ë°ì´í„° (ì˜¤ëŠ˜)
                        return code, trends[-1]
                    
                    # [Tier 3] DB Fallback (Historical Data)
                    try:
                        from shared.database.market import get_investor_trading
                        df = get_investor_trading(session, code, limit=1)
                        if not df.empty:
                            row = df.iloc[-1]
                            return code, {
                                'date': row['TRADE_DATE'].strftime('%Y%m%d'),
                                'foreigner_net_buy': int(row['FOREIGN_NET_BUY']),
                                'institution_net_buy': int(row['INSTITUTION_NET_BUY']),
                                'individual_net_buy': int(row['INDIVIDUAL_NET_BUY']),
                                'price': float(row['CLOSE_PRICE'])
                            }
                    except Exception as e:
                        logger.debug(f"   âš ï¸ [{code}] DB ìˆ˜ê¸‰ ì¡°íšŒ ì‹¤íŒ¨: {e}")
                        
                    return code, None
                except Exception as e:
                    return code, None

            with ThreadPoolExecutor(max_workers=8) as executor:
                futures = [executor.submit(process_flow_data, code) for code in candidate_stocks.keys()]
                for future in as_completed(futures):
                    code, flow_data = future.result()
                    if flow_data:
                        investor_flow_cache[code] = flow_data
                        
                        # í›„ë³´êµ° ì •ë³´ì— ìˆ˜ê¸‰ ë°ì´í„° ì¶”ê°€ (LLM í”„ë¡¬í”„íŠ¸ìš©)
                        candidate_stocks[code]['market_flow'] = {
                            'foreign_net_buy': flow_data['foreigner_net_buy'],
                            'institution_net_buy': flow_data['institution_net_buy'],
                            'individual_net_buy': flow_data['individual_net_buy']
                        }
                        
                        # Archivistì— ê¸°ë¡ (Market Flow Snapshot)
                        try:
                            # flow_dataëŠ” dict í˜•íƒœ (date, price, foreign..., institution...)
                            # Archivist.log_market_flow_snapshotì€ stock_codeë¥¼ í¬í•¨í•œ dictë¥¼ ê¸°ëŒ€í•¨
                            log_payload = flow_data.copy()
                            log_payload['stock_code'] = code
                            # volume í•„ë“œê°€ get_investor_trend ê²°ê³¼ì— ì—†ìœ¼ë¯€ë¡œ (í•„ìš”ì‹œ) ë³´ì™„
                            # log_payload['volume'] = ... 
                            
                            archivist.log_market_flow_snapshot(log_payload)
                        except Exception as log_e:
                            logger.warning(f"Failed to log market flow for {code}: {log_e}")

            logger.info(f"   (Flow) âœ… ìˆ˜ê¸‰ ë°ì´í„° {len(investor_flow_cache)}ê°œ ì¢…ëª© ë¶„ì„ ë° ê¸°ë¡ ì™„ë£Œ")

            # Phase 2: LLM ìµœì¢… ì„ ì •
            logger.info("--- [Phase 2] LLM ê¸°ë°˜ ìµœì¢… Watchlist ì„ ì • ì‹œì‘ ---")
            update_pipeline_status(
                phase=1, phase_name="Hunter Scout", status="running", 
                total_candidates=len(candidate_stocks)
            )
            
            # í•˜ì´ë¸Œë¦¬ë“œ ìŠ¤ì½”ì–´ë§ ëª¨ë“œ ë¶„ê¸°
            if is_hybrid_scoring_enabled():
                logger.info("=" * 60)
                logger.info("   ğŸš€ Scout v5 Hybrid Scoring Mode í™œì„±í™”!")
                logger.info("=" * 60)
                
                # Analyst Feedback Load
                feedback_context = None
                try:
                    redis_conn = _get_redis()
                    feedback_data = redis_conn.get("analyst:feedback:summary")
                    if feedback_data:
                        # redis-pyê°€ decode_responses=Trueì´ë©´ ì´ë¯¸ str ë°˜í™˜
                        feedback_context = feedback_data if isinstance(feedback_data, str) else feedback_data.decode('utf-8')
                        logger.info(f"   ğŸ§  [Feedback] Analyst ì „ëµ êµí›ˆ ë¡œë“œ ì™„ë£Œ ({len(feedback_context)} chars)")
                    else:
                        logger.info("   ğŸ§  [Feedback] ì €ì¥ëœ ì „ëµ êµí›ˆì´ ì—†ìŠµë‹ˆë‹¤.")
                except Exception as e:
                    logger.warning(f"   âš ï¸ [Feedback] ë¡œë“œ ì‹¤íŒ¨: {e}")
                
                try:
                    from shared.hybrid_scoring import (
                        QuantScorer, HybridScorer, 
                        create_hybrid_scoring_tables,
                        format_quant_score_for_prompt,
                    )
                    from shared.market_regime import MarketRegimeDetector
                    
                    # DB í…Œì´ë¸” ìƒì„± í™•ì¸
                    create_hybrid_scoring_tables(session)
                    
                    # ì‹œì¥ êµ­ë©´ ê°ì§€
                    kospi_prices = database.get_daily_prices(session, "0001", limit=60)
                    if not kospi_prices.empty:
                        # [Fix] ì‹¤ì‹œê°„ ì½”ìŠ¤í”¼ ì§€ìˆ˜ ì¡°íšŒ (ì¥ì¤‘ ë³€ë™ì„± ì¦‰ê° ë°˜ì˜)
                        kospi_current = None
                        try:
                            kospi_snapshot = kis_api.get_stock_snapshot("0001", is_index=True)
                            if kospi_snapshot:
                                kospi_current = float(kospi_snapshot['price'])
                                logger.info(f"   (Market) ğŸ“¡ ì‹¤ì‹œê°„ KOSPI ì§€ìˆ˜: {kospi_current:.2f}")
                        except Exception as e:
                            logger.warning(f"   (Market) âš ï¸ ì‹¤ì‹œê°„ KOSPI ì¡°íšŒ ì‹¤íŒ¨ (ì–´ì œ ì¢…ê°€ ì‚¬ìš©): {e}")

                        # ì‹¤ì‹œê°„ ì¡°íšŒ ì‹¤íŒ¨ ì‹œ, DBì˜ ë§ˆì§€ë§‰ ì¢…ê°€(ì–´ì œ) ì‚¬ìš©
                        if kospi_current is None:
                            kospi_current = float(kospi_prices['CLOSE_PRICE'].iloc[-1])

                        detector = MarketRegimeDetector()
                        current_regime, _ = detector.detect_regime(kospi_prices, kospi_current, quiet=True)
                    else:
                        current_regime = "SIDEWAYS"
                    
                    logger.info(f"   í˜„ì¬ ì‹œì¥ êµ­ë©´: {current_regime}")

                    # [NEW] Dashboard í‘œì‹œë¥¼ ìœ„í•´ Redisì— ì €ì¥
                    try:
                        redis_conn = _get_redis()
                        if redis_conn:
                            regime_data = {
                                "regime": current_regime,
                                "confidence": 0.8, # TODO: Detectorì—ì„œ confidence ë°˜í™˜í•˜ë„ë¡ ê°œì„  í•„ìš”
                                "updated_at": datetime.now().isoformat(),
                                "description": f"KOSPI Analysis Based on {len(kospi_prices)} days"
                            }
                            redis_conn.set("market:regime:data", json.dumps(regime_data))
                            logger.info("   (Redis) Market Regime ë°ì´í„° ì €ì¥ ì™„ë£Œ")
                    except Exception as re:
                        logger.warning(f"   (Redis) Market Regime ì €ì¥ ì‹¤íŒ¨: {re}")
                    
                    # QuantScorer ì´ˆê¸°í™”
                    quant_scorer = QuantScorer(session, market_regime=current_regime)
                    
                    # Step 1: ì •ëŸ‰ ì ìˆ˜ ê³„ì‚° (LLM í˜¸ì¶œ ì—†ìŒ, ë¹„ìš© 0ì›)
                    logger.info(f"\n   [Step 1] ì •ëŸ‰ ì ìˆ˜ ê³„ì‚° ({len(candidate_stocks)}ê°œ ì¢…ëª©) - ë¹„ìš© 0ì›")
                    quant_results = {}
                    
                    for code, info in candidate_stocks.items():
                        if code == '0001':
                            continue
                        stock_info = {
                            'code': code,
                            'info': info,
                            'snapshot': snapshot_cache.get(code),
                        }
                        quant_results[code] = process_quant_scoring_task(
                            stock_info, quant_scorer, session, kospi_prices
                        )
                    
                    # Step 2: ì •ëŸ‰ ê¸°ë°˜ 1ì°¨ í•„í„°ë§ (í•˜ìœ„ 60% íƒˆë½ â†’ ìƒìœ„ 40% í†µê³¼)
                    logger.info(f"\n   [Step 2] ì •ëŸ‰ ê¸°ë°˜ 1ì°¨ í•„í„°ë§ (ìƒìœ„ 40% í†µê³¼)")
                    quant_result_list = list(quant_results.values())
                    filtered_results = quant_scorer.filter_candidates(quant_result_list, cutoff_ratio=0.6)
                    
                    filtered_codes = {r.stock_code for r in filtered_results}
                    logger.info(f"   âœ… ì •ëŸ‰ í•„í„° í†µê³¼: {len(filtered_codes)}ê°œ (í‰ê·  ì ìˆ˜: {sum(r.total_score for r in filtered_results)/len(filtered_results):.1f}ì )")
                    
                    # Step 3: LLM ì •ì„± ë¶„ì„ (í†µê³¼ ì¢…ëª©ë§Œ)
                    logger.info(f"\n   [Step 3] LLM ì •ì„± ë¶„ì„ (í†µê³„ ì»¨í…ìŠ¤íŠ¸ í¬í•¨)")
                    
                    final_approved_list: List[Dict] = []
                    if '0001' in candidate_stocks:
                        final_approved_list.append({'code': '0001', 'name': 'KOSPI', 'is_tradable': False})
                    
                    llm_decision_records: Dict[str, Dict] = {}
                    
                    # 2025-12-24: Cloud vs Ollama ë³‘ë ¬ ì²˜ë¦¬ ì°¨ë“± ì ìš©
                    # Cloud (OpenAI, Gemini, Claude): 8ê°œ ë³‘ë ¬ (Rate Limit ë‚´ì—ì„œ ë¬¸ì œì—†ìŒ)
                    # Ollama (ë¡œì»¬): Hunter 4, Judge 1 (GPU ë¶€í•˜/ì•ˆì •ì„±)
                    is_ollama_active = (
                        os.getenv("TIER_REASONING_PROVIDER", "ollama").lower() == "ollama" or 
                        os.getenv("TIER_THINKING_PROVIDER", "ollama").lower() == "ollama"
                    )
                    
                    if is_ollama_active:
                        # Ollama ë¡œì»¬ ëª¨ë“œ: ë³´ìˆ˜ì  ë³‘ë ¬ ì²˜ë¦¬
                        hunter_max_workers = _parse_int_env(os.getenv("SCOUT_HUNTER_MAX_WORKERS"), 4)
                        judge_max_workers = _parse_int_env(os.getenv("SCOUT_JUDGE_MAX_WORKERS"), 1)
                        logger.info(f"   (Config) ğŸ¢ Ollama Mode - Hunter: {hunter_max_workers}, Judge: {judge_max_workers}")
                    else:
                        # Cloud ëª¨ë“œ: í’€ ë³‘ë ¬ ì²˜ë¦¬
                        hunter_max_workers = _parse_int_env(os.getenv("SCOUT_HUNTER_MAX_WORKERS"), 8)
                        judge_max_workers = _parse_int_env(os.getenv("SCOUT_JUDGE_MAX_WORKERS"), 8)
                        logger.info(f"   (Config) â˜ï¸ Cloud Mode - Hunter: {hunter_max_workers}, Judge: {judge_max_workers}")
                    
                    # Phase 1: Hunter (í†µê³„ ì»¨í…ìŠ¤íŠ¸ í¬í•¨)
                    phase1_results = []
                    # Archivist ì´ˆê¸°í™” (Phase 1/2 ê³µìš©)
                    archivist = Archivist(session_scope)

                    # Smart Skip Filter - LLM í˜¸ì¶œ ì „ ì‚¬ì „ í•„í„°ë§
                    from scout_pipeline import should_skip_hunter
                    
                    llm_candidates = []
                    smart_skipped = []
                    skip_reasons_count = {}
                    
                    # LLM ìºì‹œ ë¡œë“œ (ì´ì „ Hunter ì ìˆ˜ ì°¸ì¡°ìš©)
                    try:
                        db_conn = session.connection().connection
                    except Exception:
                        db_conn = None
                    llm_cache = _load_llm_cache_from_db(db_conn) if db_conn else {}
                    
                    for code in filtered_codes:
                        info = candidate_stocks[code]
                        quant_result = quant_results[code]
                        
                        # ê²½ìŸì‚¬ ìˆ˜í˜œ ì ìˆ˜ ì¡°íšŒ
                        competitor_benefit = database.get_competitor_benefit_score(code)
                        competitor_bonus = competitor_benefit.get('score', 0)
                        
                        # ì´ì „ ìºì‹œì—ì„œ Hunter ì ìˆ˜ ì¡°íšŒ
                        cached = llm_cache.get(code)
                        cached_hunter = cached.get('hunter_score') if cached else None
                        
                        # ë‰´ìŠ¤ ê°ì„± ì ìˆ˜ (infoì—ì„œ ê°€ì ¸ì˜¤ê¸°)
                        news_sentiment = info.get('sentiment_score')
                        
                        # Smart Skip ì²´í¬
                        should_skip, reason = should_skip_hunter(
                            quant_result, cached_hunter, news_sentiment, competitor_bonus
                        )
                        
                        if should_skip:
                            smart_skipped.append((code, info['name'], reason))
                            # Skip ì‚¬ìœ ë³„ ì¹´ìš´íŠ¸
                            reason_key = reason.split('(')[0].strip()
                            skip_reasons_count[reason_key] = skip_reasons_count.get(reason_key, 0) + 1
                        else:
                            llm_candidates.append(code)
                    
                    logger.info(f"   ğŸš€ [Smart Skip] {len(smart_skipped)}ê°œ ìŠ¤í‚µ â†’ LLM Hunter ëŒ€ìƒ: {len(llm_candidates)}/{len(filtered_codes)}ê°œ")
                    if skip_reasons_count:
                        logger.info(f"      Skip ì‚¬ìœ : {skip_reasons_count}")
                    
                    # =============================================================
                    # Phase 1: Hunter LLM í˜¸ì¶œ (Smart Skip í†µê³¼ ì¢…ëª©ë§Œ)
                    # =============================================================
                    with ThreadPoolExecutor(max_workers=hunter_max_workers) as executor:
                        future_to_code = {}
                        for code in llm_candidates:
                            info = candidate_stocks[code]
                            quant_result = quant_results[code]
                            payload = {'code': code, 'info': info}
                            future = executor.submit(
                                process_phase1_hunter_v5_task, 
                                payload, brain, quant_result, snapshot_cache, news_cache, archivist, feedback_context
                            )
                            future_to_code[future] = code
                        
                        for future in as_completed(future_to_code):
                            result = future.result()
                            if result:
                                phase1_results.append(result)
                                if not result['passed']:
                                    llm_decision_records[result['code']] = {
                                        'code': result['code'],
                                        'name': result['name'],
                                        'llm_score': result['hunter_score'],
                                        'llm_reason': result['hunter_reason'],
                                        'is_tradable': False,
                                        'approved': False,
                                        'hunter_score': result['hunter_score'],
                                        'llm_metadata': {'llm_grade': 'D', 'source': 'v5_hunter_reject'}
                                    }
                    
                    phase1_passed = [r for r in phase1_results if r['passed']]
                    logger.info(f"   âœ… v5 Hunter í†µê³¼: {len(phase1_passed)}/{len(llm_candidates)}ê°œ (ì „ì²´ ëŒ€ë¹„ {len(phase1_passed)}/{len(filtered_codes)})")
                    
                    # Phase 2-3: Debate + Judge (ìƒìœ„ ì¢…ëª©ë§Œ)
                    PHASE2_MAX = int(os.getenv("SCOUT_PHASE2_MAX_ENTRIES", "50"))
                    if len(phase1_passed) > PHASE2_MAX:
                        phase1_passed_sorted = sorted(phase1_passed, key=lambda x: x['hunter_score'], reverse=True)
                        phase1_passed = phase1_passed_sorted[:PHASE2_MAX]
                    
                    if phase1_passed:
                        logger.info(f"\n   [Step 4] Debate + Judge (í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê²°í•©)")
                        
                        with ThreadPoolExecutor(max_workers=judge_max_workers) as executor:
                            future_to_code = {}
                            
                            # Archivist ì‚¬ìš© (ìœ„ì—ì„œ ì´ˆê¸°í™”ë¨)

                            for p1_result in phase1_passed:
                                future = executor.submit(
                                    process_phase23_judge_v5_task, 
                                    p1_result, brain, archivist, current_regime, feedback_context
                                )
                                future_to_code[future] = p1_result['code']
                            
                            for future in as_completed(future_to_code):
                                record = future.result()
                                if record:
                                    llm_decision_records[record['code']] = record
                                    if record.get('approved'):
                                        final_approved_list.append(_record_to_watchlist_entry(record))
                    
                    logger.info(f"   âœ… v5 ìµœì¢… ìŠ¹ì¸: {len([r for r in llm_decision_records.values() if r.get('approved')])}ê°œ")
                    
                    # ì¿¼í„°ì œ ì ìš©
                    MAX_WATCHLIST_SIZE = 15
                    if len(final_approved_list) > MAX_WATCHLIST_SIZE:
                        final_approved_list_sorted = sorted(
                            final_approved_list,
                            key=lambda x: x.get('llm_score', 0),
                            reverse=True
                        )
                        final_approved_list = final_approved_list_sorted[:MAX_WATCHLIST_SIZE]
                    
                    logger.info(f"\n   ğŸ Scout v1.0 ì™„ë£Œ: {len(final_approved_list)}ê°œ ì¢…ëª© ì„ ì •")
                    
                except Exception as e:
                    logger.error(f"âŒ Scout v1.0 ì‹¤í–‰ ì˜¤ë¥˜: {e}", exc_info=True)
                    # v5 ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ê³„ì† ì§„í–‰ (v4 í´ë°± ì œê±°ë¨)
                    final_approved_list = []
                    if '0001' in candidate_stocks:
                        final_approved_list.append({'code': '0001', 'name': 'KOSPI', 'is_tradable': False})
            else:
                # í•˜ì´ë¸Œë¦¬ë“œ ìŠ¤ì½”ì–´ë§ ë¹„í™œì„±í™” ì‹œ ê¸°ë³¸ ì²˜ë¦¬
                logger.warning("âš ï¸ Hybrid Scoring ë¹„í™œì„±í™”ë¨. ê¸°ë³¸ Watchlistë§Œ ì €ì¥í•©ë‹ˆë‹¤.")
                final_approved_list = []
                if '0001' in candidate_stocks:
                    final_approved_list.append({'code': '0001', 'name': 'KOSPI', 'is_tradable': False})
            
            # ê³µí†µ Phase 3: ìµœì¢… Watchlist ì €ì¥
            logger.info(f"--- [Phase 3] ìµœì¢… Watchlist {len(final_approved_list)}ê°œ ì €ì¥ ---")
            database.save_to_watchlist(session, final_approved_list)
            # Watchlist íˆìŠ¤í† ë¦¬ ì €ì¥ (ë°±í…ŒìŠ¤íŠ¸ ì¬í˜„ìš© ìŠ¤ëƒ…ìƒ·)
            snapshot_date = datetime.now().strftime('%Y-%m-%d')
            database.save_to_watchlist_history(session, final_approved_list, snapshot_date=snapshot_date)
            
            # Hot Watchlist ì €ì¥ (Price Monitor WebSocket êµ¬ë…ìš©)
            # ì‹œì¥ êµ­ë©´ë³„ score_threshold ê³„ì‚°
            recon_score_by_regime = {
                "STRONG_BULL": 58,
                "BULL": 62,
                "SIDEWAYS": 65,
                "BEAR": 70,
            }
            hot_score_threshold = recon_score_by_regime.get(
                current_regime if 'current_regime' in locals() else 'SIDEWAYS', 
                65
            )
            hot_regime = current_regime if 'current_regime' in locals() else 'UNKNOWN'
            
            # LLM Score ê¸°ì¤€ ì´ìƒì¸ ì¢…ëª©ë§Œ Hot Watchlistë¡œ ì €ì¥
            hot_candidates = [
                s for s in final_approved_list 
                if s.get('llm_score', 0) >= hot_score_threshold and s.get('code') != '0001'
            ]
            # LLM Score ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ + ìƒìœ„ 15ê°œ ì œí•œ
            hot_candidates = sorted(hot_candidates, key=lambda x: x.get('llm_score', 0), reverse=True)[:15]
            
            save_hot_watchlist(
                stocks=hot_candidates,
                market_regime=hot_regime,
                score_threshold=hot_score_threshold
            )
            
            with ThreadPoolExecutor(max_workers=10) as executor:
                if hasattr(kis_api, 'API_CALL_DELAY'):
                    future_to_data = {
                        executor.submit(fetch_kis_data_task, s, kis_api): (time.sleep(kis_api.API_CALL_DELAY), s)[1]
                        for s in final_approved_list 
                    }
                else:
                    future_to_data = {
                        executor.submit(fetch_kis_data_task, s, kis_api): s
                        for s in final_approved_list 
                    }
                
                all_daily = []
                all_fund = []
                for future in as_completed(future_to_data):
                    d, f = future.result()
                    if d: all_daily.extend(d)
                    if f: all_fund.append(f)
            
            if all_daily: database.save_all_daily_prices(session, all_daily)
            if all_fund: database.update_all_stock_fundamentals(session, all_fund)
            
            # Phase 3-A: ì¬ë¬´ ë°ì´í„° (ë„¤ì´ë²„ í¬ë¡¤ë§)
            tradable_codes = [s['code'] for s in final_approved_list if s.get('is_tradable', True)]
            if tradable_codes:
                batch_update_watchlist_financial_data(session, tradable_codes)
            
            # Redis ìµœì¢… ìƒíƒœ ì—…ë°ì´íŠ¸ - ì™„ë£Œ
            update_pipeline_status(
                phase=3, phase_name="Final Judge", status="completed",
                progress=100,
                total_candidates=len(candidate_stocks) if 'candidate_stocks' in locals() else 0,
                passed_phase1=len(phase1_passed) if 'phase1_passed' in locals() else 0,
                passed_phase2=len(phase1_passed) if 'phase1_passed' in locals() else 0,
                final_selected=len(final_approved_list)
            )
            
            # Redis ê²°ê³¼ ì €ì¥ (Dashboardì—ì„œ ì¡°íšŒìš©)
            pipeline_results = [
                {
                    "stock_code": s.get('code'),
                    "stock_name": s.get('name'),
                    "grade": s.get('llm_metadata', {}).get('llm_grade', 'C'),
                    "final_score": s.get('llm_score', 0),
                    "selected": s.get('approved', False),
                    "judge_reason": s.get('llm_reason', ''),
                }
                for s in final_approved_list
            ]
            save_pipeline_results(pipeline_results)
            logger.info(f"   (Redis) Dashboardìš© ê²°ê³¼ ì €ì¥ ì™„ë£Œ ({len(pipeline_results)}ê°œ)")

    except Exception as e:
        logger.critical(f"âŒ 'Scout Job' ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
        # ì˜¤ë¥˜ ì‹œ Redis ìƒíƒœ ì—…ë°ì´íŠ¸
        update_pipeline_status(phase=0, phase_name="Error", status="error")
            
    logger.info(f"--- ğŸ¤– 'Scout Job' ì¢…ë£Œ (ì†Œìš”: {time.time() - start_time:.2f}ì´ˆ) ---")

if __name__ == "__main__":
    main()
