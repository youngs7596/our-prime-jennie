# services/scout-job/scout_cache.py
# Version: v1.0
# Scout Job Cache Management - Redis ìƒíƒœ ê´€ë¦¬ ë° LLM ìºì‹œ í•¨ìˆ˜ë“¤
#
# scout.pyì—ì„œ ë¶„ë¦¬ëœ ìºì‹œ/ìƒíƒœ ê´€ë¦¬ í•¨ìˆ˜ë“¤

import os
import json
import hashlib
import logging
import math
from datetime import datetime, timezone, timedelta
from typing import Dict, Tuple, List, Optional

import redis

import shared.database as database

logger = logging.getLogger(__name__)

# ìƒìˆ˜
STATE_PREFIX = "SCOUT"
CANDIDATE_DIGEST_SUFFIX = "CANDIDATE_DIGEST"
CANDIDATE_HASHES_SUFFIX = "CANDIDATE_HASHES"
LLM_CACHE_SUFFIX = "LLM_DECISIONS"
LLM_LAST_RUN_SUFFIX = "LAST_LLM_RUN_AT"
ISO_FORMAT_Z = "%Y-%m-%dT%H:%M:%S.%f%z"

# Redis ì—°ê²° (Dashboard ì‹¤ì‹œê°„ ìƒíƒœ í‘œì‹œìš©)
REDIS_URL = os.getenv("REDIS_URL", "redis://127.0.0.1:6379/0")
_redis_client = None


# =============================================================================
# Redis í´ë¼ì´ì–¸íŠ¸ ë° ìƒíƒœ ì—…ë°ì´íŠ¸
# =============================================================================

def _get_redis():
    """Redis í´ë¼ì´ì–¸íŠ¸ ì‹±ê¸€í†¤"""
    global _redis_client
    if _redis_client is None:
        try:
            _redis_client = redis.from_url(REDIS_URL, decode_responses=True)
            _redis_client.ping()
            logger.info("âœ… Redis ì—°ê²° ì„±ê³µ (Dashboard ìƒíƒœ ì—…ë°ì´íŠ¸ìš©)")
        except Exception as e:
            logger.warning(f"âš ï¸ Redis ì—°ê²° ì‹¤íŒ¨ (Dashboard ìƒíƒœ ì—…ë°ì´íŠ¸ ë¹„í™œì„±í™”): {e}")
            _redis_client = None
    return _redis_client


def _utcnow() -> datetime:
    return datetime.now(timezone.utc)


def update_pipeline_status(
    phase: int,
    phase_name: str,
    status: str = "running",
    progress: float = 0,
    current_stock: str = None,
    total_candidates: int = 0,
    passed_phase1: int = 0,
    passed_phase2: int = 0,
    final_selected: int = 0
):
    """
    Dashboardìš© Redis ìƒíƒœ ì—…ë°ì´íŠ¸
    Dashboardì˜ Scout Pipeline í˜ì´ì§€ì—ì„œ ì‹¤ì‹œê°„ìœ¼ë¡œ ì§„í–‰ ìƒí™©ì„ í‘œì‹œ
    """
    r = _get_redis()
    if not r:
        return
    
    try:
        r.hset("scout:pipeline:status", mapping={
            "phase": phase,
            "phase_name": phase_name,
            "status": status,
            "progress": progress,
            "current_stock": current_stock or "",
            "total_candidates": total_candidates,
            "passed_phase1": passed_phase1,
            "passed_phase2": passed_phase2,
            "final_selected": final_selected,
            "last_updated": _utcnow().isoformat(),
        })
    except Exception as e:
        logger.debug(f"Redis ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")


def save_pipeline_results(results: list):
    """
    Dashboardìš© ìµœì¢… ê²°ê³¼ ì €ì¥
    """
    r = _get_redis()
    if not r:
        return
    
    try:
        r.set("scout:pipeline:results", json.dumps(results, ensure_ascii=False, default=str))
        r.expire("scout:pipeline:results", 86400)  # 24ì‹œê°„ ìœ ì§€
    except Exception as e:
        logger.debug(f"Redis ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")


def save_hot_watchlist(
    stocks: list,
    market_regime: str,
    score_threshold: int,
    ttl_seconds: int = 86400
) -> bool:
    """
    Hot Watchlist Redis ì €ì¥ (ë²„ì €ë‹ + ìŠ¤ì™‘ íŒ¨í„´)
    
    Price Monitorì—ì„œ WebSocket êµ¬ë… ëŒ€ìƒìœ¼ë¡œ ì‚¬ìš©ë¨.
    
    Args:
        stocks: ì¢…ëª© ë¦¬ìŠ¤íŠ¸ [{"code": "005930", "name": "ì‚¼ì„±ì „ì", "llm_score": 72, ...}, ...]
        market_regime: í˜„ì¬ ì‹œì¥ êµ­ë©´ (STRONG_BULL, BULL, SIDEWAYS, BEAR)
        score_threshold: í˜„ì¬ ì ìš©ëœ LLM Score ì»¤íŠ¸ë¼ì¸
        ttl_seconds: Redis TTL (ê¸°ë³¸ 24ì‹œê°„)
    
    Returns:
        ì„±ê³µ ì—¬ë¶€
    """
    r = _get_redis()
    if not r:
        logger.warning("âš ï¸ Redis ì—°ê²° ì—†ìŒ - Hot Watchlist ì €ì¥ ì‹¤íŒ¨")
        return False
    
    try:
        from datetime import datetime, timezone
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë°˜ ë²„ì „ í‚¤ ìƒì„±
        timestamp = int(datetime.now(timezone.utc).timestamp())
        version_key = f"hot_watchlist:v{timestamp}"
        
        # Hot Watchlist ë°ì´í„° êµ¬ì„± (ë©”íƒ€ë°ì´í„° í¬í•¨)
        payload = {
            "generated_at": datetime.now(timezone.utc).isoformat(),
            "market_regime": market_regime,
            "score_threshold": score_threshold,
            "stocks": [
                {
                    "code": s.get("code"),
                    "name": s.get("name"),
                    "llm_score": s.get("llm_score", 0),
                    "rank": idx + 1,
                    "is_tradable": s.get("is_tradable", True),
                }
                for idx, s in enumerate(stocks)
                if s.get("code") != "0001"  # KOSPI ì§€ìˆ˜ ì œì™¸
            ]
        }
        
        # 1. ìƒˆ ë²„ì „ ì €ì¥
        r.set(version_key, json.dumps(payload, ensure_ascii=False, default=str))
        r.expire(version_key, ttl_seconds)
        
        # 2. active í¬ì¸í„° ìŠ¤ì™‘ (ì›ìì  ì „í™˜)
        old_version = r.get("hot_watchlist:active")
        r.set("hot_watchlist:active", version_key)
        
        # 3. ì´ì „ ë²„ì „ ì‚­ì œ (ì¦‰ì‹œ ì‚­ì œ ëŒ€ì‹  TTLë¡œ ìì—° ë§Œë£Œ - ë¡¤ë°± ê°€ëŠ¥)
        # ì´ì „ ë²„ì „ì€ TTLì´ ì„¤ì •ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ìë™ ì‚­ì œë¨
        
        stock_count = len(payload["stocks"])
        logger.info(f"   (Redis) âœ… Hot Watchlist ì €ì¥: {stock_count}ê°œ ì¢…ëª© (threshold: {score_threshold}ì , regime: {market_regime})")
        logger.info(f"   (Redis)    ë²„ì „: {version_key}")
        
        return True
        
    except Exception as e:
        logger.error(f"   (Redis) âŒ Hot Watchlist ì €ì¥ ì‹¤íŒ¨: {e}")
        return False


def get_hot_watchlist() -> dict:
    """
    Hot Watchlist Redisì—ì„œ ë¡œë“œ
    
    Returns:
        {
            "generated_at": "2026-01-09T08:30:00+00:00",
            "market_regime": "STRONG_BULL",
            "score_threshold": 58,
            "stocks": [{"code": "005930", "name": "ì‚¼ì„±ì „ì", "llm_score": 72, "rank": 1}, ...]
        }
        ë˜ëŠ” None
    """
    r = _get_redis()
    if not r:
        return None
    
    try:
        # active í¬ì¸í„°ì—ì„œ í˜„ì¬ ë²„ì „ í‚¤ ì¡°íšŒ
        version_key = r.get("hot_watchlist:active")
        if not version_key:
            logger.debug("Hot Watchlist active ë²„ì „ ì—†ìŒ")
            return None
        
        # ë²„ì „ í‚¤ì—ì„œ ë°ì´í„° ì¡°íšŒ
        data = r.get(version_key)
        if not data:
            logger.debug(f"Hot Watchlist ë°ì´í„° ì—†ìŒ: {version_key}")
            return None
        
        return json.loads(data)
        
    except Exception as e:
        logger.warning(f"Hot Watchlist ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None


def refilter_hot_watchlist_by_regime(new_regime: str) -> bool:
    """
    ì‹œì¥ êµ­ë©´ ë³€ê²½ ì‹œ Hot Watchlist ê²½ëŸ‰ ì¬í•„í„°ë§ (LLM ì¬í˜¸ì¶œ ì—†ìŒ)
    
    (ì¤€í˜¸/ë¯¼ì§€ ì œì•ˆ: ê¸°ì¡´ llm_scoreë¥¼ ì¬í™œìš©í•´ì„œ ìƒˆ ê¸°ì¤€ìœ¼ë¡œ í•„í„°ë§Œ ë‹¤ì‹œ ì ìš©)
    
    Args:
        new_regime: ìƒˆ ì‹œì¥ êµ­ë©´ (STRONG_BULL, BULL, SIDEWAYS, BEAR)
    
    Returns:
        ì„±ê³µ ì—¬ë¶€
    """
    r = _get_redis()
    if not r:
        logger.warning("âš ï¸ Redis ì—°ê²° ì—†ìŒ - ì¬í•„í„°ë§ ì‹¤íŒ¨")
        return False
    
    try:
        # 1. í˜„ì¬ Hot Watchlist ë¡œë“œ
        current = get_hot_watchlist()
        if not current or not current.get('stocks'):
            logger.info("Hot Watchlistê°€ ë¹„ì–´ìˆì–´ ì¬í•„í„°ë§ ë¶ˆí•„ìš”")
            return True
        
        old_regime = current.get('market_regime', 'UNKNOWN')
        if old_regime == new_regime:
            logger.info(f"ì‹œì¥ êµ­ë©´ ë³€ê²½ ì—†ìŒ ({new_regime}) - ì¬í•„í„°ë§ ìŠ¤í‚µ")
            return True
        
        # 2. ìƒˆ ì‹œì¥ êµ­ë©´ë³„ score threshold
        recon_score_by_regime = {
            "STRONG_BULL": 58,
            "BULL": 62,
            "SIDEWAYS": 65,
            "BEAR": 70,
        }
        new_threshold = recon_score_by_regime.get(new_regime, 65)
        old_threshold = current.get('score_threshold', 65)
        
        # 3. ìƒˆ ê¸°ì¤€ìœ¼ë¡œ ì¢…ëª© í•„í„°ë§ (llm_score >= new_threshold)
        original_stocks = current.get('stocks', [])
        filtered_stocks = [
            s for s in original_stocks 
            if s.get('llm_score', 0) >= new_threshold
        ]
        
        # ì¬ì •ë ¬ (llm_score ë‚´ë¦¼ì°¨ìˆœ)
        filtered_stocks = sorted(filtered_stocks, key=lambda x: x.get('llm_score', 0), reverse=True)
        
        logger.info(f"ğŸ”„ [Regime Change] {old_regime}({old_threshold}ì ) â†’ {new_regime}({new_threshold}ì )")
        logger.info(f"   ì¢…ëª© ìˆ˜: {len(original_stocks)} â†’ {len(filtered_stocks)}ê°œ")
        
        # 4. ìƒˆ ë²„ì „ìœ¼ë¡œ ì €ì¥ (ë²„ì €ë‹ íŒ¨í„´)
        return save_hot_watchlist(
            stocks=[
                {
                    'code': s['code'],
                    'name': s.get('name', s['code']),
                    'llm_score': s.get('llm_score', 0),
                    'is_tradable': s.get('is_tradable', True),
                }
                for s in filtered_stocks
            ],
            market_regime=new_regime,
            score_threshold=new_threshold
        )
        
    except Exception as e:
        logger.error(f"Hot Watchlist ì¬í•„í„°ë§ ì‹¤íŒ¨: {e}")
        return False


# =============================================================================
# CONFIG í…Œì´ë¸” ê¸°ë°˜ ìƒíƒœ ê´€ë¦¬
# =============================================================================

def _get_scope() -> str:
    return os.getenv("SCHEDULER_SCOPE", "real")


def _make_state_key(suffix: str) -> str:
    scope = _get_scope()
    return f"{STATE_PREFIX}::{scope}::{suffix}"


def _load_json_config(connection, suffix: str, default=None):
    raw = database.get_config(connection, _make_state_key(suffix), silent=True)
    if not raw:
        return default
    try:
        return json.loads(raw)
    except Exception as exc:
        logger.warning(f"âš ï¸ CONFIG '{suffix}' JSON íŒŒì‹± ì‹¤íŒ¨: {exc}")
        return default


def _save_json_config(connection, suffix: str, payload) -> None:
    serialized = json.dumps(payload, ensure_ascii=False)
    database.set_config(connection, _make_state_key(suffix), serialized)


def _get_last_llm_run_at(connection) -> Optional[datetime]:
    raw = database.get_config(connection, _make_state_key(LLM_LAST_RUN_SUFFIX), silent=True)
    if not raw:
        return None
    try:
        normalized = raw.replace("Z", "+00:00")
        return datetime.fromisoformat(normalized)
    except Exception as exc:
        logger.warning(f"âš ï¸ LAST_LLM_RUN_AT íŒŒì‹± ì‹¤íŒ¨: {exc}")
        return None


def _save_last_llm_run_at(connection, dt: datetime) -> None:
    database.set_config(connection, _make_state_key(LLM_LAST_RUN_SUFFIX), dt.astimezone(timezone.utc).isoformat())


def _load_candidate_state(connection) -> Tuple[str | None, Dict[str, str]]:
    digest = database.get_config(connection, _make_state_key(CANDIDATE_DIGEST_SUFFIX), silent=True)
    hashes = _load_json_config(connection, CANDIDATE_HASHES_SUFFIX, default={})
    return digest, hashes or {}


def _save_candidate_state(connection, digest: str, hashes: Dict[str, str]) -> None:
    database.set_config(connection, _make_state_key(CANDIDATE_DIGEST_SUFFIX), digest)
    _save_json_config(connection, CANDIDATE_HASHES_SUFFIX, hashes)


# =============================================================================
# ìºì‹œ ì‹œìŠ¤í…œ - LLM_EVAL_CACHE í…Œì´ë¸” ê¸°ë°˜
# =============================================================================

def _load_llm_cache_from_db(connection) -> Dict[str, Dict]:
    """
    LLM_EVAL_CACHE í…Œì´ë¸”ì—ì„œ ëª¨ë“  ìºì‹œ ë¡œë“œ
    
    Returns:
        Dict[stock_code, cache_entry]
    """
    cache = {}
    try:
        cursor = connection.cursor()
        cursor.execute("""
            SELECT STOCK_CODE, STOCK_NAME, PRICE_BUCKET, VOLUME_BUCKET, NEWS_HASH,
                   EVAL_DATE, HUNTER_SCORE, JUDGE_SCORE, LLM_GRADE, LLM_REASON,
                   NEWS_USED, IS_APPROVED, IS_TRADABLE, UPDATED_AT
            FROM LLM_EVAL_CACHE
        """)
        rows = cursor.fetchall()
        
        for row in rows:
            if isinstance(row, dict):
                code = row['STOCK_CODE']
                cache[code] = {
                    'stock_code': code,
                    'stock_name': row['STOCK_NAME'],
                    'price_bucket': row['PRICE_BUCKET'],
                    'volume_bucket': row['VOLUME_BUCKET'],
                    'news_hash': row['NEWS_HASH'],
                    'eval_date': str(row['EVAL_DATE']) if row['EVAL_DATE'] else None,
                    'hunter_score': row['HUNTER_SCORE'],
                    'judge_score': row['JUDGE_SCORE'],
                    'llm_grade': row['LLM_GRADE'],
                    'llm_reason': row['LLM_REASON'],
                    'news_used': row['NEWS_USED'],
                    'is_approved': bool(row['IS_APPROVED']),
                    'is_tradable': bool(row['IS_TRADABLE']),
                    'updated_at': row['UPDATED_AT'],
                }
            else:
                code = row[0]
                cache[code] = {
                    'stock_code': code,
                    'stock_name': row[1],
                    'price_bucket': row[2],
                    'volume_bucket': row[3],
                    'news_hash': row[4],
                    'eval_date': str(row[5]) if row[5] else None,
                    'hunter_score': row[6],
                    'judge_score': row[7],
                    'llm_grade': row[8],
                    'llm_reason': row[9],
                    'news_used': row[10],
                    'is_approved': bool(row[11]),
                    'is_tradable': bool(row[12]),
                    'updated_at': row[13],
                }
        cursor.close()
        logger.info(f"   (Cache) âœ… LLM_EVAL_CACHEì—ì„œ {len(cache)}ê°œ ë¡œë“œ")
    except Exception as e:
        logger.warning(f"   (Cache) âš ï¸ LLM_EVAL_CACHE ë¡œë“œ ì‹¤íŒ¨: {e}")
    return cache


def _save_llm_cache_to_db(connection, stock_code: str, cache_entry: Dict) -> None:
    """
    LLM_EVAL_CACHE í…Œì´ë¸”ì— ë‹¨ì¼ ì¢…ëª© ìºì‹œ ì €ì¥ (UPSERT)
    """
    try:
        cursor = connection.cursor()
        sql = """
            INSERT INTO LLM_EVAL_CACHE (
                STOCK_CODE, STOCK_NAME, PRICE_BUCKET, VOLUME_BUCKET, NEWS_HASH,
                EVAL_DATE, HUNTER_SCORE, JUDGE_SCORE, LLM_GRADE, LLM_REASON,
                NEWS_USED, IS_APPROVED, IS_TRADABLE, CREATED_AT, UPDATED_AT
            ) VALUES (
                %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, NOW(), NOW()
            ) ON DUPLICATE KEY UPDATE
                STOCK_NAME = VALUES(STOCK_NAME),
                PRICE_BUCKET = VALUES(PRICE_BUCKET),
                VOLUME_BUCKET = VALUES(VOLUME_BUCKET),
                NEWS_HASH = VALUES(NEWS_HASH),
                EVAL_DATE = VALUES(EVAL_DATE),
                HUNTER_SCORE = VALUES(HUNTER_SCORE),
                JUDGE_SCORE = VALUES(JUDGE_SCORE),
                LLM_GRADE = VALUES(LLM_GRADE),
                LLM_REASON = VALUES(LLM_REASON),
                NEWS_USED = VALUES(NEWS_USED),
                IS_APPROVED = VALUES(IS_APPROVED),
                IS_TRADABLE = VALUES(IS_TRADABLE),
                UPDATED_AT = NOW()
        """
        cursor.execute(sql, (
            stock_code,
            cache_entry.get('stock_name', ''),
            cache_entry.get('price_bucket', 0),
            cache_entry.get('volume_bucket', 0),
            cache_entry.get('news_hash'),
            cache_entry.get('eval_date'),
            cache_entry.get('hunter_score', 0),
            cache_entry.get('judge_score', 0),
            cache_entry.get('llm_grade'),
            cache_entry.get('llm_reason', '')[:60000] if cache_entry.get('llm_reason') else None,
            cache_entry.get('news_used', '')[:60000] if cache_entry.get('news_used') else None,
            1 if cache_entry.get('is_approved') else 0,
            1 if cache_entry.get('is_tradable') else 0,
        ))
        connection.commit()
        cursor.close()
    except Exception as e:
        logger.warning(f"   (Cache) âš ï¸ {stock_code} ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")


def _save_llm_cache_batch(connection, cache_entries: Dict[str, Dict]) -> None:
    """
    LLM_EVAL_CACHE í…Œì´ë¸”ì— ë°°ì¹˜ ì €ì¥
    """
    if not cache_entries:
        return
    
    try:
        cursor = connection.cursor()
        sql = """
            INSERT INTO LLM_EVAL_CACHE (
                STOCK_CODE, STOCK_NAME, PRICE_BUCKET, VOLUME_BUCKET, NEWS_HASH,
                EVAL_DATE, HUNTER_SCORE, JUDGE_SCORE, LLM_GRADE, LLM_REASON,
                NEWS_USED, IS_APPROVED, IS_TRADABLE, CREATED_AT, UPDATED_AT
            ) VALUES (
                %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, NOW(), NOW()
            ) ON DUPLICATE KEY UPDATE
                STOCK_NAME = VALUES(STOCK_NAME),
                PRICE_BUCKET = VALUES(PRICE_BUCKET),
                VOLUME_BUCKET = VALUES(VOLUME_BUCKET),
                NEWS_HASH = VALUES(NEWS_HASH),
                EVAL_DATE = VALUES(EVAL_DATE),
                HUNTER_SCORE = VALUES(HUNTER_SCORE),
                JUDGE_SCORE = VALUES(JUDGE_SCORE),
                LLM_GRADE = VALUES(LLM_GRADE),
                LLM_REASON = VALUES(LLM_REASON),
                NEWS_USED = VALUES(NEWS_USED),
                IS_APPROVED = VALUES(IS_APPROVED),
                IS_TRADABLE = VALUES(IS_TRADABLE),
                UPDATED_AT = NOW()
        """
        
        data = []
        for code, entry in cache_entries.items():
            data.append((
                code,
                entry.get('stock_name', ''),
                entry.get('price_bucket', 0),
                entry.get('volume_bucket', 0),
                entry.get('news_hash'),
                entry.get('eval_date'),
                entry.get('hunter_score', 0),
                entry.get('judge_score', 0),
                entry.get('llm_grade'),
                entry.get('llm_reason', '')[:60000] if entry.get('llm_reason') else None,
                entry.get('news_used', '')[:60000] if entry.get('news_used') else None,
                1 if entry.get('is_approved') else 0,
                1 if entry.get('is_tradable') else 0,
            ))
        
        cursor.executemany(sql, data)
        connection.commit()
        cursor.close()
        logger.info(f"   (Cache) âœ… LLM_EVAL_CACHEì— {len(data)}ê°œ ì €ì¥")
    except Exception as e:
        logger.warning(f"   (Cache) âš ï¸ ë°°ì¹˜ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")


# =============================================================================
# ìºì‹œ ìœ íš¨ì„± ê²€ì‚¬ ë° í•´ì‹œ ê³„ì‚°
# =============================================================================

def _is_cache_valid_direct(cached: Optional[Dict], current_data: Dict, today_str: str) -> bool:
    """
    ì§ì ‘ ë¹„êµë¡œ ìºì‹œ ìœ íš¨ì„± ê²€ì¦ (í•´ì‹œ ë¶ˆí•„ìš”!)
    
    Args:
        cached: DBì—ì„œ ë¡œë“œí•œ ìºì‹œ ë°ì´í„°
        current_data: í˜„ì¬ ì¢…ëª© ë°ì´í„° (price_bucket, volume_bucket, news_hash)
        today_str: ì˜¤ëŠ˜ ë‚ ì§œ (YYYY-MM-DD)
    
    Returns:
        Trueë©´ ìºì‹œ ì‚¬ìš©, Falseë©´ LLM ì¬í˜¸ì¶œ
    """
    if not cached:
        return False
    
    # 1. ë‚ ì§œê°€ ë‹¤ë¥´ë©´ ì¬í‰ê°€
    if cached.get('eval_date') != today_str:
        return False
    
    # 2. ê°€ê²© ë²„í‚·ì´ ë‹¤ë¥´ë©´ ì¬í‰ê°€ (5% ì´ìƒ ë³€ë™)
    if cached.get('price_bucket') != current_data.get('price_bucket'):
        return False
    
    # 3. ë‰´ìŠ¤ê°€ ë°”ë€Œë©´ ì¬í‰ê°€
    cached_news = cached.get('news_hash') or ''
    current_news = current_data.get('news_hash') or ''
    if cached_news != current_news:
        return False
    
    return True


def _get_price_bucket(price: float) -> int:
    """ê°€ê²©ì„ 5% ë²„í‚·ìœ¼ë¡œ ë³€í™˜ (ê°€ê²© ë³€ë™ ê°ì§€ìš©)"""
    if price <= 0:
        return 0
    # log ìŠ¤ì¼€ì¼ë¡œ 5% ë²„í‚· ìƒì„±
    bucket = int(math.log(price) / math.log(1.05))  # 5% ê°„ê²©
    return bucket


def _get_volume_bucket(volume: int) -> int:
    """ê±°ë˜ëŸ‰ì„ ë²„í‚·ìœ¼ë¡œ ë³€í™˜ (ê±°ë˜ëŸ‰ ê¸‰ë³€ ê°ì§€ìš©)"""
    if volume <= 0:
        return 0
    # ê±°ë˜ëŸ‰ì„ 10ë§Œì£¼ ë‹¨ìœ„ ë²„í‚·ìœ¼ë¡œ (ì˜ˆ: 500ë§Œì£¼ â†’ 50)
    return volume // 100000


def _get_foreign_direction(foreign_net: int) -> str:
    """ì™¸êµ­ì¸ ìˆœë§¤ìˆ˜ ë°©í–¥ (ë§¤ìˆ˜/ë§¤ë„/ì¤‘ë¦½)"""
    if foreign_net > 10000:  # 1ë§Œì£¼ ì´ìƒ ìˆœë§¤ìˆ˜
        return "buy"
    elif foreign_net < -10000:  # 1ë§Œì£¼ ì´ìƒ ìˆœë§¤ë„
        return "sell"
    return "neutral"


def _hash_candidate_payload(code: str, info: Dict) -> str:
    """
    ì¢…ëª©ë³„ í•´ì‹œ ìƒì„± (v4.1 - ì‹œì¥ ë°ì´í„° í¬í•¨)
    
    í•´ì‹œì— í¬í•¨ë˜ëŠ” ë°ì´í„°:
    - ì¢…ëª©ì½”ë“œ, ì¢…ëª©ëª…, ì„ ì •ì´ìœ  (ê¸°ë³¸)
    - ì˜¤ëŠ˜ ë‚ ì§œ (ë§¤ì¼ ì¬í‰ê°€ ë³´ì¥)
    - ê°€ê²© ë²„í‚· (5% ì´ìƒ ë³€ë™ ì‹œ ì¬í‰ê°€)
    - ê±°ë˜ëŸ‰ ë²„í‚· (ê¸‰ë³€ ê°ì§€)
    - ì™¸êµ­ì¸ ìˆœë§¤ìˆ˜ ë°©í–¥ (ìˆ˜ê¸‰ ë³€í™” ê°ì§€)
    - ìµœì‹  ë‰´ìŠ¤ ë‚ ì§œ (ë‰´ìŠ¤ ë³€ê²½ ì‹œ ì¬í‰ê°€)
    """
    # ì˜¤ëŠ˜ ë‚ ì§œ (KST ê¸°ì¤€)
    kst = timezone(timedelta(hours=9))
    today_kst = datetime.now(kst).strftime("%Y-%m-%d")
    
    normalized = {
        "code": code,
        "name": info.get("name"),
        "reasons": sorted(info.get("reasons", [])),
        "date": today_kst,  # ë§¤ì¼ ì¬í‰ê°€ ë³´ì¥
    }
    
    # ì‹œì¥ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ë²„í‚·í™”í•˜ì—¬ í¬í•¨
    if "price" in info:
        normalized["price_bucket"] = _get_price_bucket(info["price"])
    if "volume" in info:
        normalized["volume_bucket"] = _get_volume_bucket(info["volume"])
    if "foreign_net" in info:
        normalized["foreign_direction"] = _get_foreign_direction(info["foreign_net"])
    
    # ë‰´ìŠ¤ í•´ì‹œ (ë‚´ìš© ê¸°ë°˜ - ì‹œê°„ ì •ë³´ í¬í•¨)
    if "news_hash" in info:
        normalized["news_hash"] = info["news_hash"]
    
    # ë‚˜ë¨¸ì§€ í‚¤ë“¤ë„ í¬í•¨ (ê¸°ì¡´ í˜¸í™˜ì„±)
    for key in sorted(k for k in info.keys() if k not in ("name", "reasons", "price", "volume", "foreign_net", "news_date")):
        value = info.get(key)
        if isinstance(value, list):
            normalized[key] = sorted(value)
        else:
            normalized[key] = value
    
    serialized = json.dumps(normalized, ensure_ascii=True, sort_keys=True, default=str)
    return hashlib.sha256(serialized.encode("utf-8")).hexdigest()


def _compute_candidate_hashes(candidate_stocks: Dict[str, Dict]) -> Tuple[Dict[str, str], str]:
    per_stock = {}
    for code in candidate_stocks:
        per_stock[code] = _hash_candidate_payload(code, candidate_stocks[code])
    digest_source = "".join(per_stock[code] for code in sorted(per_stock))
    overall_digest = hashlib.sha256(digest_source.encode("utf-8")).hexdigest()
    return per_stock, overall_digest


def _minutes_since(timestamp: Optional[datetime]) -> float:
    if not timestamp:
        return float("inf")
    delta = _utcnow() - timestamp.astimezone(timezone.utc)
    return delta.total_seconds() / 60.0


def _parse_int_env(value: Optional[str], default: int) -> int:
    try:
        return int(value) if value is not None else default
    except ValueError:
        return default


def _is_cache_entry_valid(entry: Optional[Dict], decision_hash: str, ttl_minutes: int) -> bool:
    if not entry:
        return False
    if entry.get("decision_hash") != decision_hash:
        return False
    if ttl_minutes <= 0:
        return True
    updated_at = entry.get("llm_updated_at")
    if not updated_at:
        return False
    try:
        ts = datetime.fromisoformat(updated_at.replace("Z", "+00:00"))
    except Exception:
        return False
    return _minutes_since(ts) < ttl_minutes


def _record_to_watchlist_entry(record: Dict) -> Dict:
    return {
        "code": record["code"],
        "name": record["name"],
        "is_tradable": record.get("is_tradable", True),
        "llm_score": record.get("llm_score", 0),
        "llm_reason": record.get("llm_reason", ""),
        "llm_metadata": record.get("llm_metadata", {}),
        # ì¬ë¬´ ë°ì´í„° ì¶”ê°€ (scout íŒŒì´í”„ë¼ì¸ì—ì„œ ì „ë‹¬ë¨)
        "per": record.get("per"),
        "pbr": record.get("pbr"),
        "roe": record.get("roe"),
        "market_cap": record.get("market_cap"),
        "sales_growth": record.get("sales_growth"),
        "eps_growth": record.get("eps_growth"),
        "financial_updated_at": _utcnow().isoformat(),
    }


def _record_to_cache_payload(record: Dict) -> Dict:
    metadata = record.get("llm_metadata", {})
    return {
        "code": record["code"],
        "name": record["name"],
        "llm_score": record.get("llm_score", 0),
        "llm_reason": record.get("llm_reason", ""),
        "llm_grade": metadata.get("llm_grade"),
        "decision_hash": metadata.get("decision_hash"),
        "llm_updated_at": metadata.get("llm_updated_at"),
        "is_tradable": record.get("is_tradable", True),
        "approved": record.get("approved", False),
    }


def _cache_payload_to_record(entry: Dict, decision_hash: str) -> Dict:
    updated_at = entry.get("llm_updated_at")
    metadata = {
        "llm_grade": entry.get("llm_grade"),
        "decision_hash": decision_hash,
        "llm_updated_at": updated_at,
        "source": "cache",
    }
    return {
        "code": entry["code"],
        "name": entry.get("name", entry["code"]),
        "llm_score": entry.get("llm_score", 0),
        "llm_reason": entry.get("llm_reason", ""),
        "is_tradable": entry.get("is_tradable", True),
        "approved": entry.get("approved", False),
        "llm_metadata": metadata,
    }
