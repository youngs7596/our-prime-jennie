#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
services/news-analyzer/analyzer.py
-----------------------------------
ë‰´ìŠ¤ ë¶„ì„ ì „ìš© ì„œë¹„ìŠ¤ (Consumer B).
Redis Streamsì—ì„œ ë‰´ìŠ¤ë¥¼ ì†Œë¹„í•˜ì—¬ LLM ê°ì„±ë¶„ì„ í›„ MariaDBì— ì €ì¥í•©ë‹ˆë‹¤.
Archiverì™€ ë…ë¦½ì ìœ¼ë¡œ ë™ì‘í•˜ë©°, ëŠë ¤ë„ ë©ë‹ˆë‹¤.
"""

import os
import sys
import logging
import time
from typing import Dict, Any, Optional
from datetime import datetime, timezone

from dotenv import load_dotenv

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì„¤ì •
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, PROJECT_ROOT)

from shared.messaging.stream_client import (
    consume_messages,
    get_stream_length,
    get_pending_count,
    STREAM_NEWS_RAW,
    GROUP_ANALYZER
)


def _extract_news_title(page_content: str) -> str:
    """page_contentì—ì„œ ë‰´ìŠ¤ ì œëª© ì¶”ì¶œ (ì‹ ê·œ/ë ˆê±°ì‹œ í¬ë§· ëª¨ë‘ ì§€ì›)

    New format: "[ì¢…ëª©ëª…(ì½”ë“œ)] ì œëª© | ì¶œì²˜: ... | ë‚ ì§œ: ..."
    New format (general): "[ì‹œì¥ë‰´ìŠ¤] ì œëª© | ì¶œì²˜: ... | ë‚ ì§œ: ..."
    Legacy format: "ë‰´ìŠ¤ ì œëª©: ...\në§í¬: ..."
    """
    if not page_content:
        return "ì œëª© ì—†ìŒ"
    if page_content.startswith("["):
        bracket_end = page_content.find("] ")
        pipe_pos = page_content.find(" | ì¶œì²˜:")
        if bracket_end != -1 and pipe_pos != -1:
            return page_content[bracket_end + 2:pipe_pos].strip()
    # Legacy fallback
    lines = page_content.split('\n')
    return lines[0].replace("ë‰´ìŠ¤ ì œëª©: ", "").strip() if lines else "ì œëª© ì—†ìŒ"

# ==============================================================================
# Logging
# ==============================================================================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - [%(funcName)s] - %(message)s',
)
logger = logging.getLogger(__name__)

# ==============================================================================
# Configuration
# ==============================================================================
load_dotenv()

# LLM Model
LOCAL_MODEL_FAST = os.getenv("LOCAL_MODEL_FAST", "gemma3:27b")

# Batch Settings (for LLM)
ANALYSIS_BATCH_SIZE = int(os.getenv("ANALYZER_BATCH_SIZE", "5"))

# ==============================================================================
# JennieBrain Initialization
# ==============================================================================

_jennie_brain = None


def get_jennie_brain():
    """JennieBrain ì‹±ê¸€í†¤ ë°˜í™˜"""
    global _jennie_brain
    
    if _jennie_brain is None:
        from shared.llm import JennieBrain
        
        logger.info("ğŸ§  JennieBrain ì´ˆê¸°í™” ì¤‘...")
        _jennie_brain = JennieBrain(
            project_id=os.getenv("GCP_PROJECT_ID", "local"),
            gemini_api_key_secret=os.getenv("SECRET_ID_GEMINI_API_KEY")
        )
        logger.info("âœ… JennieBrain ì´ˆê¸°í™” ì™„ë£Œ")
    
    return _jennie_brain


# ==============================================================================
# DB Connection
# ==============================================================================

def get_db_session():
    """SQLAlchemy ì„¸ì…˜ ë°˜í™˜"""
    from shared.db.connection import session_scope
    return session_scope


# ==============================================================================
# Analysis Handler
# ==============================================================================

# Buffer for batch processing
_message_buffer = []


def _save_sentiment_to_db(
    stock_code: str,
    stock_name: str,
    news_title: str,
    score: float,
    reason: str,
    source_url: str,
    published_at: Optional[int]
) -> bool:
    """ê°ì„± ì ìˆ˜ë¥¼ DBì— ì €ì¥"""
    try:
        from shared.database.market import save_news_sentiment
        from shared.db.connection import session_scope
        
        # Convert timestamp to datetime
        pub_datetime = None
        if published_at:
            pub_datetime = datetime.fromtimestamp(published_at, tz=timezone.utc)
        
        with session_scope() as session:
            save_news_sentiment(
                session=session,
                stock_code=stock_code,
                title=news_title,
                score=score,
                reason=reason,
                url=source_url,
                published_at=pub_datetime
            )
        
        return True
    except Exception as e:
        logger.error(f"âŒ DB ì €ì¥ ì‹¤íŒ¨: {e}")
        return False


def _check_if_article_exists_in_db(source_url: str) -> bool:
    """DBì— ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ë‰´ìŠ¤ì¸ì§€ í™•ì¸ (LLM ë¶ˆí•„ìš” í˜¸ì¶œ ë°©ì§€)"""
    if not source_url:
        return False
        
    try:
        from shared.db.models import StockNewsSentiment
        from shared.db.connection import session_scope
        from sqlalchemy import select

        with session_scope() as session:
            stmt = select(StockNewsSentiment.id).where(StockNewsSentiment.article_url == source_url)
            existing = session.execute(stmt).first()
            return existing is not None
    except Exception as e:
        logger.error(f"âŒ DB ì¤‘ë³µ ì²´í¬ ì‹¤íŒ¨: {e}")
        return False


def _process_batch_analysis(batch: list) -> int:
    """ë°°ì¹˜ ë¶„ì„ ìˆ˜í–‰ ë° ì €ì¥"""
    if not batch:
        return 0
    
    # 1. Pre-filter duplicates (Save GPU/LLM cost)
    unique_batch = []
    skipped_count = 0
    
    for item in batch:
        metadata = item["metadata"]
        url = metadata.get("source_url")
        
        if _check_if_article_exists_in_db(url):
            skipped_count += 1
            continue
            
        unique_batch.append(item)
    
    if skipped_count > 0:
        logger.info(f"â„¹ï¸ [Analyzer] ì¤‘ë³µ ë‰´ìŠ¤ {skipped_count}ê±´ ë¶„ì„ ìƒëµ (Skip)")
        
    if not unique_batch:
        return 0

    brain = get_jennie_brain()
    
    # Prepare items for LLM
    batch_items = []
    for idx, item in enumerate(unique_batch):
        news_title = _extract_news_title(item["page_content"])
        
        batch_items.append({
            "id": idx,
            "title": news_title,
            "summary": news_title
        })
    
    # Call LLM
    try:
        results = brain.analyze_news_unified(batch_items)
    except Exception as e:
        logger.error(f"âŒ LLM ë¶„ì„ ì‹¤íŒ¨: {e}")
        return 0
    
    # Save results
    saved = 0
    for result in results:
        idx = result.get("id")
        if idx is None or idx >= len(unique_batch):
            continue
        
        item = unique_batch[idx]
        metadata = item["metadata"]
        
        stock_code = metadata.get("stock_code")
        if not stock_code:
            continue  # Skip non-stock news
        
        sentiment = result.get("sentiment", {})
        score = sentiment.get("score", 50)
        reason = sentiment.get("reason", "N/A")
        
        news_title = _extract_news_title(item["page_content"])

        success = _save_sentiment_to_db(
            stock_code=stock_code,
            stock_name=metadata.get("stock_name", ""),
            news_title=news_title,
            score=score,
            reason=reason,
            source_url=metadata.get("source_url", ""),
            published_at=metadata.get("created_at_utc")
        )
        
        if success:
            saved += 1
    
    logger.info(f"âœ… [Analyzer] ë°°ì¹˜ ë¶„ì„ ì™„ë£Œ: {saved}/{len(unique_batch)}ê±´ ì €ì¥ (Skipped: {skipped_count})")
    return saved


def handle_analyze_message(page_content: str, metadata: Dict[str, Any]) -> bool:
    """
    ë‰´ìŠ¤ ë©”ì‹œì§€ë¥¼ ë¶„ì„ìš© ë²„í¼ì— ì¶”ê°€í•©ë‹ˆë‹¤.
    ë²„í¼ê°€ ê°€ë“ ì°¨ë©´ ë°°ì¹˜ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    
    Note: ì´ í•¨ìˆ˜ëŠ” consume_messagesì˜ í•¸ë“¤ëŸ¬ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.
          ë‹¨, ë²„í¼ë§ ë°©ì‹ì´ë¯€ë¡œ ì¦‰ì‹œ ACKí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
          ê°„ë‹¨í•˜ê²Œ ë§¤ ë©”ì‹œì§€ë§ˆë‹¤ ë¶„ì„í•©ë‹ˆë‹¤ (ë°°ì¹˜ëŠ” ì¶”í›„ ìµœì í™”).
    """
    # Skip non-stock news (general news)
    stock_code = metadata.get("stock_code")
    if not stock_code:
        logger.info("[Analyzer] ì¢…ëª© ì½”ë“œ ì—†ìŒ (ì¼ë°˜ ë‰´ìŠ¤) â†’ Skip")
        return True  # ACK but don't analyze
    
    # CRITICAL: Check DB for Duplicate BEFORE LLM Call
    source_url = metadata.get("source_url")
    if _check_if_article_exists_in_db(source_url):
        logger.info(f"â„¹ï¸ [Analyzer] ì´ë¯¸ ë¶„ì„ëœ ë‰´ìŠ¤ì…ë‹ˆë‹¤. (Skip): {metadata.get('stock_name')} - URL ì¤‘ë³µ")
        return True # ACK to remove from stream
    
    brain = get_jennie_brain()
    
    # Prepare single item
    news_title = _extract_news_title(page_content)
    
    # [Fast Track] Emergency Keyword Check
    # Keywords: ì†ë³´, ê¸´ê¸‰, ì „ìŸ, ê´€ì„¸, Emergency, Breaking, íŒŒë³‘, ê³„ì—„
    FAST_TRACK_KEYWORDS = ["ì†ë³´", "ê¸´ê¸‰", "ì „ìŸ", "ê´€ì„¸", "Emergency", "Breaking", "íŒŒë³‘", "ê³„ì—„", "ê³µìŠµ", "í­ê²©"]
    is_emergency = any(k in news_title for k in FAST_TRACK_KEYWORDS)
    
    results = []
    
    if is_emergency:
        # ğŸš€ Use GPT-5-Nano (Reasoning Tier)
        logger.info(f"ğŸš€ [Analyzer] Fast Track Detected: {news_title}")
        try:
            # result is a single dict
            fast_result = brain.analyze_news_fast_track(news_title, news_title)
            results = [fast_result]
        except Exception as e:
            logger.error(f"âŒ [Analyzer] Fast Track ì‹¤íŒ¨, Normal Flowë¡œ í´ë°±: {e}")
            # Fast Track ì‹¤íŒ¨ ì‹œ Normal Flowë¡œ í´ë°±
            try:
                batch_item = {"id": 0, "title": news_title, "summary": news_title}
                results = brain.analyze_news_unified([batch_item])
            except Exception as e2:
                logger.error(f"âŒ [Analyzer] Normal Flow í´ë°±ë„ ì‹¤íŒ¨: {e2}")
    else:
        # ğŸ¢ Use Local Ollama (Batched logic but here single)
        batch_item = {
            "id": 0,
            "title": news_title,
            "summary": news_title
        }
        
        # Call LLM
        try:
            results = brain.analyze_news_unified([batch_item])
        except Exception as e:
            logger.error(f"âŒ [Analyzer] Normal Analysis Failed: {e}")
            return False

    if not results:
        logger.warning(f"âš ï¸ [Analyzer] LLM ê²°ê³¼ ì—†ìŒ: {news_title[:30]}...")
        return True  # ACK anyway
    
    # Save Results (Common Logic)
    try:
        sentiment = results[0].get("sentiment", {})
        score = sentiment.get("score", 50)
        reason = sentiment.get("reason", "N/A")
        
        # Competitor Risk also available in Unified/FastTrack result
        competitor_risk = results[0].get("competitor_risk", {})
        risk_detected = competitor_risk.get("is_detected", False)
        
        if risk_detected:
            # Append risk info to reason
            reason += f" [RISK: {competitor_risk.get('type')}]"

        success = _save_sentiment_to_db(
            stock_code=stock_code,
            stock_name=metadata.get("stock_name", ""),
            news_title=news_title,
            score=score,
            reason=reason,
            source_url=metadata.get("source_url", ""),
            published_at=metadata.get("created_at_utc")
        )
        
        if success:
            track_icon = "ğŸš€" if is_emergency else "ğŸ¢"
            logger.info(f"âœ… [Analyzer] {track_icon} {metadata.get('stock_name', stock_code)}: {score}ì  - {news_title[:20]}")
        
        return True  # ACK

    except Exception as e:
        logger.error(f"âŒ [Analyzer] ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
        return False  # Don't ACK, will retry
    



# ==============================================================================
# Main Analyzer
# ==============================================================================

def run_analyzer_daemon(consumer_name: str = "analyzer_1"):
    """
    Analyzer ë°ëª¬ ì‹¤í–‰ (ë¬´í•œ ë£¨í”„)
    Redis Streamì—ì„œ ë©”ì‹œì§€ë¥¼ ì†Œë¹„í•˜ì—¬ LLM ë¶„ì„ í›„ MariaDBì— ì €ì¥í•©ë‹ˆë‹¤.
    """
    logger.info("=" * 60)
    logger.info("ğŸš€ [Analyzer Daemon] ì‹œì‘")
    logger.info(f"   Stream: {STREAM_NEWS_RAW}")
    logger.info(f"   Group: {GROUP_ANALYZER}")
    logger.info(f"   Consumer: {consumer_name}")
    logger.info(f"   LLM Model: {LOCAL_MODEL_FAST}")
    logger.info("=" * 60)
    
    # Pre-initialize DB connection
    try:
        from shared.db.connection import ensure_engine_initialized
        ensure_engine_initialized()
        logger.info("âœ… DB ì—°ê²° ì´ˆê¸°í™” ì™„ë£Œ")
    except Exception as e:
        logger.error(f"âŒ DB ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return
    
    # Pre-initialize JennieBrain
    try:
        get_jennie_brain()
    except Exception as e:
        logger.error(f"âŒ JennieBrain ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return
    
    # Start consuming (slower than Archiver)
    processed = consume_messages(
        group_name=GROUP_ANALYZER,
        consumer_name=consumer_name,
        handler=handle_analyze_message,
        stream_name=STREAM_NEWS_RAW,
        batch_size=1,  # One at a time for LLM (sequential)
        block_ms=5000,  # Wait longer between messages
        max_iterations=None  # Infinite
    )
    
    logger.info(f"âœ… [Analyzer] ì¢…ë£Œ - ì´ {processed}ê°œ ì²˜ë¦¬")


def run_analyzer_once(max_messages: int = 100):
    """
    Analyzer 1íšŒ ì‹¤í–‰ (Airflow Taskìš©)
    """
    logger.info(f"ğŸš€ [Analyzer] 1íšŒ ì‹¤í–‰ (max: {max_messages})")
    
    stream_len = get_stream_length(STREAM_NEWS_RAW)
    pending = get_pending_count(STREAM_NEWS_RAW, GROUP_ANALYZER)
    logger.info(f"ğŸ“Š Stream ìƒíƒœ: ê¸¸ì´={stream_len}, ëŒ€ê¸°={pending}")
    
    # Pre-initialize DB connection
    try:
        from shared.db.connection import ensure_engine_initialized
        ensure_engine_initialized()
        logger.info("âœ… DB ì—°ê²° ì´ˆê¸°í™” ì™„ë£Œ")
    except Exception as e:
        logger.error(f"âŒ DB ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return
    
    try:
        get_jennie_brain()
    except Exception as e:
        logger.error(f"âŒ JennieBrain ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return
    
    processed = consume_messages(
        group_name=GROUP_ANALYZER,
        consumer_name="analyzer_batch",
        handler=handle_analyze_message,
        stream_name=STREAM_NEWS_RAW,
        batch_size=1,
        block_ms=2000,
        max_iterations=max_messages
    )
    
    logger.info(f"âœ… [Analyzer] ì™„ë£Œ - {processed}ê°œ ì²˜ë¦¬")


if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--daemon", action="store_true", help="Run as daemon (infinite loop)")
    parser.add_argument("--name", type=str, default="analyzer_1", help="Consumer name")
    parser.add_argument("--max", type=int, default=100, help="Max messages for one-shot mode")
    args = parser.parse_args()
    
    if args.daemon:
        run_analyzer_daemon(args.name)
    else:
        run_analyzer_once(args.max)
