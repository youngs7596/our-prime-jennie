#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
services/news-analyzer/analyzer.py
-----------------------------------
ë‰´ìŠ¤ ë¶„ì„ ì „ìš© ì„œë¹„ìŠ¤ (Consumer B).
Redis Streamsì—ì„œ ë‰´ìŠ¤ë¥¼ ì†Œë¹„í•˜ì—¬ LLM ê°ì„±ë¶„ì„ í›„ MariaDBì— ì €ì¥í•©ë‹ˆë‹¤.
Archiverì™€ ë…ë¦½ì ìœ¼ë¡œ ë™ì‘í•˜ë©°, ëŠë ¤ë„ ë©ë‹ˆë‹¤.
"""

import os
import sys
import logging
import time
from typing import Dict, Any, Optional
from datetime import datetime, timezone

from dotenv import load_dotenv

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì„¤ì •
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, PROJECT_ROOT)

from shared.messaging.stream_client import (
    consume_messages,
    get_stream_length,
    get_pending_count,
    STREAM_NEWS_RAW,
    GROUP_ANALYZER
)

# ==============================================================================
# Logging
# ==============================================================================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - [%(funcName)s] - %(message)s',
)
logger = logging.getLogger(__name__)

# ==============================================================================
# Configuration
# ==============================================================================
load_dotenv()

# LLM Model
LOCAL_MODEL_FAST = os.getenv("LOCAL_MODEL_FAST", "gemma3:27b")

# Batch Settings (for LLM)
ANALYSIS_BATCH_SIZE = int(os.getenv("ANALYZER_BATCH_SIZE", "5"))

# ==============================================================================
# JennieBrain Initialization
# ==============================================================================

_jennie_brain = None


def get_jennie_brain():
    """JennieBrain ì‹±ê¸€í†¤ ë°˜í™˜"""
    global _jennie_brain
    
    if _jennie_brain is None:
        from shared.llm import JennieBrain
        
        logger.info("ğŸ§  JennieBrain ì´ˆê¸°í™” ì¤‘...")
        _jennie_brain = JennieBrain(
            project_id=os.getenv("GCP_PROJECT_ID", "local"),
            gemini_api_key_secret=os.getenv("SECRET_ID_GEMINI_API_KEY")
        )
        logger.info("âœ… JennieBrain ì´ˆê¸°í™” ì™„ë£Œ")
    
    return _jennie_brain


# ==============================================================================
# DB Connection
# ==============================================================================

def get_db_session():
    """SQLAlchemy ì„¸ì…˜ ë°˜í™˜"""
    from shared.db.connection import session_scope
    return session_scope


# ==============================================================================
# Analysis Handler
# ==============================================================================

# Buffer for batch processing
_message_buffer = []


def _save_sentiment_to_db(
    stock_code: str,
    stock_name: str,
    news_title: str,
    score: float,
    reason: str,
    source_url: str,
    published_at: Optional[int]
) -> bool:
    """ê°ì„± ì ìˆ˜ë¥¼ DBì— ì €ì¥"""
    try:
        from shared.database.market import save_news_sentiment
        from shared.db.connection import session_scope
        
        # Convert timestamp to datetime
        pub_datetime = None
        if published_at:
            pub_datetime = datetime.fromtimestamp(published_at, tz=timezone.utc)
        
        with session_scope() as session:
            save_news_sentiment(
                session=session,
                stock_code=stock_code,
                title=news_title,
                score=score,
                reason=reason,
                url=source_url,
                published_at=pub_datetime
            )
        
        return True
    except Exception as e:
        logger.error(f"âŒ DB ì €ì¥ ì‹¤íŒ¨: {e}")
        return False


def _process_batch_analysis(batch: list) -> int:
    """ë°°ì¹˜ ë¶„ì„ ìˆ˜í–‰ ë° ì €ì¥"""
    if not batch:
        return 0
    
    brain = get_jennie_brain()
    
    # Prepare items for LLM
    batch_items = []
    for idx, item in enumerate(batch):
        content_lines = item["page_content"].split('\n')
        news_title = content_lines[0].replace("ë‰´ìŠ¤ ì œëª©: ", "") if content_lines else "ì œëª© ì—†ìŒ"
        
        batch_items.append({
            "id": idx,
            "title": news_title,
            "summary": news_title
        })
    
    # Call LLM
    try:
        results = brain.analyze_news_unified(batch_items)
    except Exception as e:
        logger.error(f"âŒ LLM ë¶„ì„ ì‹¤íŒ¨: {e}")
        return 0
    
    # Save results
    saved = 0
    for result in results:
        idx = result.get("id")
        if idx is None or idx >= len(batch):
            continue
        
        item = batch[idx]
        metadata = item["metadata"]
        
        stock_code = metadata.get("stock_code")
        if not stock_code:
            continue  # Skip non-stock news
        
        sentiment = result.get("sentiment", {})
        score = sentiment.get("score", 50)
        reason = sentiment.get("reason", "N/A")
        
        content_lines = item["page_content"].split('\n')
        news_title = content_lines[0].replace("ë‰´ìŠ¤ ì œëª©: ", "") if content_lines else "ì œëª© ì—†ìŒ"
        
        success = _save_sentiment_to_db(
            stock_code=stock_code,
            stock_name=metadata.get("stock_name", ""),
            news_title=news_title,
            score=score,
            reason=reason,
            source_url=metadata.get("source_url", ""),
            published_at=metadata.get("created_at_utc")
        )
        
        if success:
            saved += 1
    
    logger.info(f"âœ… [Analyzer] ë°°ì¹˜ ë¶„ì„ ì™„ë£Œ: {saved}/{len(batch)}ê±´ ì €ì¥")
    return saved


def handle_analyze_message(page_content: str, metadata: Dict[str, Any]) -> bool:
    """
    ë‰´ìŠ¤ ë©”ì‹œì§€ë¥¼ ë¶„ì„ìš© ë²„í¼ì— ì¶”ê°€í•©ë‹ˆë‹¤.
    ë²„í¼ê°€ ê°€ë“ ì°¨ë©´ ë°°ì¹˜ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    
    Note: ì´ í•¨ìˆ˜ëŠ” consume_messagesì˜ í•¸ë“¤ëŸ¬ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.
          ë‹¨, ë²„í¼ë§ ë°©ì‹ì´ë¯€ë¡œ ì¦‰ì‹œ ACKí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
          ê°„ë‹¨í•˜ê²Œ ë§¤ ë©”ì‹œì§€ë§ˆë‹¤ ë¶„ì„í•©ë‹ˆë‹¤ (ë°°ì¹˜ëŠ” ì¶”í›„ ìµœì í™”).
    """
    # Skip non-stock news (general news)
    stock_code = metadata.get("stock_code")
    if not stock_code:
        logger.info("[Analyzer] ì¢…ëª© ì½”ë“œ ì—†ìŒ (ì¼ë°˜ ë‰´ìŠ¤) â†’ Skip")
        return True  # ACK but don't analyze
    
    brain = get_jennie_brain()
    
    # Prepare single item
    content_lines = page_content.split('\n')
    news_title = content_lines[0].replace("ë‰´ìŠ¤ ì œëª©: ", "") if content_lines else "ì œëª© ì—†ìŒ"
    
    batch_item = {
        "id": 0,
        "title": news_title,
        "summary": news_title
    }
    
    # Call LLM
    try:
        results = brain.analyze_news_unified([batch_item])
        if not results:
            logger.warning(f"âš ï¸ [Analyzer] LLM ê²°ê³¼ ì—†ìŒ: {news_title[:30]}...")
            return True  # ACK anyway
        
        sentiment = results[0].get("sentiment", {})
        score = sentiment.get("score", 50)
        reason = sentiment.get("reason", "N/A")
        
        success = _save_sentiment_to_db(
            stock_code=stock_code,
            stock_name=metadata.get("stock_name", ""),
            news_title=news_title,
            score=score,
            reason=reason,
            source_url=metadata.get("source_url", ""),
            published_at=metadata.get("created_at_utc")
        )
        
        if success:
            logger.info(f"âœ… [Analyzer] {metadata.get('stock_name', stock_code)}: {score}ì ")
        
        return True  # ACK
    
    except Exception as e:
        logger.error(f"âŒ [Analyzer] ë¶„ì„ ì˜¤ë¥˜: {e}")
        return False  # Don't ACK, will retry


# ==============================================================================
# Main Analyzer
# ==============================================================================

def run_analyzer_daemon(consumer_name: str = "analyzer_1"):
    """
    Analyzer ë°ëª¬ ì‹¤í–‰ (ë¬´í•œ ë£¨í”„)
    Redis Streamì—ì„œ ë©”ì‹œì§€ë¥¼ ì†Œë¹„í•˜ì—¬ LLM ë¶„ì„ í›„ MariaDBì— ì €ì¥í•©ë‹ˆë‹¤.
    """
    logger.info("=" * 60)
    logger.info("ğŸš€ [Analyzer Daemon] ì‹œì‘")
    logger.info(f"   Stream: {STREAM_NEWS_RAW}")
    logger.info(f"   Group: {GROUP_ANALYZER}")
    logger.info(f"   Consumer: {consumer_name}")
    logger.info(f"   LLM Model: {LOCAL_MODEL_FAST}")
    logger.info("=" * 60)
    
    # Pre-initialize DB connection
    try:
        from shared.db.connection import ensure_engine_initialized
        ensure_engine_initialized()
        logger.info("âœ… DB ì—°ê²° ì´ˆê¸°í™” ì™„ë£Œ")
    except Exception as e:
        logger.error(f"âŒ DB ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return
    
    # Pre-initialize JennieBrain
    try:
        get_jennie_brain()
    except Exception as e:
        logger.error(f"âŒ JennieBrain ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return
    
    # Start consuming (slower than Archiver)
    processed = consume_messages(
        group_name=GROUP_ANALYZER,
        consumer_name=consumer_name,
        handler=handle_analyze_message,
        stream_name=STREAM_NEWS_RAW,
        batch_size=1,  # One at a time for LLM (sequential)
        block_ms=5000,  # Wait longer between messages
        max_iterations=None  # Infinite
    )
    
    logger.info(f"âœ… [Analyzer] ì¢…ë£Œ - ì´ {processed}ê°œ ì²˜ë¦¬")


def run_analyzer_once(max_messages: int = 100):
    """
    Analyzer 1íšŒ ì‹¤í–‰ (Airflow Taskìš©)
    """
    logger.info(f"ğŸš€ [Analyzer] 1íšŒ ì‹¤í–‰ (max: {max_messages})")
    
    stream_len = get_stream_length(STREAM_NEWS_RAW)
    pending = get_pending_count(STREAM_NEWS_RAW, GROUP_ANALYZER)
    logger.info(f"ğŸ“Š Stream ìƒíƒœ: ê¸¸ì´={stream_len}, ëŒ€ê¸°={pending}")
    
    # Pre-initialize DB connection
    try:
        from shared.db.connection import ensure_engine_initialized
        ensure_engine_initialized()
        logger.info("âœ… DB ì—°ê²° ì´ˆê¸°í™” ì™„ë£Œ")
    except Exception as e:
        logger.error(f"âŒ DB ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return
    
    try:
        get_jennie_brain()
    except Exception as e:
        logger.error(f"âŒ JennieBrain ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return
    
    processed = consume_messages(
        group_name=GROUP_ANALYZER,
        consumer_name="analyzer_batch",
        handler=handle_analyze_message,
        stream_name=STREAM_NEWS_RAW,
        batch_size=1,
        block_ms=2000,
        max_iterations=max_messages
    )
    
    logger.info(f"âœ… [Analyzer] ì™„ë£Œ - {processed}ê°œ ì²˜ë¦¬")


if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--daemon", action="store_true", help="Run as daemon (infinite loop)")
    parser.add_argument("--name", type=str, default="analyzer_1", help="Consumer name")
    parser.add_argument("--max", type=int, default=100, help="Max messages for one-shot mode")
    args = parser.parse_args()
    
    if args.daemon:
        run_analyzer_daemon(args.name)
    else:
        run_analyzer_once(args.max)
