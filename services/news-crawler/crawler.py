#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# crawler_job.py
# Version: v1.0
# ì‘ì—… LLM: Claude Opus 4.5
# Crawler Job - Cloud Scheduler(HTTP)ì— ì˜í•´ 10ë¶„ë§ˆë‹¤ ì‹¤í–‰ë˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
# KOSPI 200 ì „ì²´ ë‰´ìŠ¤ ìˆ˜ì§‘ (WatchList ì˜ì¡´ì„± ì œê±°)
# ê²½ìŸì‚¬ ìˆ˜í˜œ ë¶„ì„ ì—°ë™

from concurrent.futures import ThreadPoolExecutor, as_completed
import time
# import chromadb  # Lazy importë¡œ ë³€ê²½ (ì´ˆê¸°í™” ì‹œê°„ ë‹¨ì¶•)
import sys
import json
import urllib.parse
import feedparser # type: ignore
import logging
import os 
import calendar
import warnings
from dotenv import load_dotenv 
from datetime import datetime, timedelta, timezone

# FinanceDataReader for KOSPI 200 Universe
try:
    import FinanceDataReader as fdr
    FDR_AVAILABLE = True
except ImportError:
    FDR_AVAILABLE = False

# 'youngs75_jennie' íŒ¨í‚¤ì§€ë¥¼ ì°¾ê¸° ìœ„í•´ í”„ë¡œì íŠ¸ ë£¨íŠ¸ í´ë”ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
# Dockerfileì—ì„œ /app/crawler_job.pyë¡œ ë³µì‚¬ë˜ë¯€ë¡œ, /appì´ í”„ë¡œì íŠ¸ ë£¨íŠ¸
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
sys.path.append(PROJECT_ROOT)

# ==============================================================================
# ë¡œê±°(Logger) ì„¤ì •
# ==============================================================================
logger = logging.getLogger(__name__)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - [%(funcName)s] - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

try:
    import shared.auth as auth
    import shared.database as database
    from shared.llm import JennieBrain # ê°ì„± ë¶„ì„ì„ ìœ„í•œ JennieBrain ì„í¬íŠ¸
    from shared.db.connection import session_scope, ensure_engine_initialized
    from shared.db.models import WatchList as WatchListModel
    from shared.gemini import ensure_gemini_api_key
    # ê²½ìŸì‚¬ ìˆ˜í˜œ ë¶„ì„ ëª¨ë“ˆ
    # from shared.news_classifier import NewsClassifier, get_classifier
    # from shared.hybrid_scoring.competitor_analyzer import CompetitorAnalyzer
    logger.info("âœ… 'shared' íŒ¨í‚¤ì§€ ëª¨ë“ˆ import ì„±ê³µ")
except ImportError as e: # type: ignore
    logger.error(f"ğŸš¨ 'shared' ê³µìš© íŒ¨í‚¤ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤! (ì˜¤ë¥˜: {e})")
    auth = None
    database = None
    JennieBrain = None
    ensure_gemini_api_key = None
    NewsClassifier = None
    get_classifier = None
    CompetitorAnalyzer = None
except Exception as e:
    logger.error(f"ğŸš¨ 'shared' íŒ¨í‚¤ì§€ import ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
    auth = None
    database = None
    JennieBrain = None
    ensure_gemini_api_key = None
    NewsClassifier = None
    get_classifier = None
    CompetitorAnalyzer = None

from langchain_core.documents import Document
from langchain_chroma import Chroma
# [Cost Optimization] Cloud Embedding -> Local Embedding
# langchain_google_genai -> langchain_huggingface
try:
    from langchain_huggingface import HuggingFaceEmbeddings
    LOCAL_EMBEDDINGS_AVAILABLE = True
except ImportError:
    LOCAL_EMBEDDINGS_AVAILABLE = False
    logger.warning("âš ï¸ langchain_huggingface not available, falling back to Cloud Embeddings")
    from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

# ==============================================================================
# 1. ì „ì—­ ì„¤ì • (Constants)
# ==============================================================================

# Chroma ì„œë²„
CHROMA_SERVER_HOST = os.getenv("CHROMA_SERVER_HOST", "10.178.0.2") 
CHROMA_SERVER_PORT = 8000
COLLECTION_NAME = "rag_stock_data"

# RAG ì„¤ì •
DATA_TTL_DAYS = 7
VERTEX_AI_BATCH_SIZE = 10
MAX_SENTIMENT_DOCS_PER_RUN = int(os.getenv("MAX_SENTIMENT_DOCS_PER_RUN", "40"))
SENTIMENT_COOLDOWN_SECONDS = float(os.getenv("SENTIMENT_COOLDOWN_SECONDS", "0.2"))

# --- ğŸ”½ 'ì¼ë°˜ ê²½ì œ' RSS í”¼ë“œ ğŸ”½ ---
GENERAL_RSS_FEEDS = [
    {"source_name": "Maeil Business (Economy)", "url": "https://www.mk.co.kr/rss/50000001/"},
    {"source_name": "Maeil Business (Stock)", "url": "https://www.mk.co.kr/rss/50100001/"},
    {"source_name": "Investing.com (News)", "url": "https://kr.investing.com/rss/news.rss"}
]

# ==============================================================================
# LangChain, Chroma í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
# ==============================================================================

# ==============================================================================
# ì „ì—­ ë³€ìˆ˜ (ì§€ì—° ì´ˆê¸°í™”)
# ==============================================================================

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (ëª¨ë“ˆ ì„í¬íŠ¸ ì‹œ)
load_dotenv()

GCP_PROJECT_ID = os.getenv("GCP_PROJECT_ID")
# Oracle/OCI ê´€ë ¨ ì„¤ì •ì€ ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. (MariaDB + SQLAlchemy ë‹¨ì¼í™”)

# ì§€ì—° ì´ˆê¸°í™”ë¥¼ ìœ„í•œ ì „ì—­ ë³€ìˆ˜ (Noneìœ¼ë¡œ ì‹œì‘)
embeddings = None
text_splitter = None
db_client = None
vectorstore = None
jennie_brain = None # JennieBrain ì¸ìŠ¤í„´ìŠ¤

def initialize_services():
    """
    LangChain ë° ChromaDB ì„œë¹„ìŠ¤ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
    run_collection_job() ì‹¤í–‰ ì‹œì—ë§Œ í˜¸ì¶œë©ë‹ˆë‹¤.
    """
    global embeddings, text_splitter, db_client, vectorstore, jennie_brain
    
    # SQLAlchemy ì—”ì§„ ì´ˆê¸°í™” (session_scope ì‚¬ìš© ì „ì— í•„ìˆ˜)
    try:
        ensure_engine_initialized()
        logger.info("âœ… SQLAlchemy ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ")
    except Exception as e:
        logger.warning(f"âš ï¸ SQLAlchemy ì—”ì§„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    logger.info("... [RAG Crawler v10.0] LangChain ë° AI ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì‹œì‘ ...")
    try:
        # [Cost Optimization] Local Embeddings ì‚¬ìš© (Cloud API ë¹„ìš© â‚©0)
        if LOCAL_EMBEDDINGS_AVAILABLE:
            logger.info("="*60)
            logger.info("ğŸ  [LOCAL] Embedding ëª¨ë¸ ë¡œë”© ì¤‘ (jhgan/ko-sroberta-multitask)")
            logger.info("ğŸ  [LOCAL] Cloud API í˜¸ì¶œ ì—†ìŒ - ë¹„ìš©: â‚©0")
            logger.info("="*60)
            embeddings = HuggingFaceEmbeddings(
                model_name="jhgan/ko-sroberta-multitask",  # í•œêµ­ì–´ ìµœì í™” ëª¨ë¸
                model_kwargs={"device": "cpu"},
                encode_kwargs={"normalize_embeddings": True}
            )
            logger.info("âœ… [LOCAL] Embedding ëª¨ë¸ ë¡œë”© ì™„ë£Œ! (ë¹„ìš©: â‚©0)")
        else:
            # Fallback: Cloud Embeddings (ë¹„ìš© ë°œìƒ)
            logger.error("="*60)
            logger.error("ğŸš¨ [CLOUD] Cloud Embedding ì‚¬ìš© ì¤‘! - ë¹„ìš© ë°œìƒ!")
            logger.error("ğŸš¨ [CLOUD] langchain-huggingface ì„¤ì¹˜ í•„ìš”!")
            logger.error("="*60)
            api_key = ensure_gemini_api_key()
            embeddings = GoogleGenerativeAIEmbeddings(
                model="models/text-embedding-004",
                google_api_key=api_key,
            )
        
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
        logger.info("âœ… LangChain ì»´í¬ë„ŒíŠ¸(Embedding, Splitter) ì´ˆê¸°í™” ì„±ê³µ.")
        
        # JennieBrain ì´ˆê¸°í™” (ê°ì„± ë¶„ì„ìš©)
        try:
            jennie_brain = JennieBrain(
                project_id=GCP_PROJECT_ID,
                gemini_api_key_secret=os.getenv("SECRET_ID_GEMINI_API_KEY")
            )
            logger.info("âœ… JennieBrain (ê°ì„± ë¶„ì„ê¸°) ì´ˆê¸°í™” ì„±ê³µ.")
        except Exception as e:
            logger.warning(f"âš ï¸ JennieBrain ì´ˆê¸°í™” ì‹¤íŒ¨ (ê°ì„± ë¶„ì„ Skip): {e}")
            jennie_brain = None



    except Exception as e:
        logger.exception("ğŸ”¥ LangChain ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨!")
        raise
    
    logger.info(f"... [RAG Crawler v8.1] Chroma ì„œë²„ ({CHROMA_SERVER_HOST}:{CHROMA_SERVER_PORT}) ì—°ê²° ì‹œë„ ...")
    try:
        # Lazy import: chromadbëŠ” ì‹¤ì œ ì‚¬ìš© ì‹œì ì—ë§Œ import
        import chromadb
        
        db_client = chromadb.HttpClient(host=CHROMA_SERVER_HOST, port=CHROMA_SERVER_PORT)
        vectorstore = Chroma(client=db_client, collection_name=COLLECTION_NAME, embedding_function=embeddings)
        db_client.heartbeat() 
        logger.info(f"âœ… Chroma ì„œë²„ ({CHROMA_SERVER_HOST}) ì—°ê²° ì„±ê³µ!")
    except Exception as e:
        logger.exception(f"ğŸ”¥ Chroma ì„œë²„ ({CHROMA_SERVER_HOST}) ì—°ê²° ì‹¤íŒ¨!")
        raise

# ==============================================================================
# í•µì‹¬ í•¨ìˆ˜ ì •ì˜
# ==============================================================================

def get_kospi_200_universe():
    """
    KOSPI ì‹œê°€ì´ì•¡ ìƒìœ„ 200ê°œ ì¢…ëª©ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
    Scoutì™€ ë™ì¼í•œ Universeë¥¼ ì‚¬ìš©í•˜ì—¬ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    """
    universe_size = int(os.getenv("SCOUT_UNIVERSE_SIZE", "200"))
    logger.info(f"  (1/6) KOSPI ì‹œì´ ìƒìœ„ {universe_size}ê°œ ì¢…ëª© ë¡œë“œ ì¤‘...")
    
    # 1. FinanceDataReader ì‹œë„
    if FDR_AVAILABLE:
        try:
            logger.info("  (1/6) FinanceDataReaderë¡œ KOSPI ì¢…ëª© ì¡°íšŒ ì¤‘...")
            df = fdr.StockListing('KOSPI')
            
            if df is not None and not df.empty:
                # ì‹œê°€ì´ì•¡ ê¸°ì¤€ ì •ë ¬ (Marcap ë˜ëŠ” Market Cap ì»¬ëŸ¼)
                cap_col = None
                for col in ['Marcap', 'MarCap', 'Market Cap', 'marcap']:
                    if col in df.columns:
                        cap_col = col
                        break
                
                if cap_col:
                    df = df.sort_values(by=cap_col, ascending=False)
                
                # ìƒìœ„ Nê°œ ì¶”ì¶œ
                top_stocks = df.head(universe_size)
                
                # Code, Name ì»¬ëŸ¼ ì°¾ê¸°
                code_col = 'Code' if 'Code' in top_stocks.columns else 'Symbol'
                name_col = 'Name' if 'Name' in top_stocks.columns else 'name'
                
                universe = []
                for _, row in top_stocks.iterrows():
                    code = str(row.get(code_col, '')).zfill(6)
                    name = row.get(name_col, f'ì¢…ëª©_{code}')
                    if code and len(code) == 6:
                        universe.append({"code": code, "name": name})
                
                if universe:
                    logger.info(f"âœ… (1/6) FinanceDataReaderë¡œ {len(universe)}ê°œ ì¢…ëª© ë¡œë“œ ì™„ë£Œ!")
                    return universe
        except Exception as e:
            logger.warning(f"âš ï¸ (1/6) FinanceDataReader ì‹¤íŒ¨: {e}")
    
    # 2. Fallback: DBì˜ WatchList ì‚¬ìš©
    logger.info("  (1/6) Fallback: DB WatchList ì¡°íšŒ ì¤‘...")
    return get_watchlist_from_db()


def get_watchlist_from_db():
    """
    DBì—ì„œ WatchListë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤ (Fallbackìš©).
    MariaDB ì‚¬ìš© (pymysql ì§ì ‘ ì—°ê²°).
    """
    try:
        with session_scope(readonly=True) as session:
            rows = session.query(WatchListModel.stock_code, WatchListModel.stock_name).all()
        
        watchlist = []
        for row in rows:
            watchlist.append({"code": row[0], "name": row[1]})
 
        logger.info(f"âœ… (1/6) 'WatchList' {len(watchlist)}ê°œ ë¡œë“œ ì„±ê³µ.")
        return watchlist
        
    except Exception as e:
        logger.error(f"ğŸ”¥ (1/6) DB 'get_watchlist_from_db' í•¨ìˆ˜ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []

def get_numeric_timestamp(feed_entry):
    """
    feed_entryì—ì„œ 'ë°œí–‰ ì‹œê°„'ì„ UTC ê¸°ì¤€ ìˆ«ì íƒ€ì„ìŠ¤íƒ¬í”„ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    """
    if hasattr(feed_entry, 'published_parsed') and feed_entry.published_parsed:
        try:
            # feedparserê°€ ë°˜í™˜í•˜ëŠ” time.struct_timeì€ timezone ì •ë³´ê°€ ì—†ì„ ìˆ˜ ìˆìŒ
            # calendar.timegmì€ ì´ë¥¼ UTCë¡œ ê°„ì£¼í•˜ì—¬ timestampë¥¼ ìƒì„±
            # ì´ê²ƒì´ UTC ê¸°ì¤€ ì‹œê°„ì„ ë³´ì¥í•˜ëŠ” ê°€ì¥ ì•ˆì „í•œ ë°©ë²•
            utc_timestamp = calendar.timegm(feed_entry.published_parsed)
            return int(utc_timestamp)
        except Exception:
            return int(datetime.now(timezone.utc).timestamp())
    else:
        return int(datetime.now(timezone.utc).timestamp())

def crawl_news_for_stock(stock_code, stock_name):
    """
    Google News RSSë¥¼ ì‚¬ìš©í•˜ì—¬ íŠ¹ì • ì¢…ëª©ì˜ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    """
    logger.info(f"  (2/6) [App 5] '{stock_name}({stock_code})' Google News RSS í”¼ë“œ ìˆ˜ì§‘ ì¤‘...")
    documents = []
    try:
        query = f'"{stock_name}" OR "{stock_code}"'
        encoded_query = urllib.parse.quote(query)
        rss_url = f"https://news.google.com/rss/search?q={encoded_query}&hl=ko&gl=KR&ceid=KR:ko"
        feed = feedparser.parse(rss_url)
        
        if not feed.entries:
            logger.info(f"  (2/6) '{stock_name}' ê´€ë ¨ ì‹ ê·œ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. (Skip)")
            return []

        for entry in feed.entries:
            # 7ì¼ì´ ì§€ë‚œ ë‰´ìŠ¤ëŠ” ìˆ˜ì§‘ ë‹¨ê³„ì—ì„œ ì œì™¸
            published_timestamp = get_numeric_timestamp(entry)
            if datetime.fromtimestamp(published_timestamp, tz=timezone.utc) < datetime.now(timezone.utc) - timedelta(days=7):
                logger.debug(f"  (2/6) ì˜¤ë˜ëœ ë‰´ìŠ¤ ì œì™¸: {entry.title[:30]}...")
                continue

            doc = Document(
                page_content=f"ë‰´ìŠ¤ ì œëª©: {entry.title}\në§í¬: {entry.link}",
                metadata={
                    "stock_code": stock_code,
                    "stock_name": stock_name,
                    "source": f"Google News RSS ({entry.get('source', {}).get('title', 'N/A')})",
                    "source_url": entry.link, 
                    "created_at_utc": published_timestamp
                }
            )
            documents.append(doc)
    except Exception as e:
        logger.exception(f"ğŸ”¥ (2/6) '{stock_name}' ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
    return documents

def crawl_general_news():
    """
    ë¯¸ë¦¬ ì •ì˜ëœ 'GENERAL_RSS_FEEDS' ëª©ë¡ì˜ ì¼ë°˜ ê²½ì œ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    """
    logger.info(f"  (3/6) [App 5] 'ì¼ë°˜ ê²½ì œ' RSS {len(GENERAL_RSS_FEEDS)}ê°œ í”¼ë“œ ìˆ˜ì§‘ ì¤‘...")
    documents = []
    
    for feed_info in GENERAL_RSS_FEEDS:
        source = feed_info["source_name"]
        url = feed_info["url"]
        logger.info(f"  (3/6) ... '{source}' ìˆ˜ì§‘ ì¤‘ ...")
        try:
            feed = feedparser.parse(url)
            if not feed.entries:
                logger.info(f"  (3/6) '{source}'ì— ì‹ ê·œ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. (Skip)")
                continue

            for entry in feed.entries:
                # 7ì¼ì´ ì§€ë‚œ ë‰´ìŠ¤ëŠ” ìˆ˜ì§‘ ë‹¨ê³„ì—ì„œ ì œì™¸
                published_timestamp = get_numeric_timestamp(entry)
                if datetime.fromtimestamp(published_timestamp, tz=timezone.utc) < datetime.now(timezone.utc) - timedelta(days=7):
                    logger.debug(f"  (3/6) ì˜¤ë˜ëœ ë‰´ìŠ¤ ì œì™¸: {entry.title[:30]}...")
                    continue

                doc = Document(
                    page_content=f"ë‰´ìŠ¤ ì œëª©: {entry.title}\në§í¬: {entry.link}",
                    metadata={
                        "source": source,
                        "source_url": entry.link, 
                        "created_at_utc": published_timestamp
                    }
                )
                documents.append(doc)
        except Exception as e:
            logger.exception(f"ğŸ”¥ (3/6) '{source}' ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
            
    logger.info(f"âœ… (3/6) 'ì¼ë°˜ ê²½ì œ' ë‰´ìŠ¤ ì´ {len(documents)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ.")
    return documents

def filter_new_documents(documents):
    """
    ChromaDBì— 'source_url'ì´ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ì—¬ ìƒˆë¡œìš´ ë¬¸ì„œë§Œ í•„í„°ë§í•©ë‹ˆë‹¤.
    """
    step_id = "(4/6)"
    logger.info(f"  {step_id} [App 5] ìˆ˜ì§‘ëœ ë¬¸ì„œ {len(documents)}ê°œ ì¼ê´„ ì¤‘ë³µ ê²€ì‚¬ ì‹œì‘...")
    if not documents:
        return []

    urls_to_check = list(set([doc.metadata["source_url"] for doc in documents if "source_url" in doc.metadata]))
    if not urls_to_check:
        return documents

    existing_results = vectorstore.get(where={"source_url": {"$in": urls_to_check}})
    existing_urls = set(item['source_url'] for item in existing_results.get('metadatas', []))
    new_docs = [doc for doc in documents if doc.metadata.get("source_url") not in existing_urls]

    logger.info(f"âœ… {step_id} ì¤‘ë³µ ê²€ì‚¬ ì™„ë£Œ. ìƒˆë¡œìš´ ë¬¸ì„œ {len(new_docs)}ê°œ ë°œê²¬.")
    return new_docs

def process_unified_analysis(documents):
    """
    [2026-01 Optimized] í†µí•© ë‰´ìŠ¤ ë¶„ì„ (ê°ì„± + ê²½ìŸì‚¬ ë¦¬ìŠ¤í¬)
    Single-Pass LLM Callë¡œ ë‘ ê°€ì§€ ë¶„ì„ì„ ë™ì‹œì— ìˆ˜í–‰í•©ë‹ˆë‹¤.
    - ê°ì„± ë¶„ì„: Redis & MariaDB ì €ì¥
    - ë¦¬ìŠ¤í¬ íƒì§€: ê²½ìŸì‚¬ ìˆ˜í˜œ ì´ë²¤íŠ¸ ìƒì„±
    """
    if not jennie_brain or not documents:
        return

    logger.info("="*60)
    logger.info("ğŸš€ [Unified] í†µí•© ë‰´ìŠ¤ ë¶„ì„ ì‹œì‘ - Ollama (gpt-oss:20b)")
    logger.info("ğŸš€ [Unified] Single-Pass LLM Call (Sentiment + Risk) - ë¹„ìš©/ì‹œê°„ ìµœì í™”")
    logger.info("="*60)
    
    # stock_codeê°€ ìˆëŠ” ë¬¸ì„œë§Œ ë¶„ì„ ëŒ€ìƒ
    stock_docs = [doc for doc in documents if doc.metadata.get("stock_code")]
    logger.info(f"  [Unified] ëŒ€ìƒ ì¢…ëª© ë‰´ìŠ¤ {len(stock_docs)}ê°œ / ì „ì²´ {len(documents)}ê°œ")
    
    if not stock_docs:
        return

    # ë°°ì¹˜ ì¤€ë¹„
    batch_items = []
    doc_map = {}
    
    for idx, doc in enumerate(stock_docs):
        content_lines = doc.page_content.split('\n')
        news_title = content_lines[0].replace("ë‰´ìŠ¤ ì œëª©: ", "") if len(content_lines) > 0 else "ì œëª© ì—†ìŒ"
        
        batch_items.append({
            "id": idx,
            "title": news_title,
            "summary": news_title 
        })
        doc_map[idx] = doc
    
    # ë°°ì¹˜ ë¶„ì„ ì‹¤í–‰ (BATCH_SIZE=5)
    BATCH_SIZE = 5
    all_results = []
    
    for i in range(0, len(batch_items), BATCH_SIZE):
        batch = batch_items[i:i + BATCH_SIZE]
        logger.info(f"  [Unified] ë°°ì¹˜ {i//BATCH_SIZE + 1}/{(len(batch_items) + BATCH_SIZE - 1)//BATCH_SIZE} ë¶„ì„ ì¤‘...")
        
        try:
            results = jennie_brain.analyze_news_unified(batch)
            all_results.extend(results)
        except Exception as e:
            logger.warning(f"âš ï¸ [Unified] ë°°ì¹˜ ë¶„ì„ ì˜¤ë¥˜: {e}")
            # Fallback
            for item in batch:
                all_results.append({
                    'id': item['id'], 
                    'sentiment': {'score': 50, 'reason': 'ë¶„ì„ ì‹¤íŒ¨'},
                    'competitor_risk': {'is_detected': False, 'type': 'NONE', 'benefit_score': 0, 'reason': 'ë¶„ì„ ì‹¤íŒ¨'}
                })

    # ê²°ê³¼ ì²˜ë¦¬ (ë³‘ë ¬ ì €ì¥)
    logger.info(f"  [Unified] {len(all_results)}ê±´ ê²°ê³¼ ì²˜ë¦¬ ì‹œì‘ (ë³‘ë ¬ ì €ì¥/ì´ë²¤íŠ¸ ìƒì„±)...")
    
    # DB ì‘ì—…ì´ í˜¼í•©ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
    _process_unified_results_parallel(all_results, doc_map)


def _process_unified_results_parallel(results, doc_map):
    """
    í†µí•© ë¶„ì„ ê²°ê³¼ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    1. ê°ì„± ë¶„ì„ ê²°ê³¼ ì €ì¥ (Redis/DB)
    2. ë¦¬ìŠ¤í¬ íƒì§€ ì‹œ ê²½ìŸì‚¬ ìˆ˜í˜œ ì´ë²¤íŠ¸ ìƒì„±
    """
    MAX_WORKERS = 5 # DB Pool ê³ ë ¤
    processed_count = 0
    risk_event_count = 0
    
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = []
        for res in results:
            futures.append(executor.submit(_handle_single_unified_result, res, doc_map))
            
        for future in as_completed(futures):
            try:
                success, is_risk = future.result()
                if success: processed_count += 1
                if is_risk: risk_event_count += 1
            except Exception as e:
                logger.error(f"âŒ [Unified] ê²°ê³¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                
    logger.info(f"âœ… [Unified] ì™„ë£Œ: ê°ì„±ë¶„ì„ {processed_count}ê±´ ì €ì¥, ë¦¬ìŠ¤í¬ ì´ë²¤íŠ¸ {risk_event_count}ê±´ ìƒì„±.")


def _handle_single_unified_result(result, doc_map):
    """ê°œë³„ í†µí•© ê²°ê³¼ ì²˜ë¦¬ í•¸ë“¤ëŸ¬"""
    idx = result.get('id')
    doc = doc_map.get(idx)
    if not doc: return False, False
    
    # 1. ê°ì„± ë¶„ì„ ì €ì¥
    sentiment = result.get('sentiment', {})
    score = sentiment.get('score', 50)
    reason = sentiment.get('reason', 'N/A')
    
    # (ê¸°ì¡´ _save_single_sentiment_result ë¡œì§ ì¸ë¼ì¸ or ì¬ì‚¬ìš©)
    # ì—¬ê¸°ì„œëŠ” ë¡œì§ì„ ë‹¨ìˆœí™”í•˜ì—¬ ì§ì ‘ í˜¸ì¶œ
    save_success = _save_sentiment_to_db(doc, score, reason)
    
    # 2. ê²½ìŸì‚¬ ë¦¬ìŠ¤í¬ ì´ë²¤íŠ¸ ì²˜ë¦¬
    risk = result.get('competitor_risk', {})
    is_risk = risk.get('is_detected', False)
    
    if is_risk:
        _create_competitor_event(doc, risk)
        
    return save_success, is_risk


def _save_sentiment_to_db(doc, score, reason):
    """ê°ì„± ì ìˆ˜ ì €ì¥ ë¡œì§ (Redis + MariaDB)"""
    stock_code = doc.metadata.get("stock_code")
    stock_name = doc.metadata.get("stock_name")
    content_lines = doc.page_content.split('\n')
    news_title = content_lines[0].replace("ë‰´ìŠ¤ ì œëª©: ", "") if len(content_lines) > 0 else "ì œëª© ì—†ìŒ"
    news_link = doc.metadata.get("source_url")
    published_at = doc.metadata.get("created_at_utc")
    
    news_date_str = None
    if published_at:
        try:
            news_date_str = datetime.fromtimestamp(published_at, tz=timezone.utc).strftime('%Y-%m-%d')
        except Exception: pass
        
    # Redis
    try:
        database.set_sentiment_score(stock_code, score, reason, source_url=news_link, stock_name=stock_name, news_title=news_title, news_date=news_date_str)
    except Exception: pass
    
    # DB
    import random
    for attempt in range(3):
        try:
            with session_scope() as session:
                database.save_news_sentiment(session, stock_code, news_title, score, reason, news_link, published_at)
            return True
        except Exception as e:
            if "Deadlock" in str(e) and attempt < 2:
                time.sleep(random.uniform(0.1, 0.5))
                continue
            return False
    return False


def _create_competitor_event(doc, risk_data):
    """ê²½ìŸì‚¬ ìˆ˜í˜œ ì´ë²¤íŠ¸ ìƒì„± ë¡œì§"""
    stock_code = doc.metadata.get("stock_code")
    content_lines = doc.page_content.split('\n')
    news_title = content_lines[0].replace("ë‰´ìŠ¤ ì œëª©: ", "") if len(content_lines) > 0 else "ì œëª© ì—†ìŒ"
    news_link = doc.metadata.get("source_url")
    
    event_type = risk_data.get('type', 'OTHER')
    benefit_score = risk_data.get('benefit_score', 0)
    
    logger.info(f"  ğŸ”´ [Risk Detected] {stock_code} - {event_type} (Benefit Score: {benefit_score})")

    from shared.db.models import IndustryCompetitors, CompetitorBenefitEvents
    
    try:
        with session_scope() as session:
            # 1. ì„¹í„° í™•ì¸
            affected_stock = session.query(IndustryCompetitors).filter(IndustryCompetitors.stock_code == stock_code).first()
            if not affected_stock: return
            
            # 2. ê²½ìŸì‚¬ ì¡°íšŒ
            competitors = session.query(IndustryCompetitors).filter(
                IndustryCompetitors.sector_code == affected_stock.sector_code,
                IndustryCompetitors.stock_code != stock_code,
                IndustryCompetitors.is_active == 1
            ).all()
            
            if not competitors: return
            
            # 3. ì´ë²¤íŠ¸ ìƒì„±
            duration_days = 30 if event_type in ['FIRE', 'RECALL', 'SECURITY', 'OWNER_RISK'] else 7
            expires_at = datetime.now(timezone.utc) + timedelta(days=duration_days)
            
            for comp in competitors:
                # ì¤‘ë³µ ì²´í¬
                existing = session.query(CompetitorBenefitEvents).filter(
                    CompetitorBenefitEvents.affected_stock_code == stock_code,
                    CompetitorBenefitEvents.beneficiary_stock_code == comp.stock_code,
                    CompetitorBenefitEvents.event_type == event_type,
                    CompetitorBenefitEvents.detected_at >= datetime.now(timezone.utc) - timedelta(hours=24)
                ).first()
                
                if existing: continue
                
                # ì´ë²¤íŠ¸ ë“±ë¡
                event = CompetitorBenefitEvents(
                    affected_stock_code=stock_code,
                    affected_stock_name=affected_stock.stock_name,
                    event_type=event_type,
                    event_title=news_title[:1000],
                    event_severity=-10, # ê¸°ë³¸ê°’
                    source_url=news_link,
                    beneficiary_stock_code=comp.stock_code,
                    beneficiary_stock_name=comp.stock_name,
                    benefit_score=benefit_score,
                    sector_code=affected_stock.sector_code,
                    sector_name=affected_stock.sector_name,
                    status='ACTIVE',
                    expires_at=expires_at
                )
                session.add(event)
                
                # Redis ì—…ë°ì´íŠ¸ (Optional) - try-catch
                try:
                    database.set_competitor_benefit_score(
                        comp.stock_code, benefit_score, 
                        f"ê²½ìŸì‚¬ {affected_stock.stock_name} {event_type} ë°œìƒ (Unified Analysis)",
                        stock_code, event_type, ttl=duration_days*86400
                    )
                except: pass
                
                logger.info(f"  âœ… [ìˆ˜í˜œ ë“±ë¡] {comp.stock_name} +{benefit_score}ì  (by {event_type})")

    except Exception as e:
        logger.error(f"âŒ [Event Creation] ì‹¤íŒ¨: {e}")


                    
                    # ë™ì¼ ì„¹í„° ê²½ìŸì‚¬ ì¡°íšŒ




def add_documents_to_chroma(documents):
    """
    ìƒˆë¡œìš´ Document ë¦¬ìŠ¤íŠ¸ë¥¼ ë¶„í• (Chunking) í›„ ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ ChromaDBì— ì €ì¥í•©ë‹ˆë‹¤.
    """
    step_id = "(5/6)"
    if not documents:
        logger.info(f"  {step_id} [App 5] Chromaì— ì €ì¥í•  ìƒˆë¡œìš´ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. (Skip Write)")
        return

    logger.info(f"  {step_id} [App 5] 'ìƒˆ' ë¬¸ì„œ {len(documents)}ê°œ í…ìŠ¤íŠ¸ ë¶„í•  ë° ì„ë² ë”© ì¤‘...")
    try:
        # Local Embedding ì‚¬ìš© - í•„í„°ë§ ì—†ì´ ëª¨ë“  ë¬¸ì„œ ì„ë² ë”© (ë¹„ìš© â‚©0)
        splitted_docs = text_splitter.split_documents(documents)
        
        for i in range(0, len(splitted_docs), VERTEX_AI_BATCH_SIZE): # type: ignore
            batch_docs = splitted_docs[i : i + VERTEX_AI_BATCH_SIZE]
            logger.info(f"  {step_id} [App 4] 'ìƒˆ' ì²­í¬ {i+1} ~ {i+len(batch_docs)}ë²ˆ (ì´ {len(batch_docs)}ê°œ) ì €ì¥ ì‹œë„...")
            vectorstore.add_documents(
                batch_docs
            )
        
        logger.info("="*60)
        logger.info("ğŸ  [LOCAL] ì„ë² ë”© ì™„ë£Œ - HuggingFace (ko-sroberta) ì‚¬ìš©")
        logger.info("ğŸ  [LOCAL] Cloud API í˜¸ì¶œ ì—†ìŒ - ë¹„ìš©: â‚©0")
        logger.info("="*60)
        logger.info(f"âœ… {step_id} Chroma ì„œë²„ì— 'ìƒˆ' ì²­í¬ ì´ {len(splitted_docs)}ê°œ ì €ì¥ ì™„ë£Œ!")
    except Exception as e:
        logger.exception(f"ğŸ”¥ {step_id} [App 4] Chroma ì„œë²„ì— 'Write' ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ")

def cleanup_old_data_job():
    """
    DATA_TTL_DAYS(7ì¼)ê°€ ì§€ë‚œ ì˜¤ë˜ëœ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ChromaDBì—ì„œ ì‚­ì œí•©ë‹ˆë‹¤.
    """
    logger.info(f"\n[ë°ì´í„° ì •ë¦¬] {DATA_TTL_DAYS}ì¼ ê²½ê³¼í•œ ì˜¤ë˜ëœ RAG ë°ì´í„° ì‚­ì œ ì‹œì‘...")
    try:
        ttl_limit_timestamp = int((datetime.now(timezone.utc) - timedelta(days=DATA_TTL_DAYS)).timestamp())
        collection = vectorstore._collection
        
        logger.info(f"... [ë°ì´í„° ì •ë¦¬] created_at_utc < {ttl_limit_timestamp} ë°ì´í„° ì‚­ì œ ì¤‘ ...")
        collection.delete(where={"created_at_utc": {"$lt": ttl_limit_timestamp}})
        
        logger.info("âœ… [ë°ì´í„° ì •ë¦¬] ì˜¤ë˜ëœ ë°ì´í„° ì‚­ì œ ì™„ë£Œ.")
    except Exception as e:
        logger.warning(f"âš ï¸ [ë°ì´í„° ì •ë¦¬] ë°ì´í„° ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

# ==============================================================================
# ë©”ì¸ ì‘ì—… ì‹¤í–‰ í•¨ìˆ˜
# ==============================================================================

def run_collection_job():
    """
    ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ì €ì¥ì„ ìœ„í•œ ë©”ì¸ íƒœìŠ¤í¬.
    ì´ í•¨ìˆ˜ê°€ ìŠ¤í¬ë¦½íŠ¸ì˜ 'ì§„ì…ì (Entrypoint)'ì´ ë©ë‹ˆë‹¤.
    KOSPI 200 ì „ì²´ ë‰´ìŠ¤ ìˆ˜ì§‘ (Scout Universeì™€ ë™ì¼)
    """
    logger.info(f"\n--- [RAG ìˆ˜ì§‘ ë´‡ v9.0] ì‘ì—… ì‹œì‘ ---")
    
    # [Operating Hours Check] â€” mock/testì—ì„œëŠ” ìŠ¤í‚µ ê°€ëŠ¥
    disable_market_open_check = os.getenv("DISABLE_MARKET_OPEN_CHECK", "false").lower() in {"1", "true", "yes", "on"}
    if not disable_market_open_check:
        from shared.utils import is_operating_hours
        if not is_operating_hours():
            logger.info("ğŸ•’ í˜„ì¬ ìš´ì˜ ì‹œê°„ì´ ì•„ë‹™ë‹ˆë‹¤. (ìš´ì˜ ì‹œê°„: í‰ì¼ 07:00 ~ 17:00) ì‘ì—…ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return
    
    # ì„œë¹„ìŠ¤ ì´ˆê¸°í™” (ì§€ì—° ì´ˆê¸°í™”)
    try:
        initialize_services()
    except Exception as e:
        logger.error(f"ğŸ”¥ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return
    
    try:
        all_fetched_documents = []

        # 1. 'ì¼ë°˜ ê²½ì œ' RSS ìˆ˜ì§‘
        general_news_docs = crawl_general_news()
        all_fetched_documents.extend(general_news_docs)

        # 2. KOSPI 200 Universe ë¡œë“œ (Scoutì™€ ë™ì¼)
        universe = get_kospi_200_universe()
        logger.info(f"  (2/6) KOSPI Universe {len(universe)}ê°œ ì¢…ëª© ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘...")

        # 3. ê° ì¢…ëª©ë³„ ë‰´ìŠ¤ í¬ë¡¤ë§ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰
        with ThreadPoolExecutor(max_workers=10) as executor:
            future_to_stock = {executor.submit(crawl_news_for_stock, stock["code"], stock["name"]): stock for stock in universe}
            for future in as_completed(future_to_stock):
                stock = future_to_stock[future]
                try:
                    fetched_docs = future.result()
                    all_fetched_documents.extend(fetched_docs)
                except Exception as exc:
                    logger.error(f"ğŸ”¥ '{stock['name']}' ë‰´ìŠ¤ ìˆ˜ì§‘ ìŠ¤ë ˆë“œì—ì„œ ì˜¤ë¥˜ ë°œìƒ: {exc}")

        # 4. 'ìƒˆë¡œìš´' ë¬¸ì„œë§Œ í•„í„°ë§ (Deduplication)
        new_documents_to_add = filter_new_documents(all_fetched_documents)
        
        # [New] 4-1. ìƒˆë¡œìš´ ë¬¸ì„œ ê°ì„± ë¶„ì„ ë° ì €ì¥
        if os.getenv("ENABLE_NEWS_ANALYSIS", "true").lower() == "true":
            # [2026-01 Optimized] Unified Analysis (Sentiment + Risk)
            process_unified_analysis(new_documents_to_add)
        else:
            logger.info("âš ï¸ [Config] 'ENABLE_NEWS_ANALYSIS=false' ì„¤ì •ìœ¼ë¡œ ì¸í•´ ë¶„ì„ ë‹¨ê³„ ìƒëµ.")
        
        # 5. 'ìƒˆë¡œìš´' ë¬¸ì„œë§Œ Chroma ì„œë²„ì— ì €ì¥ (Write)
        add_documents_to_chroma(new_documents_to_add)
        
        # 6. ì˜¤ë˜ëœ ë°ì´í„° ì •ë¦¬
        cleanup_old_data_job()
        
        logger.info(f"--- [RAG ìˆ˜ì§‘ ë´‡ v9.1] ì‘ì—… ì™„ë£Œ ---")
        
    except Exception as e:
        logger.exception(f"ğŸ”¥ [RAG ìˆ˜ì§‘ ë´‡ v9.0] ë©”ì¸ ì‘ì—… ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ")

# =============================================================================
# ë©”ì¸ ì‹¤í–‰ ë¸”ë¡
# =============================================================================

if __name__ == "__main__":
    
    start_time = time.time()

    # ë©”ì¸ ì‘ì—… ì‹¤í–‰
    try:
        run_collection_job()
    except Exception as e:
        logger.critical(f"âŒ [RAG Crawler v8.1] 'run_collection_job' ì‹¤í–‰ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜: {e}")
        
    end_time = time.time()
    logger.info(f"--- [RAG ìˆ˜ì§‘ ë´‡ v8.1] ìŠ¤í¬ë¦½íŠ¸ ì¢…ë£Œ (ì´ ì†Œìš”ì‹œê°„: {end_time - start_time:.2f}ì´ˆ) ---")
