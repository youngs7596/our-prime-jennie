#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# crawler_job.py
# Version: v1.0
# ì‘ì—… LLM: Claude Opus 4.5
# Crawler Job - Cloud Scheduler(HTTP)ì— ì˜í•´ 10ë¶„ë§ˆë‹¤ ì‹¤í–‰ë˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
# KOSPI 200 ì „ì²´ ë‰´ìŠ¤ ìˆ˜ì§‘ (WatchList ì˜ì¡´ì„± ì œê±°)
# ê²½ìŸì‚¬ ìˆ˜í˜œ ë¶„ì„ ì—°ë™

from concurrent.futures import ThreadPoolExecutor, as_completed
import time
# import chromadb  # Lazy importë¡œ ë³€ê²½ (ì´ˆê¸°í™” ì‹œê°„ ë‹¨ì¶•)
import sys
import json
import urllib.parse
import feedparser # type: ignore
import logging
import os 
import calendar
import warnings
from dotenv import load_dotenv 
from datetime import datetime, timedelta, timezone

# FinanceDataReader for KOSPI 200 Universe
try:
    import FinanceDataReader as fdr
    FDR_AVAILABLE = True
except ImportError:
    FDR_AVAILABLE = False

# 'youngs75_jennie' íŒ¨í‚¤ì§€ë¥¼ ì°¾ê¸° ìœ„í•´ í”„ë¡œì íŠ¸ ë£¨íŠ¸ í´ë”ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
# Dockerfileì—ì„œ /app/crawler_job.pyë¡œ ë³µì‚¬ë˜ë¯€ë¡œ, /appì´ í”„ë¡œì íŠ¸ ë£¨íŠ¸
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
sys.path.append(PROJECT_ROOT)

# ==============================================================================
# ë¡œê±°(Logger) ì„¤ì •
# ==============================================================================
logger = logging.getLogger(__name__)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - [%(funcName)s] - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

try:
    import shared.auth as auth
    import shared.database as database
    from shared.llm import JennieBrain # ê°ì„± ë¶„ì„ì„ ìœ„í•œ JennieBrain ì„í¬íŠ¸
    from shared.db.connection import session_scope, ensure_engine_initialized
    from shared.db.models import WatchList as WatchListModel
    from shared.gemini import ensure_gemini_api_key
    # ê²½ìŸì‚¬ ìˆ˜í˜œ ë¶„ì„ ëª¨ë“ˆ
    from shared.news_classifier import NewsClassifier, get_classifier
    from shared.hybrid_scoring.competitor_analyzer import CompetitorAnalyzer
    logger.info("âœ… 'shared' íŒ¨í‚¤ì§€ ëª¨ë“ˆ import ì„±ê³µ")
except ImportError as e: # type: ignore
    logger.error(f"ğŸš¨ 'shared' ê³µìš© íŒ¨í‚¤ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤! (ì˜¤ë¥˜: {e})")
    auth = None
    database = None
    JennieBrain = None
    ensure_gemini_api_key = None
    NewsClassifier = None
    get_classifier = None
    CompetitorAnalyzer = None
except Exception as e:
    logger.error(f"ğŸš¨ 'shared' íŒ¨í‚¤ì§€ import ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
    auth = None
    database = None
    JennieBrain = None
    ensure_gemini_api_key = None
    NewsClassifier = None
    get_classifier = None
    CompetitorAnalyzer = None

from langchain_core.documents import Document
from langchain_chroma import Chroma
# [Cost Optimization] Cloud Embedding -> Local Embedding
# langchain_google_genai -> langchain_huggingface
try:
    from langchain_huggingface import HuggingFaceEmbeddings
    LOCAL_EMBEDDINGS_AVAILABLE = True
except ImportError:
    LOCAL_EMBEDDINGS_AVAILABLE = False
    logger.warning("âš ï¸ langchain_huggingface not available, falling back to Cloud Embeddings")
    from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

# ==============================================================================
# 1. ì „ì—­ ì„¤ì • (Constants)
# ==============================================================================

# Chroma ì„œë²„
CHROMA_SERVER_HOST = os.getenv("CHROMA_SERVER_HOST", "10.178.0.2") 
CHROMA_SERVER_PORT = 8000
COLLECTION_NAME = "rag_stock_data"

# RAG ì„¤ì •
DATA_TTL_DAYS = 7
VERTEX_AI_BATCH_SIZE = 10
MAX_SENTIMENT_DOCS_PER_RUN = int(os.getenv("MAX_SENTIMENT_DOCS_PER_RUN", "40"))
SENTIMENT_COOLDOWN_SECONDS = float(os.getenv("SENTIMENT_COOLDOWN_SECONDS", "0.2"))

# --- ğŸ”½ 'ì¼ë°˜ ê²½ì œ' RSS í”¼ë“œ ğŸ”½ ---
GENERAL_RSS_FEEDS = [
    {"source_name": "Maeil Business (Economy)", "url": "https://www.mk.co.kr/rss/50000001/"},
    {"source_name": "Maeil Business (Stock)", "url": "https://www.mk.co.kr/rss/50100001/"},
    {"source_name": "Investing.com (News)", "url": "https://kr.investing.com/rss/news.rss"}
]

# ==============================================================================
# LangChain, Chroma í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
# ==============================================================================

# ==============================================================================
# ì „ì—­ ë³€ìˆ˜ (ì§€ì—° ì´ˆê¸°í™”)
# ==============================================================================

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (ëª¨ë“ˆ ì„í¬íŠ¸ ì‹œ)
load_dotenv()

GCP_PROJECT_ID = os.getenv("GCP_PROJECT_ID")
# Oracle/OCI ê´€ë ¨ ì„¤ì •ì€ ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. (MariaDB + SQLAlchemy ë‹¨ì¼í™”)

# ì§€ì—° ì´ˆê¸°í™”ë¥¼ ìœ„í•œ ì „ì—­ ë³€ìˆ˜ (Noneìœ¼ë¡œ ì‹œì‘)
embeddings = None
text_splitter = None
db_client = None
vectorstore = None
jennie_brain = None # JennieBrain ì¸ìŠ¤í„´ìŠ¤
classifier = None # NewsClassifier ì¸ìŠ¤í„´ìŠ¤ (Cost saving)

def initialize_services():
    """
    LangChain ë° ChromaDB ì„œë¹„ìŠ¤ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
    run_collection_job() ì‹¤í–‰ ì‹œì—ë§Œ í˜¸ì¶œë©ë‹ˆë‹¤.
    """
    global embeddings, text_splitter, db_client, vectorstore, jennie_brain, classifier
    
    # SQLAlchemy ì—”ì§„ ì´ˆê¸°í™” (session_scope ì‚¬ìš© ì „ì— í•„ìˆ˜)
    try:
        ensure_engine_initialized()
        logger.info("âœ… SQLAlchemy ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ")
    except Exception as e:
        logger.warning(f"âš ï¸ SQLAlchemy ì—”ì§„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    logger.info("... [RAG Crawler v10.0] LangChain ë° AI ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì‹œì‘ ...")
    try:
        # [Cost Optimization] Local Embeddings ì‚¬ìš© (Cloud API ë¹„ìš© â‚©0)
        if LOCAL_EMBEDDINGS_AVAILABLE:
            logger.info("="*60)
            logger.info("ğŸ  [LOCAL] Embedding ëª¨ë¸ ë¡œë”© ì¤‘ (jhgan/ko-sroberta-multitask)")
            logger.info("ğŸ  [LOCAL] Cloud API í˜¸ì¶œ ì—†ìŒ - ë¹„ìš©: â‚©0")
            logger.info("="*60)
            embeddings = HuggingFaceEmbeddings(
                model_name="jhgan/ko-sroberta-multitask",  # í•œêµ­ì–´ ìµœì í™” ëª¨ë¸
                model_kwargs={"device": "cpu"},
                encode_kwargs={"normalize_embeddings": True}
            )
            logger.info("âœ… [LOCAL] Embedding ëª¨ë¸ ë¡œë”© ì™„ë£Œ! (ë¹„ìš©: â‚©0)")
        else:
            # Fallback: Cloud Embeddings (ë¹„ìš© ë°œìƒ)
            logger.error("="*60)
            logger.error("ğŸš¨ [CLOUD] Cloud Embedding ì‚¬ìš© ì¤‘! - ë¹„ìš© ë°œìƒ!")
            logger.error("ğŸš¨ [CLOUD] langchain-huggingface ì„¤ì¹˜ í•„ìš”!")
            logger.error("="*60)
            api_key = ensure_gemini_api_key()
            embeddings = GoogleGenerativeAIEmbeddings(
                model="models/text-embedding-004",
                google_api_key=api_key,
            )
        
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
        logger.info("âœ… LangChain ì»´í¬ë„ŒíŠ¸(Embedding, Splitter) ì´ˆê¸°í™” ì„±ê³µ.")
        
        # JennieBrain ì´ˆê¸°í™” (ê°ì„± ë¶„ì„ìš©)
        try:
            jennie_brain = JennieBrain(
                project_id=GCP_PROJECT_ID,
                gemini_api_key_secret=os.getenv("SECRET_ID_GEMINI_API_KEY")
            )
            logger.info("âœ… JennieBrain (ê°ì„± ë¶„ì„ê¸°) ì´ˆê¸°í™” ì„±ê³µ.")
        except Exception as e:
            logger.warning(f"âš ï¸ JennieBrain ì´ˆê¸°í™” ì‹¤íŒ¨ (ê°ì„± ë¶„ì„ Skip): {e}")
            jennie_brain = None

        # [Cost Optimization] NewsClassifier ì´ˆê¸°í™”
        global classifier
        if get_classifier:
            classifier = get_classifier()
            logger.info("âœ… NewsClassifier ì´ˆê¸°í™” ì„±ê³µ (ë¹„ìš© ìµœì í™” í•„í„° ê°€ë™).")
        else:
            classifier = None


    except Exception as e:
        logger.exception("ğŸ”¥ LangChain ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨!")
        raise
    
    logger.info(f"... [RAG Crawler v8.1] Chroma ì„œë²„ ({CHROMA_SERVER_HOST}:{CHROMA_SERVER_PORT}) ì—°ê²° ì‹œë„ ...")
    try:
        # Lazy import: chromadbëŠ” ì‹¤ì œ ì‚¬ìš© ì‹œì ì—ë§Œ import
        import chromadb
        
        db_client = chromadb.HttpClient(host=CHROMA_SERVER_HOST, port=CHROMA_SERVER_PORT)
        vectorstore = Chroma(client=db_client, collection_name=COLLECTION_NAME, embedding_function=embeddings)
        db_client.heartbeat() 
        logger.info(f"âœ… Chroma ì„œë²„ ({CHROMA_SERVER_HOST}) ì—°ê²° ì„±ê³µ!")
    except Exception as e:
        logger.exception(f"ğŸ”¥ Chroma ì„œë²„ ({CHROMA_SERVER_HOST}) ì—°ê²° ì‹¤íŒ¨!")
        raise

# ==============================================================================
# í•µì‹¬ í•¨ìˆ˜ ì •ì˜
# ==============================================================================

def get_kospi_200_universe():
    """
    KOSPI ì‹œê°€ì´ì•¡ ìƒìœ„ 200ê°œ ì¢…ëª©ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
    Scoutì™€ ë™ì¼í•œ Universeë¥¼ ì‚¬ìš©í•˜ì—¬ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    """
    universe_size = int(os.getenv("SCOUT_UNIVERSE_SIZE", "200"))
    logger.info(f"  (1/6) KOSPI ì‹œì´ ìƒìœ„ {universe_size}ê°œ ì¢…ëª© ë¡œë“œ ì¤‘...")
    
    # 1. FinanceDataReader ì‹œë„
    if FDR_AVAILABLE:
        try:
            logger.info("  (1/6) FinanceDataReaderë¡œ KOSPI ì¢…ëª© ì¡°íšŒ ì¤‘...")
            df = fdr.StockListing('KOSPI')
            
            if df is not None and not df.empty:
                # ì‹œê°€ì´ì•¡ ê¸°ì¤€ ì •ë ¬ (Marcap ë˜ëŠ” Market Cap ì»¬ëŸ¼)
                cap_col = None
                for col in ['Marcap', 'MarCap', 'Market Cap', 'marcap']:
                    if col in df.columns:
                        cap_col = col
                        break
                
                if cap_col:
                    df = df.sort_values(by=cap_col, ascending=False)
                
                # ìƒìœ„ Nê°œ ì¶”ì¶œ
                top_stocks = df.head(universe_size)
                
                # Code, Name ì»¬ëŸ¼ ì°¾ê¸°
                code_col = 'Code' if 'Code' in top_stocks.columns else 'Symbol'
                name_col = 'Name' if 'Name' in top_stocks.columns else 'name'
                
                universe = []
                for _, row in top_stocks.iterrows():
                    code = str(row.get(code_col, '')).zfill(6)
                    name = row.get(name_col, f'ì¢…ëª©_{code}')
                    if code and len(code) == 6:
                        universe.append({"code": code, "name": name})
                
                if universe:
                    logger.info(f"âœ… (1/6) FinanceDataReaderë¡œ {len(universe)}ê°œ ì¢…ëª© ë¡œë“œ ì™„ë£Œ!")
                    return universe
        except Exception as e:
            logger.warning(f"âš ï¸ (1/6) FinanceDataReader ì‹¤íŒ¨: {e}")
    
    # 2. Fallback: DBì˜ WatchList ì‚¬ìš©
    logger.info("  (1/6) Fallback: DB WatchList ì¡°íšŒ ì¤‘...")
    return get_watchlist_from_db()


def get_watchlist_from_db():
    """
    DBì—ì„œ WatchListë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤ (Fallbackìš©).
    MariaDB ì‚¬ìš© (pymysql ì§ì ‘ ì—°ê²°).
    """
    try:
        with session_scope(readonly=True) as session:
            rows = session.query(WatchListModel.stock_code, WatchListModel.stock_name).all()
        
        watchlist = []
        for row in rows:
            watchlist.append({"code": row[0], "name": row[1]})
 
        logger.info(f"âœ… (1/6) 'WatchList' {len(watchlist)}ê°œ ë¡œë“œ ì„±ê³µ.")
        return watchlist
        
    except Exception as e:
        logger.error(f"ğŸ”¥ (1/6) DB 'get_watchlist_from_db' í•¨ìˆ˜ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []

def get_numeric_timestamp(feed_entry):
    """
    feed_entryì—ì„œ 'ë°œí–‰ ì‹œê°„'ì„ UTC ê¸°ì¤€ ìˆ«ì íƒ€ì„ìŠ¤íƒ¬í”„ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    """
    if hasattr(feed_entry, 'published_parsed') and feed_entry.published_parsed:
        try:
            # feedparserê°€ ë°˜í™˜í•˜ëŠ” time.struct_timeì€ timezone ì •ë³´ê°€ ì—†ì„ ìˆ˜ ìˆìŒ
            # calendar.timegmì€ ì´ë¥¼ UTCë¡œ ê°„ì£¼í•˜ì—¬ timestampë¥¼ ìƒì„±
            # ì´ê²ƒì´ UTC ê¸°ì¤€ ì‹œê°„ì„ ë³´ì¥í•˜ëŠ” ê°€ì¥ ì•ˆì „í•œ ë°©ë²•
            utc_timestamp = calendar.timegm(feed_entry.published_parsed)
            return int(utc_timestamp)
        except Exception:
            return int(datetime.now(timezone.utc).timestamp())
    else:
        return int(datetime.now(timezone.utc).timestamp())

def crawl_news_for_stock(stock_code, stock_name):
    """
    Google News RSSë¥¼ ì‚¬ìš©í•˜ì—¬ íŠ¹ì • ì¢…ëª©ì˜ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    """
    logger.info(f"  (2/6) [App 5] '{stock_name}({stock_code})' Google News RSS í”¼ë“œ ìˆ˜ì§‘ ì¤‘...")
    documents = []
    try:
        query = f'"{stock_name}" OR "{stock_code}"'
        encoded_query = urllib.parse.quote(query)
        rss_url = f"https://news.google.com/rss/search?q={encoded_query}&hl=ko&gl=KR&ceid=KR:ko"
        feed = feedparser.parse(rss_url)
        
        if not feed.entries:
            logger.info(f"  (2/6) '{stock_name}' ê´€ë ¨ ì‹ ê·œ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. (Skip)")
            return []

        for entry in feed.entries:
            # 7ì¼ì´ ì§€ë‚œ ë‰´ìŠ¤ëŠ” ìˆ˜ì§‘ ë‹¨ê³„ì—ì„œ ì œì™¸
            published_timestamp = get_numeric_timestamp(entry)
            if datetime.fromtimestamp(published_timestamp, tz=timezone.utc) < datetime.now(timezone.utc) - timedelta(days=7):
                logger.debug(f"  (2/6) ì˜¤ë˜ëœ ë‰´ìŠ¤ ì œì™¸: {entry.title[:30]}...")
                continue

            doc = Document(
                page_content=f"ë‰´ìŠ¤ ì œëª©: {entry.title}\në§í¬: {entry.link}",
                metadata={
                    "stock_code": stock_code,
                    "stock_name": stock_name,
                    "source": f"Google News RSS ({entry.get('source', {}).get('title', 'N/A')})",
                    "source_url": entry.link, 
                    "created_at_utc": published_timestamp
                }
            )
            documents.append(doc)
    except Exception as e:
        logger.exception(f"ğŸ”¥ (2/6) '{stock_name}' ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
    return documents

def crawl_general_news():
    """
    ë¯¸ë¦¬ ì •ì˜ëœ 'GENERAL_RSS_FEEDS' ëª©ë¡ì˜ ì¼ë°˜ ê²½ì œ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    """
    logger.info(f"  (3/6) [App 5] 'ì¼ë°˜ ê²½ì œ' RSS {len(GENERAL_RSS_FEEDS)}ê°œ í”¼ë“œ ìˆ˜ì§‘ ì¤‘...")
    documents = []
    
    for feed_info in GENERAL_RSS_FEEDS:
        source = feed_info["source_name"]
        url = feed_info["url"]
        logger.info(f"  (3/6) ... '{source}' ìˆ˜ì§‘ ì¤‘ ...")
        try:
            feed = feedparser.parse(url)
            if not feed.entries:
                logger.info(f"  (3/6) '{source}'ì— ì‹ ê·œ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. (Skip)")
                continue

            for entry in feed.entries:
                # 7ì¼ì´ ì§€ë‚œ ë‰´ìŠ¤ëŠ” ìˆ˜ì§‘ ë‹¨ê³„ì—ì„œ ì œì™¸
                published_timestamp = get_numeric_timestamp(entry)
                if datetime.fromtimestamp(published_timestamp, tz=timezone.utc) < datetime.now(timezone.utc) - timedelta(days=7):
                    logger.debug(f"  (3/6) ì˜¤ë˜ëœ ë‰´ìŠ¤ ì œì™¸: {entry.title[:30]}...")
                    continue

                doc = Document(
                    page_content=f"ë‰´ìŠ¤ ì œëª©: {entry.title}\në§í¬: {entry.link}",
                    metadata={
                        "source": source,
                        "source_url": entry.link, 
                        "created_at_utc": published_timestamp
                    }
                )
                documents.append(doc)
        except Exception as e:
            logger.exception(f"ğŸ”¥ (3/6) '{source}' ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
            
    logger.info(f"âœ… (3/6) 'ì¼ë°˜ ê²½ì œ' ë‰´ìŠ¤ ì´ {len(documents)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ.")
    return documents

def filter_new_documents(documents):
    """
    ChromaDBì— 'source_url'ì´ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ì—¬ ìƒˆë¡œìš´ ë¬¸ì„œë§Œ í•„í„°ë§í•©ë‹ˆë‹¤.
    """
    step_id = "(4/6)"
    logger.info(f"  {step_id} [App 5] ìˆ˜ì§‘ëœ ë¬¸ì„œ {len(documents)}ê°œ ì¼ê´„ ì¤‘ë³µ ê²€ì‚¬ ì‹œì‘...")
    if not documents:
        return []

    urls_to_check = list(set([doc.metadata["source_url"] for doc in documents if "source_url" in doc.metadata]))
    if not urls_to_check:
        return documents

    existing_results = vectorstore.get(where={"source_url": {"$in": urls_to_check}})
    existing_urls = set(item['source_url'] for item in existing_results.get('metadatas', []))
    new_docs = [doc for doc in documents if doc.metadata.get("source_url") not in existing_urls]

    logger.info(f"âœ… {step_id} ì¤‘ë³µ ê²€ì‚¬ ì™„ë£Œ. ìƒˆë¡œìš´ ë¬¸ì„œ {len(new_docs)}ê°œ ë°œê²¬.")
    return new_docs

def process_sentiment_analysis(documents):
    """
    [2026-01 Optimized] ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ì¤‘ ì¢…ëª© ë‰´ìŠ¤ì— ëŒ€í•´ ì‹¤ì‹œê°„ ê°ì„± ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    ë°°ì¹˜ ì²˜ë¦¬(Batch Processing) ë„ì…ìœ¼ë¡œ ~70% ì†ë„ í–¥ìƒ
    ë¶„ì„ ê²°ê³¼ëŠ” Redis ë° MariaDBì— ì €ì¥ë©ë‹ˆë‹¤.
    """
    if not jennie_brain or not documents:
        return

    logger.info("="*60)
    logger.info("ğŸ  [LOCAL] ê°ì„± ë¶„ì„ ì‹œì‘ - Ollama (gpt-oss:20b) ì‚¬ìš©")
    logger.info("ğŸ  [LOCAL] ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™” - ë¹„ìš©: â‚©0")
    logger.info("="*60)
    
    # stock_codeê°€ ìˆëŠ” ë¬¸ì„œë§Œ ë¶„ì„ ëŒ€ìƒ
    stock_docs = [doc for doc in documents if doc.metadata.get("stock_code")]
    logger.info(f"  [Sentiment] ì¢…ëª© ë‰´ìŠ¤ {len(stock_docs)}ê°œ / ì „ì²´ {len(documents)}ê°œ")
    
    if not stock_docs:
        return

    # ë°°ì¹˜ ì¤€ë¹„: ë¬¸ì„œë¥¼ (id, title, summary, metadata) í˜•íƒœë¡œ ë³€í™˜
    batch_items = []
    doc_map = {}  # id -> doc ë§¤í•‘ (ë‚˜ì¤‘ì— ì €ì¥ìš©)
    
    for idx, doc in enumerate(stock_docs):
        content_lines = doc.page_content.split('\n')
        news_title = content_lines[0].replace("ë‰´ìŠ¤ ì œëª©: ", "") if len(content_lines) > 0 else "ì œëª© ì—†ìŒ"
        
        batch_items.append({
            "id": idx,
            "title": news_title,
            "summary": news_title  # ì œëª©ë§Œ ì‚¬ìš© (í˜„ì¬ ë¡œì§ê³¼ ë™ì¼)
        })
        doc_map[idx] = doc
    
    # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë¶„ì„ (BATCH_SIZE=5)
    BATCH_SIZE = 5
    all_results = []
    
    for i in range(0, len(batch_items), BATCH_SIZE):
        batch = batch_items[i:i + BATCH_SIZE]
        logger.info(f"  [Sentiment] ë°°ì¹˜ {i//BATCH_SIZE + 1}/{(len(batch_items) + BATCH_SIZE - 1)//BATCH_SIZE} ë¶„ì„ ì¤‘...")
        
        try:
            results = jennie_brain.analyze_news_sentiment_batch(batch)
            all_results.extend(results)
        except Exception as e:
            logger.warning(f"âš ï¸ [Sentiment] ë°°ì¹˜ ë¶„ì„ ì˜¤ë¥˜: {e}")
            # Fallback: ê¸°ë³¸ê°’ ì¶”ê°€
            for item in batch:
                all_results.append({'id': item['id'], 'score': 50, 'reason': 'ë°°ì¹˜ ë¶„ì„ ì‹¤íŒ¨'})
    
    # ê²°ê³¼ ì €ì¥
    processed_count = 0
    for result in all_results:
        idx = result.get('id')
        if idx is None or idx not in doc_map:
            continue
            
        doc = doc_map[idx]
        score = result.get('score', 50)
        reason = result.get('reason', 'ë¶„ì„ ë¶ˆê°€')
        
        stock_code = doc.metadata.get("stock_code")
        stock_name = doc.metadata.get("stock_name")
        content_lines = doc.page_content.split('\n')
        news_title = content_lines[0].replace("ë‰´ìŠ¤ ì œëª©: ", "") if len(content_lines) > 0 else "ì œëª© ì—†ìŒ"
        news_link = doc.metadata.get("source_url")
        published_at = doc.metadata.get("created_at_utc")
        
        # ë‰´ìŠ¤ ë‚ ì§œ ì¶”ì¶œ
        news_date_str = None
        if published_at:
            try:
                news_date_str = datetime.fromtimestamp(published_at, tz=timezone.utc).strftime('%Y-%m-%d')
            except Exception:
                pass
        
        # Redis ì €ì¥
        try:
            database.set_sentiment_score(
                stock_code, score, reason, 
                source_url=news_link, 
                stock_name=stock_name,
                news_title=news_title,
                news_date=news_date_str
            )
        except Exception as e:
            logger.warning(f"âš ï¸ [Sentiment] Redis ì €ì¥ ì‹¤íŒ¨ (Skip): {e}")
        
        # DB ì €ì¥ (Deadlock ì¬ì‹œë„)
        import random
        max_retries = 3
        for attempt in range(max_retries):
            try:
                with session_scope() as session:
                    database.save_news_sentiment(session, stock_code, news_title, score, reason, news_link, published_at)
                processed_count += 1
                break
            except Exception as e:
                error_str = str(e)
                is_deadlock = "1213" in error_str or "Deadlock" in error_str
                
                if is_deadlock and attempt < max_retries - 1:
                    wait_time = random.uniform(0.1, 0.5) * (attempt + 1)
                    logger.info(f"ğŸ”„ [Sentiment] Deadlock ê°ì§€, {wait_time:.2f}ì´ˆ í›„ ì¬ì‹œë„...")
                    time.sleep(wait_time)
                    continue
                else:
                    logger.warning(f"âš ï¸ [Sentiment] DB ì €ì¥ ì‹¤íŒ¨ (Skip): {e}")
                    break

    logger.info(f"âœ… [Sentiment] ì¢…ëª© ë‰´ìŠ¤ {processed_count}ê±´ ê°ì„± ë¶„ì„ ë° ì €ì¥ ì™„ë£Œ.")




def process_competitor_benefit_analysis(documents):
    """
    ë‰´ìŠ¤ì—ì„œ ê²½ìŸì‚¬ ìˆ˜í˜œ ê¸°íšŒë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
    LLM-First Analysis (JennieBrain Reasoning Tier)
    ThreadPoolExecutorë¥¼ ì‚¬ìš©í•œ ë³‘ë ¬ ì²˜ë¦¬ ë„ì… (Cloud LLM ì†ë„ í™œìš©)
    """
    if not jennie_brain or not CompetitorAnalyzer or not documents:
        return
    
    logger.info(f"  [ê²½ìŸì‚¬ ìˆ˜í˜œ] ì‹ ê·œ ë¬¸ì„œ {len(documents)}ê°œ ê²½ìŸì‚¬ ìˆ˜í˜œ ë¶„ì„ ì‹œì‘ (ë³‘ë ¬ ì²˜ë¦¬)...")
    
    from shared.db.connection import get_session, session_scope # ensure import
    from shared.db.models import IndustryCompetitors, CompetitorBenefitEvents
    
    MAX_WORKERS = 3
    
    def _analyze_single_competitor_benefit(doc):
        # ë¬¸ì„œ ì •ë³´ ì¶”ì¶œ
        stock_code = doc.metadata.get("stock_code")
        if not stock_code:
            return 0
        
        content_lines = doc.page_content.split('\n')
        news_title = content_lines[0].replace("ë‰´ìŠ¤ ì œëª©: ", "") if len(content_lines) > 0 else "ì œëª© ì—†ìŒ"
        news_link = doc.metadata.get("source_url")
        
        events_created = 0

        # 1. LLM ì‹¬ì¸µ ë¶„ì„
        try:
            analysis_result = jennie_brain.analyze_competitor_benefit(news_title)
        except Exception as e:
            logger.warning(f"âš ï¸ [ê²½ìŸì‚¬ ìˆ˜í˜œ] LLM ë¶„ì„ ì˜¤ë¥˜ '{news_title[:20]}...': {e}")
            return 0

        # 2. ë¦¬ìŠ¤í¬ ì•„ë‹ˆë©´ Skip
        if not analysis_result.get('is_risk'):
            return 0
        
        event_type = analysis_result.get('event_type', 'OTHER')
        benefit_score = analysis_result.get('competitor_benefit_score', 0)
        
        logger.info(f"  ğŸ”´ [ì•…ì¬ ê°ì§€/LLM] {stock_code} - {event_type}: {news_title[:50]}... (Score: {benefit_score})")

        # 3. DB ë¡œì§ (Thread-Safeí•˜ê²Œ ë‚´ë¶€ì—ì„œ ì„¸ì…˜ ìƒì„±) - Deadlock ì¬ì‹œë„
        import random
        max_retries = 3
        for attempt in range(max_retries):
            try:
                with session_scope() as session:
                    # í•´ë‹¹ ì¢…ëª©ì˜ ì„¹í„° í™•ì¸
                    affected_stock = session.query(IndustryCompetitors).filter(
                        IndustryCompetitors.stock_code == stock_code
                    ).first()
                    
                    if not affected_stock:
                        return 0
                    
                    sector_code = affected_stock.sector_code
                    sector_name = affected_stock.sector_name
                    affected_name = affected_stock.stock_name
                    
                    # ë™ì¼ ì„¹í„° ê²½ìŸì‚¬ ì¡°íšŒ
                    competitors = session.query(IndustryCompetitors).filter(
                        IndustryCompetitors.sector_code == sector_code,
                        IndustryCompetitors.stock_code != stock_code,
                        IndustryCompetitors.is_active == 1
                    ).all()
                    
                    if not competitors:
                        return 0
                    
                    # ì´ë²¤íŠ¸ ìƒì„±
                    duration_days = 7
                    if event_type in ['FIRE', 'RECALL', 'SECURITY', 'OWNER_RISK']:
                        duration_days = 30
                    expires_at = datetime.now(timezone.utc) + timedelta(days=duration_days)
                    
                    for competitor in competitors:
                        # ì¤‘ë³µ ì¡°íšŒ
                        existing = session.query(CompetitorBenefitEvents).filter(
                            CompetitorBenefitEvents.affected_stock_code == stock_code,
                            CompetitorBenefitEvents.beneficiary_stock_code == competitor.stock_code,
                            CompetitorBenefitEvents.event_type == event_type,
                            CompetitorBenefitEvents.detected_at >= datetime.now(timezone.utc) - timedelta(hours=24)
                        ).first()
                        
                        if existing:
                            continue
                        
                        # ìƒˆë¡œìš´ ì´ë²¤íŠ¸ ì¶”ê°€
                        benefit_event = CompetitorBenefitEvents(
                            affected_stock_code=stock_code,
                            affected_stock_name=affected_name,
                            event_type=event_type,
                            event_title=news_title[:1000],
                            event_severity=-10,
                            source_url=news_link,
                            beneficiary_stock_code=competitor.stock_code,
                            beneficiary_stock_name=competitor.stock_name,
                            benefit_score=benefit_score,
                            sector_code=sector_code,
                            sector_name=sector_name,
                            status='ACTIVE',
                            expires_at=expires_at
                        )
                        session.add(benefit_event)
                        events_created += 1
                        
                        # Redis ì €ì¥ (Loop ì•ˆì—ì„œ í˜¸ì¶œí•˜ë˜, ì—ëŸ¬ë‚˜ë„ ì§„í–‰)
                        try:
                            database.set_competitor_benefit_score(
                                stock_code=competitor.stock_code,
                                score=benefit_score,
                                reason=f"ê²½ìŸì‚¬ {affected_name}ì˜ {event_type} ì•…ì¬ë¡œ ì¸í•œ ìˆ˜í˜œ (LLM Analysis)",
                                affected_stock=stock_code,
                                event_type=event_type,
                                ttl=duration_days * 86400
                            )
                        except Exception as e:
                            logger.warning(f"âš ï¸ [ê²½ìŸì‚¬ ìˆ˜í˜œ] Redis ì €ì¥ ì‹¤íŒ¨: {e}")

                        logger.info(
                            f"  âœ… [ìˆ˜í˜œ ë“±ë¡] {competitor.stock_name}({competitor.stock_code}) "
                            f"+{benefit_score}ì  â† {affected_name} {event_type}"
                        )
                    
                    # session_scope exit -> commit
                    return events_created
                    
            except Exception as e:
                error_str = str(e)
                is_deadlock = "1213" in error_str or "Deadlock" in error_str
                
                if is_deadlock and attempt < max_retries - 1:
                    wait_time = random.uniform(0.1, 0.5) * (attempt + 1)
                    logger.info(f"ğŸ”„ [ê²½ìŸì‚¬ ìˆ˜í˜œ] Deadlock ê°ì§€, {wait_time:.2f}ì´ˆ í›„ ì¬ì‹œë„ ({attempt + 1}/{max_retries})...")
                    time.sleep(wait_time)
                    continue
                else:
                    logger.error(f"âŒ [ê²½ìŸì‚¬ ìˆ˜í˜œ] DB ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    return 0

    total_events_created = 0
    futures = []
    
    # [Local LLM] ì œí•œ ì—†ì´ ëª¨ë“  ì¢…ëª© ë‰´ìŠ¤ ë¶„ì„ (ë¹„ìš© â‚©0)
    # stock_codeê°€ ìˆëŠ” ë¬¸ì„œë§Œ ë¶„ì„ ëŒ€ìƒ
    stock_docs = [doc for doc in documents if doc.metadata.get("stock_code")]
    logger.info(f"  [ê²½ìŸì‚¬ ìˆ˜í˜œ] ì¢…ëª© ë‰´ìŠ¤ {len(stock_docs)}ê°œ / ì „ì²´ {len(documents)}ê°œ")
    
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        for doc in stock_docs:
            futures.append(executor.submit(_analyze_single_competitor_benefit, doc))
            
        for future in as_completed(futures):
            try:
                total_events_created += future.result()
            except Exception as e:
                logger.error(f"âŒ [ê²½ìŸì‚¬ ìˆ˜í˜œ] ìŠ¤ë ˆë“œ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")

    logger.info(f"âœ… [ê²½ìŸì‚¬ ìˆ˜í˜œ] ìˆ˜í˜œ ì´ë²¤íŠ¸ {total_events_created}ê±´ ìƒì„± ì™„ë£Œ (ë³‘ë ¬ ì²˜ë¦¬)")



def add_documents_to_chroma(documents):
    """
    ìƒˆë¡œìš´ Document ë¦¬ìŠ¤íŠ¸ë¥¼ ë¶„í• (Chunking) í›„ ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ ChromaDBì— ì €ì¥í•©ë‹ˆë‹¤.
    """
    step_id = "(5/6)"
    if not documents:
        logger.info(f"  {step_id} [App 5] Chromaì— ì €ì¥í•  ìƒˆë¡œìš´ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. (Skip Write)")
        return

    logger.info(f"  {step_id} [App 5] 'ìƒˆ' ë¬¸ì„œ {len(documents)}ê°œ í…ìŠ¤íŠ¸ ë¶„í•  ë° ì„ë² ë”© ì¤‘...")
    try:
        # Local Embedding ì‚¬ìš© - í•„í„°ë§ ì—†ì´ ëª¨ë“  ë¬¸ì„œ ì„ë² ë”© (ë¹„ìš© â‚©0)
        splitted_docs = text_splitter.split_documents(documents)
        
        for i in range(0, len(splitted_docs), VERTEX_AI_BATCH_SIZE): # type: ignore
            batch_docs = splitted_docs[i : i + VERTEX_AI_BATCH_SIZE]
            logger.info(f"  {step_id} [App 4] 'ìƒˆ' ì²­í¬ {i+1} ~ {i+len(batch_docs)}ë²ˆ (ì´ {len(batch_docs)}ê°œ) ì €ì¥ ì‹œë„...")
            vectorstore.add_documents(
                batch_docs
            )
        
        logger.info("="*60)
        logger.info("ğŸ  [LOCAL] ì„ë² ë”© ì™„ë£Œ - HuggingFace (ko-sroberta) ì‚¬ìš©")
        logger.info("ğŸ  [LOCAL] Cloud API í˜¸ì¶œ ì—†ìŒ - ë¹„ìš©: â‚©0")
        logger.info("="*60)
        logger.info(f"âœ… {step_id} Chroma ì„œë²„ì— 'ìƒˆ' ì²­í¬ ì´ {len(splitted_docs)}ê°œ ì €ì¥ ì™„ë£Œ!")
    except Exception as e:
        logger.exception(f"ğŸ”¥ {step_id} [App 4] Chroma ì„œë²„ì— 'Write' ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ")

def cleanup_old_data_job():
    """
    DATA_TTL_DAYS(7ì¼)ê°€ ì§€ë‚œ ì˜¤ë˜ëœ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ChromaDBì—ì„œ ì‚­ì œí•©ë‹ˆë‹¤.
    """
    logger.info(f"\n[ë°ì´í„° ì •ë¦¬] {DATA_TTL_DAYS}ì¼ ê²½ê³¼í•œ ì˜¤ë˜ëœ RAG ë°ì´í„° ì‚­ì œ ì‹œì‘...")
    try:
        ttl_limit_timestamp = int((datetime.now(timezone.utc) - timedelta(days=DATA_TTL_DAYS)).timestamp())
        collection = vectorstore._collection
        
        logger.info(f"... [ë°ì´í„° ì •ë¦¬] created_at_utc < {ttl_limit_timestamp} ë°ì´í„° ì‚­ì œ ì¤‘ ...")
        collection.delete(where={"created_at_utc": {"$lt": ttl_limit_timestamp}})
        
        logger.info("âœ… [ë°ì´í„° ì •ë¦¬] ì˜¤ë˜ëœ ë°ì´í„° ì‚­ì œ ì™„ë£Œ.")
    except Exception as e:
        logger.warning(f"âš ï¸ [ë°ì´í„° ì •ë¦¬] ë°ì´í„° ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

# ==============================================================================
# ë©”ì¸ ì‘ì—… ì‹¤í–‰ í•¨ìˆ˜
# ==============================================================================

def run_collection_job():
    """
    ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ì €ì¥ì„ ìœ„í•œ ë©”ì¸ íƒœìŠ¤í¬.
    ì´ í•¨ìˆ˜ê°€ ìŠ¤í¬ë¦½íŠ¸ì˜ 'ì§„ì…ì (Entrypoint)'ì´ ë©ë‹ˆë‹¤.
    KOSPI 200 ì „ì²´ ë‰´ìŠ¤ ìˆ˜ì§‘ (Scout Universeì™€ ë™ì¼)
    """
    logger.info(f"\n--- [RAG ìˆ˜ì§‘ ë´‡ v9.0] ì‘ì—… ì‹œì‘ ---")
    
    # [Operating Hours Check] â€” mock/testì—ì„œëŠ” ìŠ¤í‚µ ê°€ëŠ¥
    disable_market_open_check = os.getenv("DISABLE_MARKET_OPEN_CHECK", "false").lower() in {"1", "true", "yes", "on"}
    if not disable_market_open_check:
        from shared.utils import is_operating_hours
        if not is_operating_hours():
            logger.info("ğŸ•’ í˜„ì¬ ìš´ì˜ ì‹œê°„ì´ ì•„ë‹™ë‹ˆë‹¤. (ìš´ì˜ ì‹œê°„: í‰ì¼ 07:00 ~ 17:00) ì‘ì—…ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return
    
    # ì„œë¹„ìŠ¤ ì´ˆê¸°í™” (ì§€ì—° ì´ˆê¸°í™”)
    try:
        initialize_services()
    except Exception as e:
        logger.error(f"ğŸ”¥ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return
    
    try:
        all_fetched_documents = []

        # 1. 'ì¼ë°˜ ê²½ì œ' RSS ìˆ˜ì§‘
        general_news_docs = crawl_general_news()
        all_fetched_documents.extend(general_news_docs)

        # 2. KOSPI 200 Universe ë¡œë“œ (Scoutì™€ ë™ì¼)
        universe = get_kospi_200_universe()
        logger.info(f"  (2/6) KOSPI Universe {len(universe)}ê°œ ì¢…ëª© ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘...")

        # 3. ê° ì¢…ëª©ë³„ ë‰´ìŠ¤ í¬ë¡¤ë§ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰
        with ThreadPoolExecutor(max_workers=10) as executor:
            future_to_stock = {executor.submit(crawl_news_for_stock, stock["code"], stock["name"]): stock for stock in universe}
            for future in as_completed(future_to_stock):
                stock = future_to_stock[future]
                try:
                    fetched_docs = future.result()
                    all_fetched_documents.extend(fetched_docs)
                except Exception as exc:
                    logger.error(f"ğŸ”¥ '{stock['name']}' ë‰´ìŠ¤ ìˆ˜ì§‘ ìŠ¤ë ˆë“œì—ì„œ ì˜¤ë¥˜ ë°œìƒ: {exc}")

        # 4. 'ìƒˆë¡œìš´' ë¬¸ì„œë§Œ í•„í„°ë§ (Deduplication)
        new_documents_to_add = filter_new_documents(all_fetched_documents)
        
        # [New] 4-1. ìƒˆë¡œìš´ ë¬¸ì„œ ê°ì„± ë¶„ì„ ë° ì €ì¥
        if os.getenv("ENABLE_NEWS_ANALYSIS", "true").lower() == "true":
            process_sentiment_analysis(new_documents_to_add)
        
            # 4-2. ê²½ìŸì‚¬ ìˆ˜í˜œ ë¶„ì„ ë° ì €ì¥
            process_competitor_benefit_analysis(new_documents_to_add)
        else:
            logger.info("âš ï¸ [Config] 'ENABLE_NEWS_ANALYSIS=false' ì„¤ì •ìœ¼ë¡œ ì¸í•´ ë¶„ì„ ë‹¨ê³„ ìƒëµ.")
        
        # 5. 'ìƒˆë¡œìš´' ë¬¸ì„œë§Œ Chroma ì„œë²„ì— ì €ì¥ (Write)
        add_documents_to_chroma(new_documents_to_add)
        
        # 6. ì˜¤ë˜ëœ ë°ì´í„° ì •ë¦¬
        cleanup_old_data_job()
        
        logger.info(f"--- [RAG ìˆ˜ì§‘ ë´‡ v9.1] ì‘ì—… ì™„ë£Œ ---")
        
    except Exception as e:
        logger.exception(f"ğŸ”¥ [RAG ìˆ˜ì§‘ ë´‡ v9.0] ë©”ì¸ ì‘ì—… ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ")

# =============================================================================
# ë©”ì¸ ì‹¤í–‰ ë¸”ë¡
# =============================================================================

if __name__ == "__main__":
    
    start_time = time.time()

    # ë©”ì¸ ì‘ì—… ì‹¤í–‰
    try:
        run_collection_job()
    except Exception as e:
        logger.critical(f"âŒ [RAG Crawler v8.1] 'run_collection_job' ì‹¤í–‰ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜: {e}")
        
    end_time = time.time()
    logger.info(f"--- [RAG ìˆ˜ì§‘ ë´‡ v8.1] ìŠ¤í¬ë¦½íŠ¸ ì¢…ë£Œ (ì´ ì†Œìš”ì‹œê°„: {end_time - start_time:.2f}ì´ˆ) ---")
