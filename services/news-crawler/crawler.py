#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# crawler_job.py
# Version: v1.0
# ì‘ì—… LLM: Claude Opus 4.5
# Crawler Job - Cloud Scheduler(HTTP)ì— ì˜í•´ 10ë¶„ë§ˆë‹¤ ì‹¤í–‰ë˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
# KOSPI 200 ì „ì²´ ë‰´ìŠ¤ ìˆ˜ì§‘ (WatchList ì˜ì¡´ì„± ì œê±°)
# ê²½ìŸì‚¬ ìˆ˜í˜œ ë¶„ì„ ì—°ë™

from concurrent.futures import ThreadPoolExecutor, as_completed
import time
# import chromadb  # Lazy importë¡œ ë³€ê²½ (ì´ˆê¸°í™” ì‹œê°„ ë‹¨ì¶•)
import sys
import json
import urllib.parse
import feedparser # type: ignore
import logging
import os 
import calendar
import warnings
from dotenv import load_dotenv 
from datetime import datetime, timedelta, timezone

# FinanceDataReader for KOSPI 200 Universe
try:
    import FinanceDataReader as fdr
    FDR_AVAILABLE = True
except ImportError:
    FDR_AVAILABLE = False

# 'youngs75_jennie' íŒ¨í‚¤ì§€ë¥¼ ì°¾ê¸° ìœ„í•´ í”„ë¡œì íŠ¸ ë£¨íŠ¸ í´ë”ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
# Dockerfileì—ì„œ /app/crawler_job.pyë¡œ ë³µì‚¬ë˜ë¯€ë¡œ, /appì´ í”„ë¡œì íŠ¸ ë£¨íŠ¸
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
sys.path.append(PROJECT_ROOT)

# ==============================================================================
# ë¡œê±°(Logger) ì„¤ì •
# ==============================================================================
logger = logging.getLogger(__name__)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - [%(funcName)s] - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

try:
    import shared.auth as auth
    import shared.database as database
    from shared.llm import JennieBrain # ê°ì„± ë¶„ì„ì„ ìœ„í•œ JennieBrain ì„í¬íŠ¸
    from shared.db.connection import session_scope, ensure_engine_initialized
    from shared.db.models import WatchList as WatchListModel
    from shared.gemini import ensure_gemini_api_key
    # ê²½ìŸì‚¬ ìˆ˜í˜œ ë¶„ì„ ëª¨ë“ˆ
    # from shared.news_classifier import NewsClassifier, get_classifier
    # from shared.hybrid_scoring.competitor_analyzer import CompetitorAnalyzer
    logger.info("âœ… 'shared' íŒ¨í‚¤ì§€ ëª¨ë“ˆ import ì„±ê³µ")
except ImportError as e: # type: ignore
    logger.error(f"ğŸš¨ 'shared' ê³µìš© íŒ¨í‚¤ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤! (ì˜¤ë¥˜: {e})")
    auth = None
    database = None
    JennieBrain = None
    ensure_gemini_api_key = None
    NewsClassifier = None
    get_classifier = None
    CompetitorAnalyzer = None
except Exception as e:
    logger.error(f"ğŸš¨ 'shared' íŒ¨í‚¤ì§€ import ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
    auth = None
    database = None
    JennieBrain = None
    ensure_gemini_api_key = None
    NewsClassifier = None
    get_classifier = None
    CompetitorAnalyzer = None

from langchain_core.documents import Document
from langchain_chroma import Chroma
# [Cost Optimization] Cloud Embedding -> Local Embedding
# langchain_google_genai -> langchain_huggingface
try:
    from langchain_huggingface import HuggingFaceEmbeddings
    LOCAL_EMBEDDINGS_AVAILABLE = True
except ImportError:
    LOCAL_EMBEDDINGS_AVAILABLE = False
    logger.warning("âš ï¸ langchain_huggingface not available, falling back to Cloud Embeddings")
    from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

# ==============================================================================
# 1. ì „ì—­ ì„¤ì • (Constants)
# ==============================================================================

# Chroma ì„œë²„
CHROMA_SERVER_HOST = os.getenv("CHROMA_SERVER_HOST", "10.178.0.2") 
CHROMA_SERVER_PORT = 8000
COLLECTION_NAME = "rag_stock_data"

# RAG ì„¤ì •
DATA_TTL_DAYS = 7
VERTEX_AI_BATCH_SIZE = 10
MAX_SENTIMENT_DOCS_PER_RUN = int(os.getenv("MAX_SENTIMENT_DOCS_PER_RUN", "40"))
SENTIMENT_COOLDOWN_SECONDS = float(os.getenv("SENTIMENT_COOLDOWN_SECONDS", "0.2"))

# --- ğŸ”½ ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ í¬ë¡¤ë§ ì„¤ì • ğŸ”½ ---
# ë‰´ìŠ¤ ì†ŒìŠ¤ ì„ íƒ: "google" (ê¸°ì¡´), "naver" (ì‹ ê·œ), "hybrid" (ë‘˜ ë‹¤)
NEWS_CRAWLER_SOURCE = os.getenv("NEWS_CRAWLER_SOURCE", "naver")
NAVER_FINANCE_NEWS_URL = "https://finance.naver.com/item/news_news.naver?code={code}&page={page}"
NAVER_NEWS_MAX_PAGES = int(os.getenv("NAVER_NEWS_MAX_PAGES", "2"))  # í˜ì´ì§€ë‹¹ ~15ê±´
NAVER_NEWS_REQUEST_DELAY = float(os.getenv("NAVER_NEWS_REQUEST_DELAY", "0.3"))  # Rate limit ëŒ€ì‘

# --- ğŸ”½ 'ì¼ë°˜ ê²½ì œ' RSS í”¼ë“œ ğŸ”½ ---
GENERAL_RSS_FEEDS = [
    {"source_name": "Hankyung (Finance)", "url": "https://www.hankyung.com/feed/finance"},
    {"source_name": "Hankyung (Economy)", "url": "https://www.hankyung.com/feed/economy"},
    {"source_name": "Maeil Business (Economy)", "url": "https://www.mk.co.kr/rss/50000001/"},
    {"source_name": "Maeil Business (Stock)", "url": "https://www.mk.co.kr/rss/50100001/"},
]

# ==============================================================================
# ë‰´ìŠ¤ ì†ŒìŠ¤ í•„í„°ë§ ì„¤ì • (2026-01 í˜„ì 3ì¸ í”¼ë“œë°± ë°˜ì˜)
# ==============================================================================

# Tier 1: ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ê²½ì œ/ê¸ˆìœµ ì „ë¬¸ì§€ ë„ë©”ì¸ (hostname suffix ë§¤ì¹­)
TRUSTED_NEWS_DOMAINS = {
    "hankyung.com",      # í•œêµ­ê²½ì œ
    "mk.co.kr",          # ë§¤ì¼ê²½ì œ
    "sedaily.com",       # ì„œìš¸ê²½ì œ
    "mt.co.kr",          # ë¨¸ë‹ˆíˆ¬ë°ì´
    "fnnews.com",        # íŒŒì´ë‚¸ì…œë‰´ìŠ¤
    "thebell.co.kr",     # ë”ë²¨ (M&A/IB)
    "newspim.com",       # ë‰´ìŠ¤í•Œ
    "edaily.co.kr",      # ì´ë°ì¼ë¦¬
    "etoday.co.kr",      # ì´íˆ¬ë°ì´
    "yna.co.kr",         # ì—°í•©ë‰´ìŠ¤
    "etnews.com",        # ì „ìì‹ ë¬¸ (IT/ë°˜ë„ì²´)
    "biz.chosun.com",    # ì¡°ì„ ë¹„ì¦ˆ
    "newsis.com",        # ë‰´ì‹œìŠ¤
}

# Wrapper ë„ë©”ì¸ (í¬í„¸/êµ¬ê¸€ - ì‹ ë¢° ì†ŒìŠ¤ë¡œ ì·¨ê¸‰í•˜ì§€ ì•ŠìŒ)
WRAPPER_DOMAINS = {
    "news.naver.com", "n.news.naver.com",
    "v.daum.net", "news.v.daum.net",
    "news.google.com",
}

# ë…¸ì´ì¦ˆ í‚¤ì›Œë“œ (ì œëª©ì— ìˆìœ¼ë©´ ì €í’ˆì§ˆë¡œ íŒë‹¨í•˜ì—¬ ì œì™¸)
NOISE_KEYWORDS = [
    "íŠ¹ì§•ì£¼", "ì˜¤ì „ ì‹œí™©", "ì¥ë§ˆê°", "ë§ˆê° ì‹œí™©", "ê¸‰ë“±ë½",
    "ì˜¤ëŠ˜ì˜ ì¦ì‹œ", "í™˜ìœ¨", "ê°œì¥", "ì¶œë°œ", "ìƒìœ„ ì¢…ëª©",
    "ì¥ì¤‘ ì‹œí™©", "ê±°ë˜ëŸ‰ ìƒìœ„", "ì™¸ì¸ ìˆœë§¤ìˆ˜", "ê¸°ê´€ ìˆœë§¤ìˆ˜",
]

# ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì–¸ë¡ ì‚¬ ì´ë¦„ (Google News source.title ë§¤ì¹­ìš©)
TRUSTED_SOURCE_NAMES = {
    "í•œêµ­ê²½ì œ", "í•œê²½", "Hankyung",
    "ë§¤ì¼ê²½ì œ", "ë§¤ê²½", "MK",
    "ì„œìš¸ê²½ì œ",
    "ë¨¸ë‹ˆíˆ¬ë°ì´",
    "íŒŒì´ë‚¸ì…œë‰´ìŠ¤",
    "ë”ë²¨", "thebell",
    "ë‰´ìŠ¤í•Œ",
    "ì´ë°ì¼ë¦¬",
    "ì´íˆ¬ë°ì´",
    "ì—°í•©ë‰´ìŠ¤", "ì—°í•©ë‰´ìŠ¤TV",
    "ì „ìì‹ ë¬¸", "ETNews",
    "ì¡°ì„ ë¹„ì¦ˆ",
    "ë‰´ì‹œìŠ¤",
    "í—¤ëŸ´ë“œê²½ì œ",
    "ì•„ì‹œì•„ê²½ì œ",
    "ë°ì¼ë¦¬ì•ˆ",
    "ë‰´ìŠ¤1",
}

# ==============================================================================
# LangChain, Chroma í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
# ==============================================================================

# ==============================================================================
# ì „ì—­ ë³€ìˆ˜ (ì§€ì—° ì´ˆê¸°í™”)
# ==============================================================================

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (ëª¨ë“ˆ ì„í¬íŠ¸ ì‹œ)
load_dotenv()

GCP_PROJECT_ID = os.getenv("GCP_PROJECT_ID")
# Oracle/OCI ê´€ë ¨ ì„¤ì •ì€ ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. (MariaDB + SQLAlchemy ë‹¨ì¼í™”)

# ì§€ì—° ì´ˆê¸°í™”ë¥¼ ìœ„í•œ ì „ì—­ ë³€ìˆ˜ (Noneìœ¼ë¡œ ì‹œì‘)
embeddings = None
text_splitter = None
db_client = None
vectorstore = None
jennie_brain = None # JennieBrain ì¸ìŠ¤í„´ìŠ¤

def initialize_services():
    """
    LangChain ë° ChromaDB ì„œë¹„ìŠ¤ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
    run_collection_job() ì‹¤í–‰ ì‹œì—ë§Œ í˜¸ì¶œë©ë‹ˆë‹¤.
    """
    global embeddings, text_splitter, db_client, vectorstore, jennie_brain
    
    # SQLAlchemy ì—”ì§„ ì´ˆê¸°í™” (session_scope ì‚¬ìš© ì „ì— í•„ìˆ˜)
    try:
        ensure_engine_initialized()
        logger.info("âœ… SQLAlchemy ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ")
    except Exception as e:
        logger.warning(f"âš ï¸ SQLAlchemy ì—”ì§„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    logger.info("... [RAG Crawler v10.0] LangChain ë° AI ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì‹œì‘ ...")
    try:
        # [Cost Optimization] Local Embeddings ì‚¬ìš© (Cloud API ë¹„ìš© â‚©0)
        if LOCAL_EMBEDDINGS_AVAILABLE:
            logger.info("="*60)
            logger.info("ğŸ  [LOCAL] Embedding ëª¨ë¸ ë¡œë”© ì¤‘ (jhgan/ko-sroberta-multitask)")
            logger.info("ğŸ  [LOCAL] Cloud API í˜¸ì¶œ ì—†ìŒ - ë¹„ìš©: â‚©0")
            logger.info("="*60)
            embeddings = HuggingFaceEmbeddings(
                model_name="jhgan/ko-sroberta-multitask",  # í•œêµ­ì–´ ìµœì í™” ëª¨ë¸
                model_kwargs={"device": "cpu"},
                encode_kwargs={"normalize_embeddings": True}
            )
            logger.info("âœ… [LOCAL] Embedding ëª¨ë¸ ë¡œë”© ì™„ë£Œ! (ë¹„ìš©: â‚©0)")
        else:
            # Fallback: Cloud Embeddings (ë¹„ìš© ë°œìƒ)
            logger.error("="*60)
            logger.error("ğŸš¨ [CLOUD] Cloud Embedding ì‚¬ìš© ì¤‘! - ë¹„ìš© ë°œìƒ!")
            logger.error("ğŸš¨ [CLOUD] langchain-huggingface ì„¤ì¹˜ í•„ìš”!")
            logger.error("="*60)
            api_key = ensure_gemini_api_key()
            embeddings = GoogleGenerativeAIEmbeddings(
                model="models/text-embedding-004",
                google_api_key=api_key,
            )
        
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
        logger.info("âœ… LangChain ì»´í¬ë„ŒíŠ¸(Embedding, Splitter) ì´ˆê¸°í™” ì„±ê³µ.")
        
        # JennieBrain ì´ˆê¸°í™” (ê°ì„± ë¶„ì„ìš©)
        try:
            jennie_brain = JennieBrain(
                project_id=GCP_PROJECT_ID,
                gemini_api_key_secret=os.getenv("SECRET_ID_GEMINI_API_KEY")
            )
            logger.info("âœ… JennieBrain (ê°ì„± ë¶„ì„ê¸°) ì´ˆê¸°í™” ì„±ê³µ.")
        except Exception as e:
            logger.warning(f"âš ï¸ JennieBrain ì´ˆê¸°í™” ì‹¤íŒ¨ (ê°ì„± ë¶„ì„ Skip): {e}")
            jennie_brain = None



    except Exception as e:
        logger.exception("ğŸ”¥ LangChain ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨!")
        raise
    
    logger.info(f"... [RAG Crawler v8.1] Chroma ì„œë²„ ({CHROMA_SERVER_HOST}:{CHROMA_SERVER_PORT}) ì—°ê²° ì‹œë„ ...")
    try:
        # Lazy import: chromadbëŠ” ì‹¤ì œ ì‚¬ìš© ì‹œì ì—ë§Œ import
        import chromadb
        
        db_client = chromadb.HttpClient(host=CHROMA_SERVER_HOST, port=CHROMA_SERVER_PORT)
        vectorstore = Chroma(client=db_client, collection_name=COLLECTION_NAME, embedding_function=embeddings)
        db_client.heartbeat() 
        logger.info(f"âœ… Chroma ì„œë²„ ({CHROMA_SERVER_HOST}) ì—°ê²° ì„±ê³µ!")
    except Exception as e:
        logger.exception(f"ğŸ”¥ Chroma ì„œë²„ ({CHROMA_SERVER_HOST}) ì—°ê²° ì‹¤íŒ¨!")
        raise

# ==============================================================================
# í•µì‹¬ í•¨ìˆ˜ ì •ì˜
# ==============================================================================

def get_kospi_200_universe():
    """
    KOSPI ì‹œê°€ì´ì•¡ ìƒìœ„ 200ê°œ ì¢…ëª©ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
    Scoutì™€ ë™ì¼í•œ Universeë¥¼ ì‚¬ìš©í•˜ì—¬ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    
    Fallback ìˆœì„œ:
    1. FinanceDataReader API
    2. ë„¤ì´ë²„ ê¸ˆìœµ ì‹œì´ ìŠ¤í¬ë˜í•‘
    3. DB WatchList (ìµœí›„ì˜ ìˆ˜ë‹¨)
    """
    universe_size = int(os.getenv("SCOUT_UNIVERSE_SIZE", "200"))
    logger.info(f"  (1/6) KOSPI ì‹œì´ ìƒìœ„ {universe_size}ê°œ ì¢…ëª© ë¡œë“œ ì¤‘...")
    
    # 1. FinanceDataReader ì‹œë„
    if FDR_AVAILABLE:
        try:
            logger.info("  (1/6) FinanceDataReaderë¡œ KOSPI ì¢…ëª© ì¡°íšŒ ì¤‘...")
            df = fdr.StockListing('KOSPI')
            
            if df is not None and not df.empty:
                # ì‹œê°€ì´ì•¡ ê¸°ì¤€ ì •ë ¬ (Marcap ë˜ëŠ” Market Cap ì»¬ëŸ¼)
                cap_col = None
                for col in ['Marcap', 'MarCap', 'Market Cap', 'marcap']:
                    if col in df.columns:
                        cap_col = col
                        break
                
                if cap_col:
                    df = df.sort_values(by=cap_col, ascending=False)
                
                # ìƒìœ„ Nê°œ ì¶”ì¶œ
                top_stocks = df.head(universe_size)
                
                # Code, Name ì»¬ëŸ¼ ì°¾ê¸°
                code_col = 'Code' if 'Code' in top_stocks.columns else 'Symbol'
                name_col = 'Name' if 'Name' in top_stocks.columns else 'name'
                
                universe = []
                for _, row in top_stocks.iterrows():
                    code = str(row.get(code_col, '')).zfill(6)
                    name = row.get(name_col, f'ì¢…ëª©_{code}')
                    if code and len(code) == 6:
                        universe.append({"code": code, "name": name})
                
                if universe:
                    logger.info(f"âœ… (1/6) FinanceDataReaderë¡œ {len(universe)}ê°œ ì¢…ëª© ë¡œë“œ ì™„ë£Œ!")
                    return universe
        except Exception as e:
            logger.warning(f"âš ï¸ (1/6) FinanceDataReader ì‹¤íŒ¨: {e}")
    
    # 2. Fallback: ë„¤ì´ë²„ ê¸ˆìœµ ì‹œì´ ìŠ¤í¬ë˜í•‘
    universe = _scrape_naver_finance_top_stocks(universe_size)
    if universe:
        return universe
    
    # 3. ìµœí›„ Fallback: DBì˜ WatchList ì‚¬ìš©
    logger.info("  (1/6) ìµœí›„ Fallback: DB WatchList ì¡°íšŒ ì¤‘...")
    return get_watchlist_from_db()


def _scrape_naver_finance_top_stocks(limit: int = 200) -> list:
    """
    ë„¤ì´ë²„ ê¸ˆìœµì—ì„œ KOSPI ì‹œì´ ìƒìœ„ ì¢…ëª©ì„ ìŠ¤í¬ë˜í•‘í•©ë‹ˆë‹¤.
    FDR API ì¥ì•  ì‹œ Fallbackìœ¼ë¡œ ì‚¬ìš©.
    """
    import requests
    from bs4 import BeautifulSoup
    
    logger.info("  (1/6) ë„¤ì´ë²„ ê¸ˆìœµ ì‹œì´ ìŠ¤í¬ë˜í•‘ ì‹œë„ ì¤‘...")
    
    universe = []
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
    
    try:
        # KOSPI ì‹œì´ ìƒìœ„ (í˜ì´ì§€ë‹¹ 50ê°œ, ìµœëŒ€ 4í˜ì´ì§€ = 200ê°œ)
        pages_needed = (limit // 50) + 1
        
        for page in range(1, pages_needed + 1):
            if len(universe) >= limit:
                break
                
            url = f'https://finance.naver.com/sise/sise_market_sum.naver?sosok=0&page={page}'
            resp = requests.get(url, headers=headers, timeout=10)
            soup = BeautifulSoup(resp.text, 'html.parser')
            
            rows = soup.select('table.type_2 tbody tr')
            for row in rows:
                if len(universe) >= limit:
                    break
                    
                cells = row.select('td')
                if len(cells) >= 2:
                    link = cells[1].select_one('a')
                    if link:
                        name = link.text.strip()
                        href = link.get('href', '')
                        code = href.split('code=')[-1][:6] if 'code=' in href else ''
                        if code and len(code) == 6 and code.isdigit():
                            universe.append({"code": code, "name": name})
        
        if universe:
            logger.info(f"âœ… (1/6) ë„¤ì´ë²„ ê¸ˆìœµ ìŠ¤í¬ë˜í•‘ìœ¼ë¡œ {len(universe)}ê°œ ì¢…ëª© ë¡œë“œ ì™„ë£Œ!")
            return universe
        else:
            logger.warning("âš ï¸ (1/6) ë„¤ì´ë²„ ê¸ˆìœµ ìŠ¤í¬ë˜í•‘ ê²°ê³¼ ì—†ìŒ")
            
    except Exception as e:
        logger.warning(f"âš ï¸ (1/6) ë„¤ì´ë²„ ê¸ˆìœµ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
    
    return []


def get_watchlist_from_db():
    """
    DBì—ì„œ WatchListë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤ (Fallbackìš©).
    MariaDB ì‚¬ìš© (pymysql ì§ì ‘ ì—°ê²°).
    """
    try:
        with session_scope(readonly=True) as session:
            rows = session.query(WatchListModel.stock_code, WatchListModel.stock_name).all()
        
        watchlist = []
        for row in rows:
            watchlist.append({"code": row[0], "name": row[1]})
 
        logger.info(f"âœ… (1/6) 'WatchList' {len(watchlist)}ê°œ ë¡œë“œ ì„±ê³µ.")
        return watchlist
        
    except Exception as e:
        logger.error(f"ğŸ”¥ (1/6) DB 'get_watchlist_from_db' í•¨ìˆ˜ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []

def get_numeric_timestamp(feed_entry):
    """
    feed_entryì—ì„œ 'ë°œí–‰ ì‹œê°„'ì„ UTC ê¸°ì¤€ ìˆ«ì íƒ€ì„ìŠ¤íƒ¬í”„ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    """
    if hasattr(feed_entry, 'published_parsed') and feed_entry.published_parsed:
        try:
            # feedparserê°€ ë°˜í™˜í•˜ëŠ” time.struct_timeì€ timezone ì •ë³´ê°€ ì—†ì„ ìˆ˜ ìˆìŒ
            # calendar.timegmì€ ì´ë¥¼ UTCë¡œ ê°„ì£¼í•˜ì—¬ timestampë¥¼ ìƒì„±
            # ì´ê²ƒì´ UTC ê¸°ì¤€ ì‹œê°„ì„ ë³´ì¥í•˜ëŠ” ê°€ì¥ ì•ˆì „í•œ ë°©ë²•
            utc_timestamp = calendar.timegm(feed_entry.published_parsed)
            return int(utc_timestamp)
        except Exception:
            return int(datetime.now(timezone.utc).timestamp())
    else:
        return int(datetime.now(timezone.utc).timestamp())

# ==============================================================================
# ë‰´ìŠ¤ ì†ŒìŠ¤ í•„í„°ë§ ìœ í‹¸ í•¨ìˆ˜ (Phase 1,2,3)
# ==============================================================================

def get_hostname(url: str) -> str:
    """URLì—ì„œ hostname ì¶”ì¶œ"""
    try:
        return (urllib.parse.urlparse(url).hostname or "").lower().strip(".")
    except Exception:
        return ""

def is_trusted_hostname(host: str) -> bool:
    """hostnameì´ ì‹ ë¢° ë„ë©”ì¸ì¸ì§€ í™•ì¸ (suffix ë§¤ì¹­)"""
    return any(host == d or host.endswith("." + d) for d in TRUSTED_NEWS_DOMAINS)

def is_wrapper_domain(host: str) -> bool:
    """hostnameì´ wrapper ë„ë©”ì¸(í¬í„¸/êµ¬ê¸€)ì¸ì§€ í™•ì¸"""
    return any(host == d or host.endswith("." + d) for d in WRAPPER_DOMAINS)

def extract_date_from_url(url: str):
    """
    URL íŒ¨í„´ì—ì„œ ë°œí–‰ì¼ ì¶”ì¶œ (ì˜ˆ: /20250102...)
    í•œê²½, ë§¤ê²½ ë“± ëŒ€ë¶€ë¶„ì˜ êµ­ë‚´ ì–¸ë¡ ì‚¬ ì§€ì›
    """
    import re
    from datetime import date as date_class
    match = re.search(r'/(\d{4})(\d{2})(\d{2})\d+', url)
    if match:
        try:
            return date_class(int(match.group(1)), int(match.group(2)), int(match.group(3)))
        except ValueError:
            return None
    return None

def is_noise_title(title: str) -> bool:
    """ì œëª©ì´ ë…¸ì´ì¦ˆ(ì €í’ˆì§ˆ) ë‰´ìŠ¤ì¸ì§€ í™•ì¸"""
    for noise in NOISE_KEYWORDS:
        if noise in title:
            return True
    return False

def is_trusted_source_name(source_name: str) -> bool:
    """Google Newsì˜ source.titleì´ ì‹ ë¢° ì–¸ë¡ ì‚¬ì¸ì§€ í™•ì¸"""
    if not source_name:
        return False
    for trusted in TRUSTED_SOURCE_NAMES:
        if trusted in source_name:
            return True
    return False

def compute_news_hash(title: str) -> str:
    """ì œëª© ê¸°ë°˜ ì¤‘ë³µ ì²´í¬ìš© í•´ì‹œ"""
    import hashlib
    import re as re_module
    # íŠ¹ìˆ˜ë¬¸ì, ê³µë°± ì •ê·œí™” í›„ í•´ì‹±
    normalized = re_module.sub(r'[^\w]', '', title.lower())
    return hashlib.md5(normalized.encode()).hexdigest()[:12]

# ì„¸ì…˜ ë‚´ ì¤‘ë³µ ì œê±°ìš© ìºì‹œ
_seen_news_hashes = set()

def crawl_naver_finance_news(stock_code: str, stock_name: str, max_pages: int = None) -> list:
    """
    ë„¤ì´ë²„ ê¸ˆìœµì—ì„œ íŠ¹ì • ì¢…ëª©ì˜ ë‰´ìŠ¤ë¥¼ ì§ì ‘ í¬ë¡¤ë§í•©ë‹ˆë‹¤.
    [2026-01-03] Google News RSS ëŒ€ì‹ /ë³´ì¡°ë¡œ ì‚¬ìš©.
    
    Args:
        stock_code: ì¢…ëª© ì½”ë“œ (ì˜ˆ: "005930")
        stock_name: ì¢…ëª©ëª… (ì˜ˆ: "ì‚¼ì„±ì „ì")
        max_pages: ìˆ˜ì§‘í•  ìµœëŒ€ í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸ê°’: NAVER_NEWS_MAX_PAGES í™˜ê²½ë³€ìˆ˜)
    
    Returns:
        Document ë¦¬ìŠ¤íŠ¸ (ê¸°ì¡´ crawl_news_for_stockê³¼ ë™ì¼í•œ í˜•ì‹)
    """
    import requests
    from bs4 import BeautifulSoup
    from datetime import date as date_class
    
    if max_pages is None:
        max_pages = NAVER_NEWS_MAX_PAGES
    
    logger.info(f"  (2/6) [Naver Finance] '{stock_name}({stock_code})' ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘ (max_pages={max_pages})")
    documents = []
    
    # í•„í„°ë§ í†µê³„
    stats = {"total": 0, "noise": 0, "old": 0, "dup": 0, "accepted": 0}
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Referer': f'https://finance.naver.com/item/news.naver?code={stock_code}'
    }
    
    for page in range(1, max_pages + 1):
        try:
            url = NAVER_FINANCE_NEWS_URL.format(code=stock_code, page=page)
            resp = requests.get(url, headers=headers, timeout=10)
            resp.encoding = 'euc-kr'
            
            if resp.status_code != 200:
                logger.warning(f"  [Naver Finance] HTTP {resp.status_code} for {stock_code} page {page}")
                continue
            
            soup = BeautifulSoup(resp.text, 'html.parser')
            news_table = soup.select_one('table.type5')
            
            if not news_table:
                logger.debug(f"  [Naver Finance] ë‰´ìŠ¤ í…Œì´ë¸” ì—†ìŒ - {stock_code} page {page}")
                break  # ë” ì´ìƒ í˜ì´ì§€ ì—†ìŒ
            
            rows = news_table.select('tr')
            page_news_count = 0
            
            for row in rows:
                title_td = row.select_one('td.title')
                if not title_td:
                    continue
                
                link = title_td.select_one('a')
                if not link:
                    continue
                
                info_td = row.select_one('td.info')  # ì–¸ë¡ ì‚¬
                date_td = row.select_one('td.date')  # ë‚ ì§œ
                
                title = link.text.strip()
                href = link.get('href', '')
                source = info_td.text.strip() if info_td else 'N/A'
                date_str = date_td.text.strip() if date_td else ''
                
                if not title:
                    continue
                
                stats["total"] += 1
                
                # 1. ë…¸ì´ì¦ˆ í•„í„°ë§
                if is_noise_title(title):
                    stats["noise"] += 1
                    continue
                
                # 2. ë‚ ì§œ í•„í„°ë§ (7ì¼ ì´ë‚´)
                if date_str:
                    try:
                        # í˜•ì‹: "2026.01.03 08:54"
                        date_part = date_str.split()[0]  # "2026.01.03"
                        parts = date_part.split('.')
                        if len(parts) == 3:
                            article_date = date_class(int(parts[0]), int(parts[1]), int(parts[2]))
                            days_old = (date_class.today() - article_date).days
                            if days_old > 7:
                                stats["old"] += 1
                                continue
                    except (ValueError, IndexError):
                        pass  # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì¼ë‹¨ í¬í•¨
                
                # 3. ì¤‘ë³µ ì²´í¬
                news_hash = compute_news_hash(title)
                if news_hash in _seen_news_hashes:
                    stats["dup"] += 1
                    continue
                _seen_news_hashes.add(news_hash)
                
                # ëª¨ë“  í•„í„° í†µê³¼ -> ìˆ˜ì§‘
                stats["accepted"] += 1
                page_news_count += 1
                
                # ë§í¬ ì •ê·œí™”
                if href.startswith('/'):
                    full_link = 'https://finance.naver.com' + href
                else:
                    full_link = href
                
                # íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„± (ë‚ ì§œ ë¬¸ìì—´ ê¸°ë°˜)
                published_timestamp = int(datetime.now(timezone.utc).timestamp())
                if date_str:
                    try:
                        dt = datetime.strptime(date_str, '%Y.%m.%d %H:%M')
                        dt = dt.replace(tzinfo=timezone(timedelta(hours=9)))  # KST
                        published_timestamp = int(dt.timestamp())
                    except ValueError:
                        pass
                
                doc = Document(
                    page_content=f"ë‰´ìŠ¤ ì œëª©: {title}\në§í¬: {full_link}",
                    metadata={
                        "stock_code": stock_code,
                        "stock_name": stock_name,
                        "source": f"Naver Finance ({source})",
                        "source_url": full_link,
                        "created_at_utc": published_timestamp
                    }
                )
                documents.append(doc)
            
            logger.debug(f"  [Naver Finance] {stock_code} page {page}: {page_news_count}ê±´ ìˆ˜ì§‘")
            
            # Rate limit ëŒ€ì‘
            if page < max_pages:
                time.sleep(NAVER_NEWS_REQUEST_DELAY)
                
        except Exception as e:
            logger.exception(f"ğŸ”¥ [Naver Finance] {stock_code} page {page} í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
    
    # í•„í„°ë§ í†µê³„ ë¡œê·¸
    if stats["total"] > 0:
        logger.info(f"  (2/6) [{stock_name}] Naver í•„í„°ë§: ì´{stats['total']} â†’ noise:{stats['noise']} old:{stats['old']} dup:{stats['dup']} â†’ ìˆ˜ì§‘:{stats['accepted']}")
    else:
        logger.info(f"  (2/6) [{stock_name}] Naver ë‰´ìŠ¤ ì—†ìŒ")
    
    return documents


def crawl_stock_news_with_fallback(stock_code: str, stock_name: str) -> list:
    """
    ì¢…ëª© ë‰´ìŠ¤ í¬ë¡¤ë§ (Naver ìš°ì„ , Google Fallback)
    
    1. ë„¤ì´ë²„ ê¸ˆìœµì—ì„œ ë¨¼ì € í¬ë¡¤ë§ ì‹œë„
    2. ì‹¤íŒ¨í•˜ê±°ë‚˜ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ Google News RSSë¡œ Fallback
    
    Args:
        stock_code: ì¢…ëª© ì½”ë“œ
        stock_name: ì¢…ëª©ëª…
    
    Returns:
        Document ë¦¬ìŠ¤íŠ¸
    """
    # 1ì°¨: ë„¤ì´ë²„ ê¸ˆìœµ ì‹œë„
    try:
        naver_docs = crawl_naver_finance_news(stock_code, stock_name)
        if naver_docs:
            return naver_docs
        else:
            # ë„¤ì´ë²„ì—ì„œ ë‰´ìŠ¤ê°€ ì—†ìœ¼ë©´ Google Fallback
            logger.info(f"  [Fallback] {stock_name}: Naver ë‰´ìŠ¤ 0ê±´ â†’ Google News ì‹œë„")
    except Exception as e:
        logger.warning(f"  [Fallback] {stock_name}: Naver í¬ë¡¤ë§ ì˜¤ë¥˜ ({e}) â†’ Google News ì‹œë„")
    
    # 2ì°¨: Google News Fallback
    try:
        google_docs = crawl_news_for_stock(stock_code, stock_name)
        if google_docs:
            logger.info(f"  [Fallback] {stock_name}: Google Newsì—ì„œ {len(google_docs)}ê±´ ìˆ˜ì§‘")
        return google_docs
    except Exception as e:
        logger.error(f"ğŸ”¥ [Fallback] {stock_name}: Google Newsë„ ì‹¤íŒ¨ - {e}")
        return []


def crawl_news_for_stock(stock_code, stock_name):
    """
    Google News RSSë¥¼ ì‚¬ìš©í•˜ì—¬ íŠ¹ì • ì¢…ëª©ì˜ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    [2026-01 ê°œì„ ] ì‹ ë¢° ì†ŒìŠ¤ í•„í„°ë§ + ë…¸ì´ì¦ˆ í‚¤ì›Œë“œ ì œì™¸ + ì¤‘ë³µ ì œê±°
    """
    logger.info(f"  (2/6) [App 5] '{stock_name}({stock_code})' Google News RSS í”¼ë“œ ìˆ˜ì§‘ ì¤‘...")
    documents = []
    
    # í•„í„°ë§ í†µê³„
    stats = {"total": 0, "wrapper": 0, "untrusted": 0, "noise": 0, "old": 0, "dup": 0, "accepted": 0}
    
    try:
        query = f'"{stock_name}" OR "{stock_code}"'
        encoded_query = urllib.parse.quote(query)
        rss_url = f"https://news.google.com/rss/search?q={encoded_query}&hl=ko&gl=KR&ceid=KR:ko"
        feed = feedparser.parse(rss_url)
        
        if not feed.entries:
            logger.info(f"  (2/6) '{stock_name}' ê´€ë ¨ ì‹ ê·œ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. (Skip)")
            return []

        for entry in feed.entries:
            stats["total"] += 1
            
            # 1. ì†ŒìŠ¤ ê²€ì¦ (Google wrapper vs ì§ì ‘ URL)
            host = get_hostname(entry.link)
            source_title = entry.get('source', {}).get('title', '')
            
            if is_wrapper_domain(host):
                # Google/í¬í„¸ wrapperì¸ ê²½ìš° -> source.titleë¡œ ì‹ ë¢° ê²€ì¦
                if not is_trusted_source_name(source_title):
                    stats["untrusted"] += 1
                    continue
                # ì‹ ë¢° ì–¸ë¡ ì‚¬ë©´ wrapperì—¬ë„ í†µê³¼
            else:
                # ì§ì ‘ URLì¸ ê²½ìš° -> hostnameìœ¼ë¡œ ì‹ ë¢° ê²€ì¦
                if not is_trusted_hostname(host):
                    stats["untrusted"] += 1
                    continue
            
            # 3. ë…¸ì´ì¦ˆ í‚¤ì›Œë“œ ì²´í¬
            if is_noise_title(entry.title):
                stats["noise"] += 1
                continue
            
            # 4. ë‚ ì§œ í•„í„°ë§ (URL íŒ¨í„´ ìš°ì„ , fallbackì€ RSS)
            article_date = extract_date_from_url(entry.link)
            if article_date:
                from datetime import date as date_class
                days_old = (date_class.today() - article_date).days
                if days_old > 7:
                    stats["old"] += 1
                    continue
            else:
                # fallback: RSS published ë‚ ì§œ
                published_timestamp = get_numeric_timestamp(entry)
                if datetime.fromtimestamp(published_timestamp, tz=timezone.utc) < datetime.now(timezone.utc) - timedelta(days=7):
                    stats["old"] += 1
                    continue
            
            # 5. ì¤‘ë³µ ì²´í¬ (ì œëª© í•´ì‹œ)
            news_hash = compute_news_hash(entry.title)
            if news_hash in _seen_news_hashes:
                stats["dup"] += 1
                continue
            _seen_news_hashes.add(news_hash)
            
            # ëª¨ë“  í•„í„° í†µê³¼ -> ìˆ˜ì§‘
            stats["accepted"] += 1
            published_timestamp = get_numeric_timestamp(entry)
            
            doc = Document(
                page_content=f"ë‰´ìŠ¤ ì œëª©: {entry.title}\në§í¬: {entry.link}",
                metadata={
                    "stock_code": stock_code,
                    "stock_name": stock_name,
                    "source": f"Google News RSS ({entry.get('source', {}).get('title', 'N/A')})",
                    "source_url": entry.link, 
                    "created_at_utc": published_timestamp
                }
            )
            documents.append(doc)
            
    except Exception as e:
        logger.exception(f"ğŸ”¥ (2/6) '{stock_name}' ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
    
    # í•„í„°ë§ í†µê³„ ë¡œê·¸
    if stats["total"] > 0:
        logger.info(f"  (2/6) [{stock_name}] í•„í„°ë§: ì´{stats['total']} â†’ wrapper:{stats['wrapper']} untrusted:{stats['untrusted']} noise:{stats['noise']} old:{stats['old']} dup:{stats['dup']} â†’ ìˆ˜ì§‘:{stats['accepted']}")
    
    return documents


def crawl_general_news():
    """
    ë¯¸ë¦¬ ì •ì˜ëœ 'GENERAL_RSS_FEEDS' ëª©ë¡ì˜ ì¼ë°˜ ê²½ì œ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    """
    logger.info(f"  (3/6) [App 5] 'ì¼ë°˜ ê²½ì œ' RSS {len(GENERAL_RSS_FEEDS)}ê°œ í”¼ë“œ ìˆ˜ì§‘ ì¤‘...")
    documents = []
    
    for feed_info in GENERAL_RSS_FEEDS:
        source = feed_info["source_name"]
        url = feed_info["url"]
        logger.info(f"  (3/6) ... '{source}' ìˆ˜ì§‘ ì¤‘ ...")
        try:
            feed = feedparser.parse(url)
            if not feed.entries:
                logger.info(f"  (3/6) '{source}'ì— ì‹ ê·œ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. (Skip)")
                continue

            for entry in feed.entries:
                # 7ì¼ì´ ì§€ë‚œ ë‰´ìŠ¤ëŠ” ìˆ˜ì§‘ ë‹¨ê³„ì—ì„œ ì œì™¸
                published_timestamp = get_numeric_timestamp(entry)
                if datetime.fromtimestamp(published_timestamp, tz=timezone.utc) < datetime.now(timezone.utc) - timedelta(days=7):
                    logger.debug(f"  (3/6) ì˜¤ë˜ëœ ë‰´ìŠ¤ ì œì™¸: {entry.title[:30]}...")
                    continue

                doc = Document(
                    page_content=f"ë‰´ìŠ¤ ì œëª©: {entry.title}\në§í¬: {entry.link}",
                    metadata={
                        "source": source,
                        "source_url": entry.link, 
                        "created_at_utc": published_timestamp
                    }
                )
                documents.append(doc)
        except Exception as e:
            logger.exception(f"ğŸ”¥ (3/6) '{source}' ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
            
    logger.info(f"âœ… (3/6) 'ì¼ë°˜ ê²½ì œ' ë‰´ìŠ¤ ì´ {len(documents)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ.")
    return documents

def filter_new_documents(documents):
    """
    ChromaDBì— 'source_url'ì´ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ì—¬ ìƒˆë¡œìš´ ë¬¸ì„œë§Œ í•„í„°ë§í•©ë‹ˆë‹¤.
    """
    step_id = "(4/6)"
    logger.info(f"  {step_id} [App 5] ìˆ˜ì§‘ëœ ë¬¸ì„œ {len(documents)}ê°œ ì¼ê´„ ì¤‘ë³µ ê²€ì‚¬ ì‹œì‘...")
    if not documents:
        return []

    urls_to_check = list(set([doc.metadata["source_url"] for doc in documents if "source_url" in doc.metadata]))
    if not urls_to_check:
        return documents

    existing_results = vectorstore.get(where={"source_url": {"$in": urls_to_check}})
    existing_urls = set(item['source_url'] for item in existing_results.get('metadatas', []))
    new_docs = [doc for doc in documents if doc.metadata.get("source_url") not in existing_urls]

    logger.info(f"âœ… {step_id} ì¤‘ë³µ ê²€ì‚¬ ì™„ë£Œ. ìƒˆë¡œìš´ ë¬¸ì„œ {len(new_docs)}ê°œ ë°œê²¬.")
    return new_docs

def process_unified_analysis(documents):
    """
    [2026-01 Optimized] í†µí•© ë‰´ìŠ¤ ë¶„ì„ (ê°ì„± + ê²½ìŸì‚¬ ë¦¬ìŠ¤í¬)
    Single-Pass LLM Callë¡œ ë‘ ê°€ì§€ ë¶„ì„ì„ ë™ì‹œì— ìˆ˜í–‰í•©ë‹ˆë‹¤.
    - ê°ì„± ë¶„ì„: Redis & MariaDB ì €ì¥
    - ë¦¬ìŠ¤í¬ íƒì§€: ê²½ìŸì‚¬ ìˆ˜í˜œ ì´ë²¤íŠ¸ ìƒì„±
    """
    if not jennie_brain or not documents:
        return

    logger.info("="*60)
    # [Fix] Log actual model name from Env (since JennieBrain doesn't expose it directly)
    model_name = os.getenv("LOCAL_MODEL_FAST", "gemma3:27b")
    logger.info(f"ğŸš€ [Unified] í†µí•© ë‰´ìŠ¤ ë¶„ì„ ì‹œì‘ - Ollama ({model_name})")
    logger.info("ğŸš€ [Unified] Single-Pass LLM Call (Sentiment + Risk) - ë¹„ìš©/ì‹œê°„ ìµœì í™”")
    logger.info("="*60)
    
    # stock_codeê°€ ìˆëŠ” ë¬¸ì„œë§Œ ë¶„ì„ ëŒ€ìƒ
    stock_docs = [doc for doc in documents if doc.metadata.get("stock_code")]
    logger.info(f"  [Unified] ëŒ€ìƒ ì¢…ëª© ë‰´ìŠ¤ {len(stock_docs)}ê°œ / ì „ì²´ {len(documents)}ê°œ")
    
    if not stock_docs:
        return

    # ë°°ì¹˜ ì¤€ë¹„
    batch_items = []
    doc_map = {}
    
    for idx, doc in enumerate(stock_docs):
        content_lines = doc.page_content.split('\n')
        news_title = content_lines[0].replace("ë‰´ìŠ¤ ì œëª©: ", "") if len(content_lines) > 0 else "ì œëª© ì—†ìŒ"
        
        batch_items.append({
            "id": idx,
            "title": news_title,
            "summary": news_title 
        })
        doc_map[idx] = doc
    
    # ë°°ì¹˜ ë¶„ì„ ì‹¤í–‰ (Sequential Batch Processing - Best Performance for Local LLM)
    BATCH_SIZE = 5
    batches = [batch_items[i:i + BATCH_SIZE] for i in range(0, len(batch_items), BATCH_SIZE)]
    all_results = []
    
    logger.info(f"  [Unified] ì´ {len(batches)}ê°œ ë°°ì¹˜ ìˆœì°¨ ë¶„ì„ ì‹œì‘ (Single Thread)...")
    
    for i, batch in enumerate(batches):
        try:
            results = jennie_brain.analyze_news_unified(batch)
            all_results.extend(results)
            logger.info(f"  [Unified] ë°°ì¹˜ {i+1}/{len(batches)} ë¶„ì„ ì™„ë£Œ ({len(results)}ê±´)")
        except Exception as e:
            logger.warning(f"âš ï¸ [Unified] ë°°ì¹˜ {i+1} ë¶„ì„ ì‹¤íŒ¨: {e}")
            # Fallback handled inside analyze_news_unified usually using get_unified_fallback_response

    # ê²°ê³¼ ì²˜ë¦¬ (ë³‘ë ¬ ì €ì¥)
    logger.info(f"  [Unified] {len(all_results)}ê±´ ê²°ê³¼ ì²˜ë¦¬ ì‹œì‘ (ë³‘ë ¬ ì €ì¥/ì´ë²¤íŠ¸ ìƒì„±)...")
    
    # DB ì‘ì—…ì´ í˜¼í•©ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
    _process_unified_results_parallel(all_results, doc_map)


def _process_unified_results_parallel(results, doc_map):
    """
    í†µí•© ë¶„ì„ ê²°ê³¼ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    1. ê°ì„± ë¶„ì„ ê²°ê³¼ ì €ì¥ (Redis/DB)
    2. ë¦¬ìŠ¤í¬ íƒì§€ ì‹œ ê²½ìŸì‚¬ ìˆ˜í˜œ ì´ë²¤íŠ¸ ìƒì„±
    """
    MAX_WORKERS = 5 # DB Pool ê³ ë ¤
    processed_count = 0
    risk_event_count = 0
    
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = []
        for res in results:
            futures.append(executor.submit(_handle_single_unified_result, res, doc_map))
            
        for future in as_completed(futures):
            try:
                success, is_risk = future.result()
                if success: processed_count += 1
                if is_risk: risk_event_count += 1
            except Exception as e:
                logger.error(f"âŒ [Unified] ê²°ê³¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                
    logger.info(f"âœ… [Unified] ì™„ë£Œ: ê°ì„±ë¶„ì„ {processed_count}ê±´ ì €ì¥, ë¦¬ìŠ¤í¬ ì´ë²¤íŠ¸ {risk_event_count}ê±´ ìƒì„±.")


def _handle_single_unified_result(result, doc_map):
    """ê°œë³„ í†µí•© ê²°ê³¼ ì²˜ë¦¬ í•¸ë“¤ëŸ¬"""
    idx = result.get('id')
    doc = doc_map.get(idx)
    if not doc: return False, False
    
    # 1. ê°ì„± ë¶„ì„ ì €ì¥
    sentiment = result.get('sentiment', {})
    score = sentiment.get('score', 50)
    reason = sentiment.get('reason', 'N/A')
    
    # (ê¸°ì¡´ _save_single_sentiment_result ë¡œì§ ì¸ë¼ì¸ or ì¬ì‚¬ìš©)
    # ì—¬ê¸°ì„œëŠ” ë¡œì§ì„ ë‹¨ìˆœí™”í•˜ì—¬ ì§ì ‘ í˜¸ì¶œ
    save_success = _save_sentiment_to_db(doc, score, reason)
    
    # 2. ê²½ìŸì‚¬ ë¦¬ìŠ¤í¬ ì´ë²¤íŠ¸ ì²˜ë¦¬
    risk = result.get('competitor_risk', {})
    is_risk = risk.get('is_detected', False)
    
    if is_risk:
        _create_competitor_event(doc, risk)
        
    return save_success, is_risk


def _save_sentiment_to_db(doc, score, reason):
    """ê°ì„± ì ìˆ˜ ì €ì¥ ë¡œì§ (Redis + MariaDB)"""
    stock_code = doc.metadata.get("stock_code")
    stock_name = doc.metadata.get("stock_name")
    content_lines = doc.page_content.split('\n')
    news_title = content_lines[0].replace("ë‰´ìŠ¤ ì œëª©: ", "") if len(content_lines) > 0 else "ì œëª© ì—†ìŒ"
    news_link = doc.metadata.get("source_url")
    published_at = doc.metadata.get("created_at_utc")
    
    news_date_str = None
    if published_at:
        try:
            news_date_str = datetime.fromtimestamp(published_at, tz=timezone.utc).strftime('%Y-%m-%d')
        except Exception: pass
        
    # Redis
    try:
        database.set_sentiment_score(stock_code, score, reason, source_url=news_link, stock_name=stock_name, news_title=news_title, news_date=news_date_str)
    except Exception: pass
    
    # DB
    import random
    for attempt in range(3):
        try:
            with session_scope() as session:
                database.save_news_sentiment(session, stock_code, news_title, score, reason, news_link, published_at)
            return True
        except Exception as e:
            if "Deadlock" in str(e) and attempt < 2:
                time.sleep(random.uniform(0.1, 0.5))
                continue
            return False
    return False


def _create_competitor_event(doc, risk_data):
    """ê²½ìŸì‚¬ ìˆ˜í˜œ ì´ë²¤íŠ¸ ìƒì„± ë¡œì§"""
    stock_code = doc.metadata.get("stock_code")
    content_lines = doc.page_content.split('\n')
    news_title = content_lines[0].replace("ë‰´ìŠ¤ ì œëª©: ", "") if len(content_lines) > 0 else "ì œëª© ì—†ìŒ"
    news_link = doc.metadata.get("source_url")
    
    event_type = risk_data.get('type', 'OTHER')
    benefit_score = risk_data.get('benefit_score', 0)
    
    logger.info(f"  ğŸ”´ [Risk Detected] {stock_code} - {event_type} (Benefit Score: {benefit_score})")

    from shared.db.models import IndustryCompetitors, CompetitorBenefitEvents
    
    try:
        with session_scope() as session:
            # 1. ì„¹í„° í™•ì¸
            affected_stock = session.query(IndustryCompetitors).filter(IndustryCompetitors.stock_code == stock_code).first()
            if not affected_stock: return
            
            # 2. ê²½ìŸì‚¬ ì¡°íšŒ
            competitors = session.query(IndustryCompetitors).filter(
                IndustryCompetitors.sector_code == affected_stock.sector_code,
                IndustryCompetitors.stock_code != stock_code,
                IndustryCompetitors.is_active == 1
            ).all()
            
            if not competitors: return
            
            # 3. ì´ë²¤íŠ¸ ìƒì„±
            duration_days = 30 if event_type in ['FIRE', 'RECALL', 'SECURITY', 'OWNER_RISK'] else 7
            expires_at = datetime.now(timezone.utc) + timedelta(days=duration_days)
            
            for comp in competitors:
                # ì¤‘ë³µ ì²´í¬
                existing = session.query(CompetitorBenefitEvents).filter(
                    CompetitorBenefitEvents.affected_stock_code == stock_code,
                    CompetitorBenefitEvents.beneficiary_stock_code == comp.stock_code,
                    CompetitorBenefitEvents.event_type == event_type,
                    CompetitorBenefitEvents.detected_at >= datetime.now(timezone.utc) - timedelta(hours=24)
                ).first()
                
                if existing: continue
                
                # ì´ë²¤íŠ¸ ë“±ë¡
                event = CompetitorBenefitEvents(
                    affected_stock_code=stock_code,
                    affected_stock_name=affected_stock.stock_name,
                    event_type=event_type,
                    event_title=news_title[:1000],
                    event_severity=-10, # ê¸°ë³¸ê°’
                    source_url=news_link,
                    beneficiary_stock_code=comp.stock_code,
                    beneficiary_stock_name=comp.stock_name,
                    benefit_score=benefit_score,
                    sector_code=affected_stock.sector_code,
                    sector_name=affected_stock.sector_name,
                    status='ACTIVE',
                    expires_at=expires_at
                )
                session.add(event)
                
                # Redis ì—…ë°ì´íŠ¸ (Optional) - try-catch
                try:
                    database.set_competitor_benefit_score(
                        comp.stock_code, benefit_score, 
                        f"ê²½ìŸì‚¬ {affected_stock.stock_name} {event_type} ë°œìƒ (Unified Analysis)",
                        stock_code, event_type, ttl=duration_days*86400
                    )
                except: pass
                
                logger.info(f"  âœ… [ìˆ˜í˜œ ë“±ë¡] {comp.stock_name} +{benefit_score}ì  (by {event_type})")

    except Exception as e:
        logger.error(f"âŒ [Event Creation] ì‹¤íŒ¨: {e}")


                    
                    # ë™ì¼ ì„¹í„° ê²½ìŸì‚¬ ì¡°íšŒ




def add_documents_to_chroma(documents):
    """
    ìƒˆë¡œìš´ Document ë¦¬ìŠ¤íŠ¸ë¥¼ ë¶„í• (Chunking) í›„ ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ ChromaDBì— ì €ì¥í•©ë‹ˆë‹¤.
    """
    step_id = "(5/6)"
    if not documents:
        logger.info(f"  {step_id} [App 5] Chromaì— ì €ì¥í•  ìƒˆë¡œìš´ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. (Skip Write)")
        return

    logger.info(f"  {step_id} [App 5] 'ìƒˆ' ë¬¸ì„œ {len(documents)}ê°œ í…ìŠ¤íŠ¸ ë¶„í•  ë° ì„ë² ë”© ì¤‘...")
    try:
        # Local Embedding ì‚¬ìš© - í•„í„°ë§ ì—†ì´ ëª¨ë“  ë¬¸ì„œ ì„ë² ë”© (ë¹„ìš© â‚©0)
        splitted_docs = text_splitter.split_documents(documents)
        
        for i in range(0, len(splitted_docs), VERTEX_AI_BATCH_SIZE): # type: ignore
            batch_docs = splitted_docs[i : i + VERTEX_AI_BATCH_SIZE]
            logger.info(f"  {step_id} [App 4] 'ìƒˆ' ì²­í¬ {i+1} ~ {i+len(batch_docs)}ë²ˆ (ì´ {len(batch_docs)}ê°œ) ì €ì¥ ì‹œë„...")
            vectorstore.add_documents(
                batch_docs
            )
        
        logger.info("="*60)
        logger.info("ğŸ  [LOCAL] ì„ë² ë”© ì™„ë£Œ - HuggingFace (ko-sroberta) ì‚¬ìš©")
        logger.info("ğŸ  [LOCAL] Cloud API í˜¸ì¶œ ì—†ìŒ - ë¹„ìš©: â‚©0")
        logger.info("="*60)
        logger.info(f"âœ… {step_id} Chroma ì„œë²„ì— 'ìƒˆ' ì²­í¬ ì´ {len(splitted_docs)}ê°œ ì €ì¥ ì™„ë£Œ!")
    except Exception as e:
        logger.exception(f"ğŸ”¥ {step_id} [App 4] Chroma ì„œë²„ì— 'Write' ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ")

def cleanup_old_data_job():
    """
    DATA_TTL_DAYS(7ì¼)ê°€ ì§€ë‚œ ì˜¤ë˜ëœ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ChromaDBì—ì„œ ì‚­ì œí•©ë‹ˆë‹¤.
    """
    logger.info(f"\n[ë°ì´í„° ì •ë¦¬] {DATA_TTL_DAYS}ì¼ ê²½ê³¼í•œ ì˜¤ë˜ëœ RAG ë°ì´í„° ì‚­ì œ ì‹œì‘...")
    try:
        ttl_limit_timestamp = int((datetime.now(timezone.utc) - timedelta(days=DATA_TTL_DAYS)).timestamp())
        collection = vectorstore._collection
        
        logger.info(f"... [ë°ì´í„° ì •ë¦¬] created_at_utc < {ttl_limit_timestamp} ë°ì´í„° ì‚­ì œ ì¤‘ ...")
        collection.delete(where={"created_at_utc": {"$lt": ttl_limit_timestamp}})
        
        logger.info("âœ… [ë°ì´í„° ì •ë¦¬] ì˜¤ë˜ëœ ë°ì´í„° ì‚­ì œ ì™„ë£Œ.")
    except Exception as e:
        logger.warning(f"âš ï¸ [ë°ì´í„° ì •ë¦¬] ë°ì´í„° ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

# ==============================================================================
# ë©”ì¸ ì‘ì—… ì‹¤í–‰ í•¨ìˆ˜
# ==============================================================================

def run_collection_job():
    """
    ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ì €ì¥ì„ ìœ„í•œ ë©”ì¸ íƒœìŠ¤í¬.
    ì´ í•¨ìˆ˜ê°€ ìŠ¤í¬ë¦½íŠ¸ì˜ 'ì§„ì…ì (Entrypoint)'ì´ ë©ë‹ˆë‹¤.
    KOSPI 200 ì „ì²´ ë‰´ìŠ¤ ìˆ˜ì§‘ (Scout Universeì™€ ë™ì¼)
    """
    logger.info(f"\n--- [RAG ìˆ˜ì§‘ ë´‡ v9.0] ì‘ì—… ì‹œì‘ ---")
    
    # [Operating Hours Check] â€” mock/testì—ì„œëŠ” ìŠ¤í‚µ ê°€ëŠ¥
    disable_market_open_check = os.getenv("DISABLE_MARKET_OPEN_CHECK", "false").lower() in {"1", "true", "yes", "on"}
    if not disable_market_open_check:
        from shared.utils import is_operating_hours
        if not is_operating_hours():
            logger.info("ğŸ•’ í˜„ì¬ ìš´ì˜ ì‹œê°„ì´ ì•„ë‹™ë‹ˆë‹¤. (ìš´ì˜ ì‹œê°„: í‰ì¼ 07:00 ~ 17:00) ì‘ì—…ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return
    
    # ì„œë¹„ìŠ¤ ì´ˆê¸°í™” (ì§€ì—° ì´ˆê¸°í™”)
    try:
        initialize_services()
    except Exception as e:
        logger.error(f"ğŸ”¥ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return
    
    try:
        all_fetched_documents = []

        # 1. 'ì¼ë°˜ ê²½ì œ' RSS ìˆ˜ì§‘
        general_news_docs = crawl_general_news()
        all_fetched_documents.extend(general_news_docs)

        # 2. KOSPI 200 Universe ë¡œë“œ (Scoutì™€ ë™ì¼)
        universe = get_kospi_200_universe()
        logger.info(f"  (2/6) KOSPI Universe {len(universe)}ê°œ ì¢…ëª© ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘...")

        # 3. ê° ì¢…ëª©ë³„ ë‰´ìŠ¤ í¬ë¡¤ë§ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰
        with ThreadPoolExecutor(max_workers=10) as executor:
            future_to_stock = {executor.submit(crawl_stock_news_with_fallback, stock["code"], stock["name"]): stock for stock in universe}
            for future in as_completed(future_to_stock):
                stock = future_to_stock[future]
                try:
                    fetched_docs = future.result()
                    all_fetched_documents.extend(fetched_docs)
                except Exception as exc:
                    logger.error(f"ğŸ”¥ '{stock['name']}' ë‰´ìŠ¤ ìˆ˜ì§‘ ìŠ¤ë ˆë“œì—ì„œ ì˜¤ë¥˜ ë°œìƒ: {exc}")

        # 4. 'ìƒˆë¡œìš´' ë¬¸ì„œë§Œ í•„í„°ë§ (Deduplication)
        new_documents_to_add = filter_new_documents(all_fetched_documents)
        
        # [New] 4-1. ìƒˆë¡œìš´ ë¬¸ì„œ ê°ì„± ë¶„ì„ ë° ì €ì¥
        if os.getenv("ENABLE_NEWS_ANALYSIS", "true").lower() == "true":
            # [2026-01 Optimized] Unified Analysis (Sentiment + Risk)
            process_unified_analysis(new_documents_to_add)
        else:
            logger.info("âš ï¸ [Config] 'ENABLE_NEWS_ANALYSIS=false' ì„¤ì •ìœ¼ë¡œ ì¸í•´ ë¶„ì„ ë‹¨ê³„ ìƒëµ.")
        
        # 5. 'ìƒˆë¡œìš´' ë¬¸ì„œë§Œ Chroma ì„œë²„ì— ì €ì¥ (Write)
        add_documents_to_chroma(new_documents_to_add)
        
        # 6. ì˜¤ë˜ëœ ë°ì´í„° ì •ë¦¬
        cleanup_old_data_job()
        
        logger.info(f"--- [RAG ìˆ˜ì§‘ ë´‡ v9.1] ì‘ì—… ì™„ë£Œ ---")
        
    except Exception as e:
        logger.exception(f"ğŸ”¥ [RAG ìˆ˜ì§‘ ë´‡ v9.0] ë©”ì¸ ì‘ì—… ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ")

# =============================================================================
# ë©”ì¸ ì‹¤í–‰ ë¸”ë¡
# =============================================================================

if __name__ == "__main__":
    
    start_time = time.time()

    # ë©”ì¸ ì‘ì—… ì‹¤í–‰
    try:
        run_collection_job()
    except Exception as e:
        logger.critical(f"âŒ [RAG Crawler v8.1] 'run_collection_job' ì‹¤í–‰ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜: {e}")
        
    end_time = time.time()
    logger.info(f"--- [RAG ìˆ˜ì§‘ ë´‡ v8.1] ìŠ¤í¬ë¦½íŠ¸ ì¢…ë£Œ (ì´ ì†Œìš”ì‹œê°„: {end_time - start_time:.2f}ì´ˆ) ---")
