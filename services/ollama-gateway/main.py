"""
services/ollama-gateway/main.py - Ollama Local LLM Gateway
============================================================

ì´ ì„œë¹„ìŠ¤ëŠ” Local LLM (Ollama/Qwen3) ìš”ì²­ì„ ì¤‘ì•™í™”í•˜ì—¬ ìˆœì°¨ ì²˜ë¦¬í•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
---------
1. ìš”ì²­ íì‰: ì—¬ëŸ¬ ì„œë¹„ìŠ¤ì˜ ë™ì‹œ ìš”ì²­ì„ ìˆœì°¨ ì²˜ë¦¬
2. Rate Limiting: Ollama ê³¼ë¶€í•˜ ë°©ì§€
3. Circuit Breaker: ì¥ì•  ì „íŒŒ ì°¨ë‹¨
4. í†µê³„/ëª¨ë‹ˆí„°ë§: ìš”ì²­ í˜„í™© ì¤‘ì•™ ê´€ë¦¬

API Endpoints:
-------------
- POST /api/generate      - í…ìŠ¤íŠ¸ ìƒì„± ìš”ì²­
- POST /api/generate-json - JSON ìƒì„± ìš”ì²­ (ìŠ¤í‚¤ë§ˆ ê²€ì¦ í¬í•¨)
- GET  /health            - í—¬ìŠ¤ ì²´í¬
- GET  /stats             - ìš”ì²­ í†µê³„

ì°¸ì¡°:
----
- KIS Gateway íŒ¨í„´ ì ìš© (services/kis-gateway/main.py)
"""

import json
import logging
import os
import time
import threading
from collections import deque
from datetime import datetime, timezone
from functools import wraps
from typing import Dict, Any, Optional

from flask import Flask, request, jsonify
from flask_limiter import Limiter
from flask_limiter.util import get_remote_address
from pybreaker import CircuitBreaker, CircuitBreakerError, CircuitBreakerListener
import requests

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - [%(funcName)s] - %(message)s'
)
logger = logging.getLogger("ollama-gateway")

app = Flask(__name__)

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
OLLAMA_HOST = os.getenv("OLLAMA_HOST", "http://localhost:11434")
REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:6379/0")

# Secret Loading (DeepSeek/Cloud Proxy)
SECRETS_FILE = os.getenv("SECRETS_FILE", "/app/config/secrets.json")
OLLAMA_API_KEY = os.getenv("OLLAMA_API_KEY")

if not OLLAMA_API_KEY and os.path.exists(SECRETS_FILE):
    try:
        with open(SECRETS_FILE, "r") as f:
            secrets = json.load(f)
            # Support both naming conventions
            OLLAMA_API_KEY = secrets.get("ollama_api_key") or secrets.get("ollama-api-key")
    except Exception as e:
        logger.warning(f"âš ï¸ Failed to load secrets from {SECRETS_FILE}: {e}")

if OLLAMA_API_KEY:
    logger.info("ğŸ” OLLAMA_API_KEY loaded (Cloud Proxy Enabled)")
else:
    logger.info("â„¹ï¸ OLLAMA_API_KEY not found (Local Mode Only)")

# Rate Limit ì„¤ì • (ì—”ë“œí¬ì¸íŠ¸ë³„ + ëª¨ë¸ë³„ ì°¨ë“±)
# generate/chat (heavy): LLM ì¶”ë¡  (GPU ë¶€í•˜ ë†’ìŒ, 10-30ì´ˆ/ìš”ì²­)
# generate/chat (fast):  ê²½ëŸ‰ ëª¨ë¸ (exaone ë“±, 1-5ì´ˆ/ìš”ì²­)
# embed:                 ì„ë² ë”© ìƒì„± (GPU ë¶€í•˜ ë‚®ìŒ, 0.15-0.25ì´ˆ/ìš”ì²­)
RATE_LIMIT = os.getenv("OLLAMA_RATE_LIMIT", "60 per minute")
RATE_LIMIT_FAST = os.getenv("OLLAMA_RATE_LIMIT_FAST", "120 per minute")
RATE_LIMIT_EMBED = os.getenv("OLLAMA_RATE_LIMIT_EMBED", "3000 per minute")

# ê²½ëŸ‰ ëª¨ë¸ íŒ¨í„´ (FAST rate limit ì ìš© ëŒ€ìƒ)
FAST_MODEL_PREFIXES = os.getenv("OLLAMA_FAST_MODELS", "exaone").split(",")

logger.info(f"ğŸš€ Ollama Gateway ì‹œì‘")
logger.info(f"   OLLAMA_HOST: {OLLAMA_HOST}")
logger.info(f"   REDIS_URL: {REDIS_URL}")
logger.info(f"   RATE_LIMIT: {RATE_LIMIT}")
logger.info(f"   RATE_LIMIT_FAST: {RATE_LIMIT_FAST} (models: {FAST_MODEL_PREFIXES})")
logger.info(f"   RATE_LIMIT_EMBED: {RATE_LIMIT_EMBED}")


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# Rate Limiter ì„¤ì •
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

def get_global_key():
    """
    ëª¨ë“  í´ë¼ì´ì–¸íŠ¸ì˜ ìš”ì²­ì„ í•˜ë‚˜ì˜ ë²„í‚·ìœ¼ë¡œ í†µí•©í•˜ê¸° ìœ„í•œ Key í•¨ìˆ˜.
    OllamaëŠ” ë‹¨ì¼ GPUì—ì„œ ì‹¤í–‰ë˜ë¯€ë¡œ IP ê¸°ë°˜ì´ ì•„ë‹Œ ì „ì—­ í‚¤ë¥¼ ì‚¬ìš©í•´ì•¼ í•¨.
    """
    return "ollama_global"


def _is_fast_model(model_name: str) -> bool:
    """ê²½ëŸ‰ ëª¨ë¸ ì—¬ë¶€ íŒë³„"""
    if not model_name:
        return False
    model_lower = model_name.lower()
    return any(prefix.strip().lower() in model_lower for prefix in FAST_MODEL_PREFIXES)


def get_model_aware_key():
    """
    ëª¨ë¸ ê¸°ë°˜ rate limit í‚¤.
    ê²½ëŸ‰ ëª¨ë¸(exaone ë“±)ì€ ë³„ë„ ë²„í‚·ìœ¼ë¡œ ë¶„ë¦¬í•˜ì—¬ heavy ëª¨ë¸ê³¼ ë…ë¦½ì ìœ¼ë¡œ rate limit ì ìš©.
    """
    try:
        data = request.get_json(silent=True) or {}
        model = data.get("model", "")
        if _is_fast_model(model):
            return "ollama_fast"
    except Exception:
        pass
    return "ollama_global"


def get_dynamic_generate_limit():
    """generate/chat ìš”ì²­ì˜ ëª¨ë¸ë³„ ë™ì  rate limit ë°˜í™˜"""
    try:
        data = request.get_json(silent=True) or {}
        model = data.get("model", "")
        if _is_fast_model(model):
            return RATE_LIMIT_FAST
    except Exception:
        pass
    return RATE_LIMIT


limiter = Limiter(
    app=app,
    key_func=get_global_key,  # â­ï¸ ì¤‘ìš”: IP ê¸°ë°˜ì´ ì•„ë‹Œ ì „ì—­ í‚¤ ì‚¬ìš©
    storage_uri=REDIS_URL,
    default_limits=[],  # ì—”ë“œí¬ì¸íŠ¸ë³„ë¡œ ê°œë³„ ì„¤ì •
    strategy="fixed-window"
)


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# Circuit Breaker ì„¤ì •
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

class GatewayCircuitBreakerListener(CircuitBreakerListener):
    """Circuit Breaker ìƒíƒœ ë³€ê²½ ê°ì§€ ë¦¬ìŠ¤ë„ˆ"""
    
    def state_change(self, breaker, old, new):
        logger.warning(f"ğŸ”Œ [Circuit Breaker] ìƒíƒœ ë³€ê²½: {old.name} â†’ {new.name}")
        if new.name == "open":
            stats['circuit_breaker_open_count'] += 1


ollama_circuit_breaker = CircuitBreaker(
    fail_max=int(os.getenv('OLLAMA_CIRCUIT_FAIL_MAX', '5')),  # 5íšŒ ì—°ì† ì‹¤íŒ¨ ì‹œ OPEN
    reset_timeout=int(os.getenv('OLLAMA_CIRCUIT_RESET_TIMEOUT', '120')),  # 2ë¶„ í›„ HALF-OPEN
    listeners=[GatewayCircuitBreakerListener()]
)


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# í†µê³„
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

stats = {
    'total_requests': 0,
    'successful_requests': 0,
    'failed_requests': 0,
    'rate_limited_requests': 0,
    'circuit_breaker_open_count': 0,
    'avg_response_time_ms': 0,
    'request_history': deque(maxlen=100),  # ìµœê·¼ 100ê°œ ìš”ì²­ ê¸°ë¡
    'queue_depth': 0,  # í˜„ì¬ ëŒ€ê¸° ì¤‘ì¸ ìš”ì²­ ìˆ˜
}

# ìš”ì²­ í ì„¸ë§ˆí¬ì–´ (ë™ì‹œ ì²˜ë¦¬ ì œì–´)
max_concurrent = int(os.getenv("OLLAMA_MAX_CONCURRENT_REQUESTS", "3"))
request_lock = threading.BoundedSemaphore(value=max_concurrent)


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# Ollama API í˜¸ì¶œ ë˜í¼
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

def call_ollama_with_breaker(endpoint: str, payload: Dict[str, Any], timeout: int = 600) -> Dict[str, Any]:
    """
    Circuit Breakerë¥¼ ì ìš©í•œ Ollama API í˜¸ì¶œ ë˜í¼
    [Cloud Proxy] Intercepts requests for known cloud models and routes to external API.
    """
    model_name = payload.get("model", "")
    
    # ğŸŒ©ï¸ Cloud Proxy Logic (Ollama Cloud)
    # Triggered for models ending in ':cloud'
    if model_name.endswith(":cloud"):
         if OLLAMA_API_KEY:
             try:
                return _call_ollama_cloud(endpoint, payload, timeout)
             except Exception as e:
                logger.warning(f"âš ï¸ [Cloud Proxy] Failed (Quota/Error): {e}. Fallback to Local 'gpt-oss:20b'...")
                # Fallback to local model
                payload["model"] = "gpt-oss:20b"
         else:
             # No API key, fallback immediately
             logger.warning(f"âš ï¸ [Cloud Proxy] No API Key for {model_name}. Fallback to Local 'gpt-oss:20b'")
             payload["model"] = "gpt-oss:20b"

    url = f"{OLLAMA_HOST}{endpoint}"
    
    # ìŠ¤íŠ¸ë¦¬ë° ë¹„í™œì„±í™”, ëª¨ë¸ ìœ ì§€
    payload["stream"] = False
    payload["keep_alive"] = -1
    
    @ollama_circuit_breaker
    def _call():
        response = requests.post(url, json=payload, timeout=timeout)
        response.raise_for_status()
        return response.json()
    
    return _call()

def _call_ollama_cloud(endpoint: str, payload: Dict[str, Any], timeout: int) -> Dict[str, Any]:
    """Ollama Cloud API Proxy (Transparent Proxy)"""
    # https://ollama.com API endpoint
    # The client library uses https://ollama.com as host, modifying the path.
    # Typically /api/chat -> https://ollama.com/api/chat
    cloud_host = "https://ollama.com"

    headers = {
        "Authorization": f"Bearer {OLLAMA_API_KEY}",
        "Content-Type": "application/json"
    }
    
    # Ollama Cloudì—ì„œ :cloudëŠ” ì‹¤ì œ ëª¨ë¸ íƒœê·¸ â†’ ìœ ì§€
    upstream_model = payload["model"]
    if not upstream_model.endswith(":cloud"):
        upstream_model = f"{upstream_model}:cloud"
    payload["model"] = upstream_model

    # Ensure stream is false for our gateway
    payload["stream"] = False

    # Ollama CloudëŠ” /api/chatë§Œ ì§€ì› â†’ /api/generateë¥¼ /api/chatìœ¼ë¡œ ë³€í™˜
    cloud_endpoint = endpoint
    if endpoint == "/api/generate":
        cloud_endpoint = "/api/chat"
        prompt = payload.pop("prompt", "")
        payload["messages"] = [{"role": "user", "content": prompt}]

    url = f"{cloud_host}{cloud_endpoint}"
    logger.info(f"ğŸŒ©ï¸ [Cloud Proxy] Routing to Ollama Cloud: {upstream_model} ({cloud_endpoint})")

    response = requests.post(url, json=payload, headers=headers, timeout=timeout)
    try:
        response.raise_for_status()
    except requests.exceptions.HTTPError as e:
        logger.error(f"Cloud API Error: {response.text}")
        raise e

    result = response.json()

    # /api/generate í˜¸í™˜: chat ì‘ë‹µì„ generate í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    if endpoint == "/api/generate" and "message" in result:
        result["response"] = result.get("message", {}).get("content", "")

    return result


def check_ollama_health() -> bool:
    """Ollama ì„œë²„ í—¬ìŠ¤ ì²´í¬"""
    try:
        response = requests.get(f"{OLLAMA_HOST}/api/tags", timeout=5)
        return response.status_code == 200
    except Exception:
        return False


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# Health Check
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

@app.route("/health", methods=["GET"])
def health():
    """í—¬ìŠ¤ ì²´í¬"""
    ollama_healthy = check_ollama_health()
    circuit_state = ollama_circuit_breaker.current_state
    
    status = "healthy" if ollama_healthy and circuit_state != "open" else "degraded"
    
    return jsonify({
        "status": status,
        "service": "ollama-gateway",
        "ollama_status": "connected" if ollama_healthy else "disconnected",
        "circuit_breaker": str(circuit_state),
        "queue_depth": stats['queue_depth'],
        "timestamp": datetime.now(timezone.utc).isoformat(),
    }), 200 if status == "healthy" else 503


@app.route("/stats", methods=["GET"])
def get_stats():
    """ìš”ì²­ í†µê³„ ì¡°íšŒ"""
    return jsonify({
        "total_requests": stats['total_requests'],
        "successful_requests": stats['successful_requests'],
        "failed_requests": stats['failed_requests'],
        "rate_limited_requests": stats['rate_limited_requests'],
        "circuit_breaker_open_count": stats['circuit_breaker_open_count'],
        "avg_response_time_ms": round(stats['avg_response_time_ms'], 2),
        "queue_depth": stats['queue_depth'],
        "circuit_breaker_state": str(ollama_circuit_breaker.current_state),
        "recent_requests": list(stats['request_history'])[-10:],  # ìµœê·¼ 10ê°œ
    })


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# API Endpoints
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

@app.route("/api/generate", methods=["POST"])
@limiter.limit(get_dynamic_generate_limit, key_func=get_model_aware_key)
def generate():
    """
    í…ìŠ¤íŠ¸ ìƒì„± ìš”ì²­ (Ollama /api/generate Proxy)
    
    Request Body:
    {
        "model": "qwen3:32b",
        "prompt": "Hello, world!",
        "options": {"temperature": 0.2}
    }
    """
    stats['total_requests'] += 1
    stats['queue_depth'] += 1
    
    start_time = time.time()
    request_id = f"gen_{int(start_time * 1000)}"
    
    try:
        data = request.get_json()
        model = data.get("model", "qwen3:32b")
        prompt = data.get("prompt", "")
        
        logger.info(f"ğŸ“¥ [Request {request_id}] í…ìŠ¤íŠ¸ ìƒì„± ìš”ì²­ (model={model}, prompt_len={len(prompt)})")
        
        with request_lock:  # ìˆœì°¨ ì²˜ë¦¬ ë³´ì¥
            result = call_ollama_with_breaker("/api/generate", data)
        
        elapsed_ms = (time.time() - start_time) * 1000
        stats['successful_requests'] += 1
        _update_avg_response_time(elapsed_ms)
        
        logger.info(f"âœ… [Request {request_id}] ì™„ë£Œ ({elapsed_ms:.0f}ms)")
        
        stats['request_history'].append({
            "id": request_id,
            "type": "generate",
            "model": model,
            "status": "success",
            "elapsed_ms": round(elapsed_ms, 0),
            "timestamp": datetime.now(timezone.utc).isoformat(),
        })
        
        return jsonify(result)
        
    except CircuitBreakerError:
        stats['failed_requests'] += 1
        logger.error(f"âŒ [Request {request_id}] Circuit Breaker OPEN - ìš”ì²­ ê±°ë¶€")
        return jsonify({
            "error": "Service temporarily unavailable (Circuit Breaker Open)",
            "retry_after": ollama_circuit_breaker.reset_timeout,
        }), 503
        
    except requests.exceptions.Timeout:
        stats['failed_requests'] += 1
        logger.error(f"âŒ [Request {request_id}] Timeout")
        return jsonify({"error": "Ollama request timeout"}), 504
        
    except Exception as e:
        stats['failed_requests'] += 1
        logger.error(f"âŒ [Request {request_id}] Error: {e}")
        return jsonify({"error": str(e)}), 500
        
    finally:
        stats['queue_depth'] -= 1

@app.route("/api/chat", methods=["POST"])
@limiter.limit(get_dynamic_generate_limit, key_func=get_model_aware_key)
def chat():
    """
    ì±„íŒ… ì™„ë£Œ ìš”ì²­ (Ollama /api/chat Proxy)
    
    Request Body:
    {
        "model": "qwen3:32b",
        "messages": [
            {"role": "user", "content": "Hello"}
        ],
        "options": {"temperature": 0.2}
    }
    """
    stats['total_requests'] += 1
    stats['queue_depth'] += 1
    
    start_time = time.time()
    request_id = f"chat_{int(start_time * 1000)}"
    
    try:
        data = request.get_json()
        model = data.get("model", "qwen3:32b")
        messages = data.get("messages", [])
        
        logger.info(f"ğŸ“¥ [Request {request_id}] ì±„íŒ… ìš”ì²­ (model={model}, msgs={len(messages)})")
        
        with request_lock:  # ìˆœì°¨ ì²˜ë¦¬ ë³´ì¥
            result = call_ollama_with_breaker("/api/chat", data)
        
        elapsed_ms = (time.time() - start_time) * 1000
        stats['successful_requests'] += 1
        _update_avg_response_time(elapsed_ms)
        
        logger.info(f"âœ… [Request {request_id}] ì™„ë£Œ ({elapsed_ms:.0f}ms)")
        
        stats['request_history'].append({
            "id": request_id,
            "type": "chat",
            "model": model,
            "status": "success",
            "elapsed_ms": round(elapsed_ms, 0),
            "timestamp": datetime.now(timezone.utc).isoformat(),
        })
        
        return jsonify(result)
        
    except CircuitBreakerError:
        stats['failed_requests'] += 1
        logger.error(f"âŒ [Request {request_id}] Circuit Breaker OPEN - ìš”ì²­ ê±°ë¶€")
        return jsonify({
            "error": "Service temporarily unavailable (Circuit Breaker Open)",
            "retry_after": ollama_circuit_breaker.reset_timeout,
        }), 503
        
    except requests.exceptions.Timeout:
        stats['failed_requests'] += 1
        logger.error(f"âŒ [Request {request_id}] Timeout")
        return jsonify({"error": "Ollama request timeout"}), 504
        
    except Exception as e:
        stats['failed_requests'] += 1
        logger.error(f"âŒ [Request {request_id}] Error: {e}")
        return jsonify({"error": str(e)}), 500
        
    finally:
        stats['queue_depth'] -= 1


@app.route("/api/embed", methods=["POST"])
@app.route("/api/embeddings", methods=["POST"])
@limiter.limit(RATE_LIMIT_EMBED)
def embed():
    """
    ì„ë² ë”© ìƒì„± ìš”ì²­ (Ollama /api/embed Proxy)
    
    Request Body:
    {
        "model": "daynice/kure-v1",
        "input": "Hello world" or ["Hello", "world"]
    }
    """
    stats['total_requests'] += 1
    stats['queue_depth'] += 1
    
    start_time = time.time()
    request_id = f"emb_{int(start_time * 1000)}"
    
    try:
        data = request.get_json()
        model = data.get("model", "unknown")
        input_data = data.get("input", "")
        
        # input ê¸¸ì´ ë¡œê¹… (ë¬¸ìì—´ ë˜ëŠ” ë¦¬ìŠ¤íŠ¸)
        input_len = len(input_data) if isinstance(input_data, list) else len(str(input_data))
        logger.info(f"ğŸ“¥ [Request {request_id}] ì„ë² ë”© ìš”ì²­ (model={model}, input_len={input_len})")
        
        # Endpoint determination based on request path
        # langchain_ollama uses /api/embed (new), others might use /api/embeddings (old)
        target_endpoint = request.path
        
        with request_lock:  # ìˆœì°¨ ì²˜ë¦¬ ë³´ì¥
            result = call_ollama_with_breaker(target_endpoint, data)
        
        elapsed_ms = (time.time() - start_time) * 1000
        stats['successful_requests'] += 1
        _update_avg_response_time(elapsed_ms)
        
        logger.info(f"âœ… [Request {request_id}] ì™„ë£Œ ({elapsed_ms:.0f}ms)")
        
        stats['request_history'].append({
            "id": request_id,
            "type": "embed",
            "model": model,
            "status": "success",
            "elapsed_ms": round(elapsed_ms, 0),
            "timestamp": datetime.now(timezone.utc).isoformat(),
        })
        
        return jsonify(result)
        
    except CircuitBreakerError:
        stats['failed_requests'] += 1
        logger.error(f"âŒ [Request {request_id}] Circuit Breaker OPEN - ìš”ì²­ ê±°ë¶€")
        return jsonify({
            "error": "Service temporarily unavailable (Circuit Breaker Open)",
            "retry_after": ollama_circuit_breaker.reset_timeout,
        }), 503
        
    except requests.exceptions.Timeout:
        stats['failed_requests'] += 1
        logger.error(f"âŒ [Request {request_id}] Timeout")
        return jsonify({"error": "Ollama request timeout"}), 504
        
    except Exception as e:
        stats['failed_requests'] += 1
        logger.error(f"âŒ [Request {request_id}] Error: {e}")
        return jsonify({"error": str(e)}), 500
        
    finally:
        stats['queue_depth'] -= 1
@limiter.limit(get_dynamic_generate_limit, key_func=get_model_aware_key)
def generate_json():
    """
    JSON ìƒì„± ìš”ì²­ (ìŠ¤í‚¤ë§ˆ ê²€ì¦ í¬í•¨)
    
    Request Body:
    {
        "model": "qwen3:32b",
        "prompt": "...",
        "response_schema": {...},
        "options": {"temperature": 0.2}
    }
    """
    stats['total_requests'] += 1
    stats['queue_depth'] += 1
    
    start_time = time.time()
    request_id = f"json_{int(start_time * 1000)}"
    
    try:
        data = request.get_json()
        model = data.get("model", "qwen3:32b")
        prompt = data.get("prompt", "")
        response_schema = data.get("response_schema")
        timeout = data.get("timeout", 180)
        
        logger.info(f"ğŸ“¥ [Request {request_id}] JSON ìƒì„± ìš”ì²­ (model={model}, prompt_len={len(prompt)})")
        
        # Ollama ìš”ì²­ í˜ì´ë¡œë“œ êµ¬ì„±
        ollama_payload = {
            "model": model,
            "prompt": prompt,
            "options": data.get("options", {"temperature": 0.2, "num_ctx": 8192}),
        }
        
        with request_lock:  # ìˆœì°¨ ì²˜ë¦¬ ë³´ì¥
            result = call_ollama_with_breaker("/api/generate", ollama_payload, timeout=timeout)
        
        # ì‘ë‹µ íŒŒì‹±
        response_text = result.get("response", "")
        
        # JSON ì¶”ì¶œ (ë§ˆí¬ë‹¤ìš´ ì½”ë“œë¸”ë¡ ì²˜ë¦¬)
        json_content = _extract_json_from_response(response_text)
        
        if json_content is None:
            raise ValueError(f"JSON íŒŒì‹± ì‹¤íŒ¨: {response_text[:200]}")
        
        elapsed_ms = (time.time() - start_time) * 1000
        stats['successful_requests'] += 1
        _update_avg_response_time(elapsed_ms)
        
        logger.info(f"âœ… [Request {request_id}] ì™„ë£Œ ({elapsed_ms:.0f}ms)")
        
        stats['request_history'].append({
            "id": request_id,
            "type": "generate-json",
            "model": model,
            "status": "success",
            "elapsed_ms": round(elapsed_ms, 0),
            "timestamp": datetime.now(timezone.utc).isoformat(),
        })
        
        return jsonify({
            "parsed_json": json_content,
            "raw_response": response_text,
            "model": model,
            "elapsed_ms": round(elapsed_ms, 0),
        })
        
    except CircuitBreakerError:
        stats['failed_requests'] += 1
        logger.error(f"âŒ [Request {request_id}] Circuit Breaker OPEN - ìš”ì²­ ê±°ë¶€")
        return jsonify({
            "error": "Service temporarily unavailable (Circuit Breaker Open)",
            "retry_after": ollama_circuit_breaker.reset_timeout,
        }), 503
        
    except requests.exceptions.Timeout:
        stats['failed_requests'] += 1
        logger.error(f"âŒ [Request {request_id}] Timeout")
        return jsonify({"error": "Ollama request timeout"}), 504
        
    except ValueError as e:
        stats['failed_requests'] += 1
        logger.error(f"âŒ [Request {request_id}] JSON Parse Error: {e}")
        return jsonify({"error": str(e)}), 422
        
    except Exception as e:
        stats['failed_requests'] += 1
        logger.error(f"âŒ [Request {request_id}] Error: {e}")
        return jsonify({"error": str(e)}), 500
        
    finally:
        stats['queue_depth'] -= 1


@app.route("/api/models", methods=["GET"])
def list_models():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ (Ollama /api/tags Proxy)"""
    try:
        response = requests.get(f"{OLLAMA_HOST}/api/tags", timeout=10)
        response.raise_for_status()
        return jsonify(response.json())
    except Exception as e:
        logger.error(f"âŒ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return jsonify({"error": str(e)}), 500


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

def _extract_json_from_response(response_text: str) -> Optional[Dict]:
    """ì‘ë‹µ í…ìŠ¤íŠ¸ì—ì„œ JSON ì¶”ì¶œ (ë§ˆí¬ë‹¤ìš´ ì½”ë“œë¸”ë¡ ì²˜ë¦¬)"""
    content = response_text.strip()
    
    # <think>...</think> íƒœê·¸ ì œê±° (Qwen3 íŠ¹ì„±)
    if "<think>" in content and "</think>" in content:
        think_end = content.find("</think>") + len("</think>")
        content = content[think_end:].strip()
    
    # ë§ˆí¬ë‹¤ìš´ ì½”ë“œë¸”ë¡ ì¶”ì¶œ
    if "```json" in content:
        start = content.find("```json") + 7
        end = content.find("```", start)
        if end > start:
            content = content[start:end].strip()
    elif "```" in content:
        start = content.find("```") + 3
        end = content.find("```", start)
        if end > start:
            content = content[start:end].strip()
    
    # JSON íŒŒì‹± ì‹œë„
    try:
        return json.loads(content)
    except json.JSONDecodeError:
        # { } ë²”ìœ„ ì¶”ì¶œ ì‹œë„
        if "{" in content and "}" in content:
            start = content.find("{")
            end = content.rfind("}") + 1
            try:
                return json.loads(content[start:end])
            except json.JSONDecodeError:
                pass
        return None


def _update_avg_response_time(new_time_ms: float):
    """í‰ê·  ì‘ë‹µ ì‹œê°„ ì—…ë°ì´íŠ¸ (ì§€ìˆ˜ ì´ë™ í‰ê· )"""
    alpha = 0.1  # ìŠ¤ë¬´ë”© ê³„ìˆ˜
    if stats['avg_response_time_ms'] == 0:
        stats['avg_response_time_ms'] = new_time_ms
    else:
        stats['avg_response_time_ms'] = (alpha * new_time_ms) + ((1 - alpha) * stats['avg_response_time_ms'])


# Rate Limit ì´ˆê³¼ ì—ëŸ¬ í•¸ë“¤ëŸ¬
@app.errorhandler(429)
def ratelimit_handler(e):
    stats['rate_limited_requests'] += 1
    logger.warning(f"âš ï¸ Rate limit exceeded: {e.description}")
    return jsonify({
        "error": "Rate limit exceeded",
        "message": "Ollama Gateway is busy. Please retry after a moment.",
        "retry_after": 5,
    }), 429


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ë©”ì¸ ì‹¤í–‰
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

if __name__ == "__main__":
    # ê°œë°œ ëª¨ë“œ ì‹¤í–‰
    app.run(host="0.0.0.0", port=11500, debug=False)
