"""
services/ollama-gateway/main.py - Ollama Local LLM Gateway
============================================================

ì´ ì„œë¹„ìŠ¤ëŠ” Local LLM (Ollama/Qwen3) ìš”ì²­ì„ ì¤‘ì•™í™”í•˜ì—¬ ìˆœì°¨ ì²˜ë¦¬í•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
---------
1. ìš”ì²­ íì‰: ì—¬ëŸ¬ ì„œë¹„ìŠ¤ì˜ ë™ì‹œ ìš”ì²­ì„ ìˆœì°¨ ì²˜ë¦¬
2. Rate Limiting: Ollama ê³¼ë¶€í•˜ ë°©ì§€
3. Circuit Breaker: ì¥ì•  ì „íŒŒ ì°¨ë‹¨
4. í†µê³„/ëª¨ë‹ˆí„°ë§: ìš”ì²­ í˜„í™© ì¤‘ì•™ ê´€ë¦¬

API Endpoints:
-------------
- POST /api/generate      - í…ìŠ¤íŠ¸ ìƒì„± ìš”ì²­
- POST /api/generate-json - JSON ìƒì„± ìš”ì²­ (ìŠ¤í‚¤ë§ˆ ê²€ì¦ í¬í•¨)
- GET  /health            - í—¬ìŠ¤ ì²´í¬
- GET  /stats             - ìš”ì²­ í†µê³„

ì°¸ì¡°:
----
- KIS Gateway íŒ¨í„´ ì ìš© (services/kis-gateway/main.py)
"""

import json
import logging
import os
import time
import threading
from collections import deque
from datetime import datetime, timezone
from functools import wraps
from typing import Dict, Any, Optional

from flask import Flask, request, jsonify
from flask_limiter import Limiter
from flask_limiter.util import get_remote_address
from pybreaker import CircuitBreaker, CircuitBreakerError, CircuitBreakerListener
import requests

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - [%(funcName)s] - %(message)s'
)
logger = logging.getLogger("ollama-gateway")

app = Flask(__name__)

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
OLLAMA_HOST = os.getenv("OLLAMA_HOST", "http://localhost:11434")
REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:6379/0")

# Rate Limit ì„¤ì •
# Qwen3:32b ì²˜ë¦¬ ì†ë„ ê³ ë ¤ (ì•½ 10-30ì´ˆ/ìš”ì²­)
RATE_LIMIT = os.getenv("OLLAMA_RATE_LIMIT", "60 per minute")

logger.info(f"ğŸš€ Ollama Gateway ì‹œì‘")
logger.info(f"   OLLAMA_HOST: {OLLAMA_HOST}")
logger.info(f"   REDIS_URL: {REDIS_URL}")
logger.info(f"   RATE_LIMIT: {RATE_LIMIT}")


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# Rate Limiter ì„¤ì •
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

def get_global_key():
    """
    ëª¨ë“  í´ë¼ì´ì–¸íŠ¸ì˜ ìš”ì²­ì„ í•˜ë‚˜ì˜ ë²„í‚·ìœ¼ë¡œ í†µí•©í•˜ê¸° ìœ„í•œ Key í•¨ìˆ˜.
    OllamaëŠ” ë‹¨ì¼ GPUì—ì„œ ì‹¤í–‰ë˜ë¯€ë¡œ IP ê¸°ë°˜ì´ ì•„ë‹Œ ì „ì—­ í‚¤ë¥¼ ì‚¬ìš©í•´ì•¼ í•¨.
    """
    return "ollama_global"


limiter = Limiter(
    app=app,
    key_func=get_global_key,  # â­ï¸ ì¤‘ìš”: IP ê¸°ë°˜ì´ ì•„ë‹Œ ì „ì—­ í‚¤ ì‚¬ìš©
    storage_uri=REDIS_URL,
    default_limits=[],  # ì—”ë“œí¬ì¸íŠ¸ë³„ë¡œ ê°œë³„ ì„¤ì •
    strategy="fixed-window"
)


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# Circuit Breaker ì„¤ì •
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

class GatewayCircuitBreakerListener(CircuitBreakerListener):
    """Circuit Breaker ìƒíƒœ ë³€ê²½ ê°ì§€ ë¦¬ìŠ¤ë„ˆ"""
    
    def state_change(self, breaker, old, new):
        logger.warning(f"ğŸ”Œ [Circuit Breaker] ìƒíƒœ ë³€ê²½: {old.name} â†’ {new.name}")
        if new.name == "open":
            stats['circuit_breaker_open_count'] += 1


ollama_circuit_breaker = CircuitBreaker(
    fail_max=int(os.getenv('OLLAMA_CIRCUIT_FAIL_MAX', '5')),  # 5íšŒ ì—°ì† ì‹¤íŒ¨ ì‹œ OPEN
    reset_timeout=int(os.getenv('OLLAMA_CIRCUIT_RESET_TIMEOUT', '120')),  # 2ë¶„ í›„ HALF-OPEN
    listeners=[GatewayCircuitBreakerListener()]
)


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# í†µê³„
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

stats = {
    'total_requests': 0,
    'successful_requests': 0,
    'failed_requests': 0,
    'rate_limited_requests': 0,
    'circuit_breaker_open_count': 0,
    'avg_response_time_ms': 0,
    'request_history': deque(maxlen=100),  # ìµœê·¼ 100ê°œ ìš”ì²­ ê¸°ë¡
    'queue_depth': 0,  # í˜„ì¬ ëŒ€ê¸° ì¤‘ì¸ ìš”ì²­ ìˆ˜
}

# ìš”ì²­ í ì„¸ë§ˆí¬ì–´ (ë™ì‹œ ì²˜ë¦¬ ì œì–´)
max_concurrent = int(os.getenv("OLLAMA_MAX_CONCURRENT_REQUESTS", "3"))
request_lock = threading.BoundedSemaphore(value=max_concurrent)


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# Ollama API í˜¸ì¶œ ë˜í¼
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

def call_ollama_with_breaker(endpoint: str, payload: Dict[str, Any], timeout: int = 600) -> Dict[str, Any]:
    """
    Circuit Breakerë¥¼ ì ìš©í•œ Ollama API í˜¸ì¶œ ë˜í¼
    
    Args:
        endpoint: API ì—”ë“œí¬ì¸íŠ¸ (ì˜ˆ: /api/generate)
        payload: ìš”ì²­ í˜ì´ë¡œë“œ
        timeout: ìš”ì²­ íƒ€ì„ì•„ì›ƒ (ì´ˆ)
    
    Returns:
        Ollama API ì‘ë‹µ
    """
    url = f"{OLLAMA_HOST}{endpoint}"
    
    # ìŠ¤íŠ¸ë¦¬ë° ë¹„í™œì„±í™”, ëª¨ë¸ ìœ ì§€
    payload["stream"] = False
    payload["keep_alive"] = -1
    
    @ollama_circuit_breaker
    def _call():
        response = requests.post(url, json=payload, timeout=timeout)
        response.raise_for_status()
        return response.json()
    
    return _call()


def check_ollama_health() -> bool:
    """Ollama ì„œë²„ í—¬ìŠ¤ ì²´í¬"""
    try:
        response = requests.get(f"{OLLAMA_HOST}/api/tags", timeout=5)
        return response.status_code == 200
    except Exception:
        return False


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# Health Check
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

@app.route("/health", methods=["GET"])
def health():
    """í—¬ìŠ¤ ì²´í¬"""
    ollama_healthy = check_ollama_health()
    circuit_state = ollama_circuit_breaker.current_state
    
    status = "healthy" if ollama_healthy and circuit_state != "open" else "degraded"
    
    return jsonify({
        "status": status,
        "service": "ollama-gateway",
        "ollama_status": "connected" if ollama_healthy else "disconnected",
        "circuit_breaker": str(circuit_state),
        "queue_depth": stats['queue_depth'],
        "timestamp": datetime.now(timezone.utc).isoformat(),
    }), 200 if status == "healthy" else 503


@app.route("/stats", methods=["GET"])
def get_stats():
    """ìš”ì²­ í†µê³„ ì¡°íšŒ"""
    return jsonify({
        "total_requests": stats['total_requests'],
        "successful_requests": stats['successful_requests'],
        "failed_requests": stats['failed_requests'],
        "rate_limited_requests": stats['rate_limited_requests'],
        "circuit_breaker_open_count": stats['circuit_breaker_open_count'],
        "avg_response_time_ms": round(stats['avg_response_time_ms'], 2),
        "queue_depth": stats['queue_depth'],
        "circuit_breaker_state": str(ollama_circuit_breaker.current_state),
        "recent_requests": list(stats['request_history'])[-10:],  # ìµœê·¼ 10ê°œ
    })


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# API Endpoints
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

@app.route("/api/generate", methods=["POST"])
@limiter.limit(RATE_LIMIT)
def generate():
    """
    í…ìŠ¤íŠ¸ ìƒì„± ìš”ì²­ (Ollama /api/generate Proxy)
    
    Request Body:
    {
        "model": "qwen3:32b",
        "prompt": "Hello, world!",
        "options": {"temperature": 0.2}
    }
    """
    stats['total_requests'] += 1
    stats['queue_depth'] += 1
    
    start_time = time.time()
    request_id = f"gen_{int(start_time * 1000)}"
    
    try:
        data = request.get_json()
        model = data.get("model", "qwen3:32b")
        prompt = data.get("prompt", "")
        
        logger.info(f"ğŸ“¥ [Request {request_id}] í…ìŠ¤íŠ¸ ìƒì„± ìš”ì²­ (model={model}, prompt_len={len(prompt)})")
        
        with request_lock:  # ìˆœì°¨ ì²˜ë¦¬ ë³´ì¥
            result = call_ollama_with_breaker("/api/generate", data)
        
        elapsed_ms = (time.time() - start_time) * 1000
        stats['successful_requests'] += 1
        _update_avg_response_time(elapsed_ms)
        
        logger.info(f"âœ… [Request {request_id}] ì™„ë£Œ ({elapsed_ms:.0f}ms)")
        
        stats['request_history'].append({
            "id": request_id,
            "type": "generate",
            "model": model,
            "status": "success",
            "elapsed_ms": round(elapsed_ms, 0),
            "timestamp": datetime.now(timezone.utc).isoformat(),
        })
        
        return jsonify(result)
        
    except CircuitBreakerError:
        stats['failed_requests'] += 1
        logger.error(f"âŒ [Request {request_id}] Circuit Breaker OPEN - ìš”ì²­ ê±°ë¶€")
        return jsonify({
            "error": "Service temporarily unavailable (Circuit Breaker Open)",
            "retry_after": ollama_circuit_breaker.reset_timeout,
        }), 503
        
    except requests.exceptions.Timeout:
        stats['failed_requests'] += 1
        logger.error(f"âŒ [Request {request_id}] Timeout")
        return jsonify({"error": "Ollama request timeout"}), 504
        
    except Exception as e:
        stats['failed_requests'] += 1
        logger.error(f"âŒ [Request {request_id}] Error: {e}")
        return jsonify({"error": str(e)}), 500
        
    finally:
        stats['queue_depth'] -= 1

@app.route("/api/chat", methods=["POST"])
@limiter.limit(RATE_LIMIT)
def chat():
    """
    ì±„íŒ… ì™„ë£Œ ìš”ì²­ (Ollama /api/chat Proxy)
    
    Request Body:
    {
        "model": "qwen3:32b",
        "messages": [
            {"role": "user", "content": "Hello"}
        ],
        "options": {"temperature": 0.2}
    }
    """
    stats['total_requests'] += 1
    stats['queue_depth'] += 1
    
    start_time = time.time()
    request_id = f"chat_{int(start_time * 1000)}"
    
    try:
        data = request.get_json()
        model = data.get("model", "qwen3:32b")
        messages = data.get("messages", [])
        
        logger.info(f"ğŸ“¥ [Request {request_id}] ì±„íŒ… ìš”ì²­ (model={model}, msgs={len(messages)})")
        
        with request_lock:  # ìˆœì°¨ ì²˜ë¦¬ ë³´ì¥
            result = call_ollama_with_breaker("/api/chat", data)
        
        elapsed_ms = (time.time() - start_time) * 1000
        stats['successful_requests'] += 1
        _update_avg_response_time(elapsed_ms)
        
        logger.info(f"âœ… [Request {request_id}] ì™„ë£Œ ({elapsed_ms:.0f}ms)")
        
        stats['request_history'].append({
            "id": request_id,
            "type": "chat",
            "model": model,
            "status": "success",
            "elapsed_ms": round(elapsed_ms, 0),
            "timestamp": datetime.now(timezone.utc).isoformat(),
        })
        
        return jsonify(result)
        
    except CircuitBreakerError:
        stats['failed_requests'] += 1
        logger.error(f"âŒ [Request {request_id}] Circuit Breaker OPEN - ìš”ì²­ ê±°ë¶€")
        return jsonify({
            "error": "Service temporarily unavailable (Circuit Breaker Open)",
            "retry_after": ollama_circuit_breaker.reset_timeout,
        }), 503
        
    except requests.exceptions.Timeout:
        stats['failed_requests'] += 1
        logger.error(f"âŒ [Request {request_id}] Timeout")
        return jsonify({"error": "Ollama request timeout"}), 504
        
    except Exception as e:
        stats['failed_requests'] += 1
        logger.error(f"âŒ [Request {request_id}] Error: {e}")
        return jsonify({"error": str(e)}), 500
        
    finally:
        stats['queue_depth'] -= 1


@app.route("/api/embed", methods=["POST"])
@app.route("/api/embeddings", methods=["POST"])
@limiter.limit(RATE_LIMIT)
def embed():
    """
    ì„ë² ë”© ìƒì„± ìš”ì²­ (Ollama /api/embed Proxy)
    
    Request Body:
    {
        "model": "daynice/kure-v1",
        "input": "Hello world" or ["Hello", "world"]
    }
    """
    stats['total_requests'] += 1
    stats['queue_depth'] += 1
    
    start_time = time.time()
    request_id = f"emb_{int(start_time * 1000)}"
    
    try:
        data = request.get_json()
        model = data.get("model", "unknown")
        input_data = data.get("input", "")
        
        # input ê¸¸ì´ ë¡œê¹… (ë¬¸ìì—´ ë˜ëŠ” ë¦¬ìŠ¤íŠ¸)
        input_len = len(input_data) if isinstance(input_data, list) else len(str(input_data))
        logger.info(f"ğŸ“¥ [Request {request_id}] ì„ë² ë”© ìš”ì²­ (model={model}, input_len={input_len})")
        
        # Endpoint determination based on request path
        # langchain_ollama uses /api/embed (new), others might use /api/embeddings (old)
        target_endpoint = request.path
        
        with request_lock:  # ìˆœì°¨ ì²˜ë¦¬ ë³´ì¥
            result = call_ollama_with_breaker(target_endpoint, data)
        
        elapsed_ms = (time.time() - start_time) * 1000
        stats['successful_requests'] += 1
        _update_avg_response_time(elapsed_ms)
        
        logger.info(f"âœ… [Request {request_id}] ì™„ë£Œ ({elapsed_ms:.0f}ms)")
        
        stats['request_history'].append({
            "id": request_id,
            "type": "embed",
            "model": model,
            "status": "success",
            "elapsed_ms": round(elapsed_ms, 0),
            "timestamp": datetime.now(timezone.utc).isoformat(),
        })
        
        return jsonify(result)
        
    except CircuitBreakerError:
        stats['failed_requests'] += 1
        logger.error(f"âŒ [Request {request_id}] Circuit Breaker OPEN - ìš”ì²­ ê±°ë¶€")
        return jsonify({
            "error": "Service temporarily unavailable (Circuit Breaker Open)",
            "retry_after": ollama_circuit_breaker.reset_timeout,
        }), 503
        
    except requests.exceptions.Timeout:
        stats['failed_requests'] += 1
        logger.error(f"âŒ [Request {request_id}] Timeout")
        return jsonify({"error": "Ollama request timeout"}), 504
        
    except Exception as e:
        stats['failed_requests'] += 1
        logger.error(f"âŒ [Request {request_id}] Error: {e}")
        return jsonify({"error": str(e)}), 500
        
    finally:
        stats['queue_depth'] -= 1
@limiter.limit(RATE_LIMIT)
def generate_json():
    """
    JSON ìƒì„± ìš”ì²­ (ìŠ¤í‚¤ë§ˆ ê²€ì¦ í¬í•¨)
    
    Request Body:
    {
        "model": "qwen3:32b",
        "prompt": "...",
        "response_schema": {...},
        "options": {"temperature": 0.2}
    }
    """
    stats['total_requests'] += 1
    stats['queue_depth'] += 1
    
    start_time = time.time()
    request_id = f"json_{int(start_time * 1000)}"
    
    try:
        data = request.get_json()
        model = data.get("model", "qwen3:32b")
        prompt = data.get("prompt", "")
        response_schema = data.get("response_schema")
        timeout = data.get("timeout", 180)
        
        logger.info(f"ğŸ“¥ [Request {request_id}] JSON ìƒì„± ìš”ì²­ (model={model}, prompt_len={len(prompt)})")
        
        # Ollama ìš”ì²­ í˜ì´ë¡œë“œ êµ¬ì„±
        ollama_payload = {
            "model": model,
            "prompt": prompt,
            "options": data.get("options", {"temperature": 0.2, "num_ctx": 8192}),
        }
        
        with request_lock:  # ìˆœì°¨ ì²˜ë¦¬ ë³´ì¥
            result = call_ollama_with_breaker("/api/generate", ollama_payload, timeout=timeout)
        
        # ì‘ë‹µ íŒŒì‹±
        response_text = result.get("response", "")
        
        # JSON ì¶”ì¶œ (ë§ˆí¬ë‹¤ìš´ ì½”ë“œë¸”ë¡ ì²˜ë¦¬)
        json_content = _extract_json_from_response(response_text)
        
        if json_content is None:
            raise ValueError(f"JSON íŒŒì‹± ì‹¤íŒ¨: {response_text[:200]}")
        
        elapsed_ms = (time.time() - start_time) * 1000
        stats['successful_requests'] += 1
        _update_avg_response_time(elapsed_ms)
        
        logger.info(f"âœ… [Request {request_id}] ì™„ë£Œ ({elapsed_ms:.0f}ms)")
        
        stats['request_history'].append({
            "id": request_id,
            "type": "generate-json",
            "model": model,
            "status": "success",
            "elapsed_ms": round(elapsed_ms, 0),
            "timestamp": datetime.now(timezone.utc).isoformat(),
        })
        
        return jsonify({
            "parsed_json": json_content,
            "raw_response": response_text,
            "model": model,
            "elapsed_ms": round(elapsed_ms, 0),
        })
        
    except CircuitBreakerError:
        stats['failed_requests'] += 1
        logger.error(f"âŒ [Request {request_id}] Circuit Breaker OPEN - ìš”ì²­ ê±°ë¶€")
        return jsonify({
            "error": "Service temporarily unavailable (Circuit Breaker Open)",
            "retry_after": ollama_circuit_breaker.reset_timeout,
        }), 503
        
    except requests.exceptions.Timeout:
        stats['failed_requests'] += 1
        logger.error(f"âŒ [Request {request_id}] Timeout")
        return jsonify({"error": "Ollama request timeout"}), 504
        
    except ValueError as e:
        stats['failed_requests'] += 1
        logger.error(f"âŒ [Request {request_id}] JSON Parse Error: {e}")
        return jsonify({"error": str(e)}), 422
        
    except Exception as e:
        stats['failed_requests'] += 1
        logger.error(f"âŒ [Request {request_id}] Error: {e}")
        return jsonify({"error": str(e)}), 500
        
    finally:
        stats['queue_depth'] -= 1


@app.route("/api/models", methods=["GET"])
def list_models():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ (Ollama /api/tags Proxy)"""
    try:
        response = requests.get(f"{OLLAMA_HOST}/api/tags", timeout=10)
        response.raise_for_status()
        return jsonify(response.json())
    except Exception as e:
        logger.error(f"âŒ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return jsonify({"error": str(e)}), 500


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

def _extract_json_from_response(response_text: str) -> Optional[Dict]:
    """ì‘ë‹µ í…ìŠ¤íŠ¸ì—ì„œ JSON ì¶”ì¶œ (ë§ˆí¬ë‹¤ìš´ ì½”ë“œë¸”ë¡ ì²˜ë¦¬)"""
    content = response_text.strip()
    
    # <think>...</think> íƒœê·¸ ì œê±° (Qwen3 íŠ¹ì„±)
    if "<think>" in content and "</think>" in content:
        think_end = content.find("</think>") + len("</think>")
        content = content[think_end:].strip()
    
    # ë§ˆí¬ë‹¤ìš´ ì½”ë“œë¸”ë¡ ì¶”ì¶œ
    if "```json" in content:
        start = content.find("```json") + 7
        end = content.find("```", start)
        if end > start:
            content = content[start:end].strip()
    elif "```" in content:
        start = content.find("```") + 3
        end = content.find("```", start)
        if end > start:
            content = content[start:end].strip()
    
    # JSON íŒŒì‹± ì‹œë„
    try:
        return json.loads(content)
    except json.JSONDecodeError:
        # { } ë²”ìœ„ ì¶”ì¶œ ì‹œë„
        if "{" in content and "}" in content:
            start = content.find("{")
            end = content.rfind("}") + 1
            try:
                return json.loads(content[start:end])
            except json.JSONDecodeError:
                pass
        return None


def _update_avg_response_time(new_time_ms: float):
    """í‰ê·  ì‘ë‹µ ì‹œê°„ ì—…ë°ì´íŠ¸ (ì§€ìˆ˜ ì´ë™ í‰ê· )"""
    alpha = 0.1  # ìŠ¤ë¬´ë”© ê³„ìˆ˜
    if stats['avg_response_time_ms'] == 0:
        stats['avg_response_time_ms'] = new_time_ms
    else:
        stats['avg_response_time_ms'] = (alpha * new_time_ms) + ((1 - alpha) * stats['avg_response_time_ms'])


# Rate Limit ì´ˆê³¼ ì—ëŸ¬ í•¸ë“¤ëŸ¬
@app.errorhandler(429)
def ratelimit_handler(e):
    stats['rate_limited_requests'] += 1
    logger.warning(f"âš ï¸ Rate limit exceeded: {e.description}")
    return jsonify({
        "error": "Rate limit exceeded",
        "message": "Ollama Gateway is busy. Please retry after a moment.",
        "retry_after": 5,
    }), 429


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ë©”ì¸ ì‹¤í–‰
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

if __name__ == "__main__":
    # ê°œë°œ ëª¨ë“œ ì‹¤í–‰
    app.run(host="0.0.0.0", port=11500, debug=False)
