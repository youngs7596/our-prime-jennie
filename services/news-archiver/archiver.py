#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
services/news-archiver/archiver.py
-----------------------------------
ë‰´ìŠ¤ ì•„ì¹´ì´ë¹™ ì „ìš© ì„œë¹„ìŠ¤ (Consumer A).
Redis Streamsì—ì„œ ë‰´ìŠ¤ë¥¼ ì†Œë¹„í•˜ì—¬ ChromaDBì— ì €ì¥í•©ë‹ˆë‹¤.
LLM ë¶„ì„ ì—†ì´ ë¹ ë¥´ê²Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
"""

import os
import sys
import logging
from typing import Dict, Any

from dotenv import load_dotenv

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì„¤ì •
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, PROJECT_ROOT)

from shared.messaging.stream_client import (
    consume_messages, 
    get_stream_length, 
    get_pending_count,
    STREAM_NEWS_RAW, 
    GROUP_ARCHIVER
)

# ==============================================================================
# Logging
# ==============================================================================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - [%(funcName)s] - %(message)s',
)
logger = logging.getLogger(__name__)

# ==============================================================================
# Configuration
# ==============================================================================
load_dotenv()

QDRANT_HOST = os.getenv("QDRANT_HOST", "localhost")
QDRANT_PORT = int(os.getenv("QDRANT_PORT", "6333"))
COLLECTION_NAME = os.getenv("CHROMA_COLLECTION_NAME", "rag_stock_data") # Maintain env var for compat or rename? Let's keep it but comment.

# Embedding Model
EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "jhgan/ko-sroberta-multitask")

# ==============================================================================
# ChromaDB & Embedding Initialization
# ==============================================================================

_vectorstore = None
_text_splitter = None
_qdrant_client = None


def get_vectorstore():
    """Qdrant Vectorstore ì‹±ê¸€í†¤ ë°˜í™˜"""
    global _vectorstore, _text_splitter, _qdrant_client

    if _vectorstore is None:
        from langchain_qdrant import QdrantVectorStore
        from qdrant_client import QdrantClient
        from langchain_openai import OpenAIEmbeddings
        from langchain_text_splitters import RecursiveCharacterTextSplitter

        # Qdrant Connection (Port 6333)
        QDRANT_HOST = os.getenv("QDRANT_HOST", "localhost")
        QDRANT_PORT = int(os.getenv("QDRANT_PORT", "6333"))

        logger.info(f"ğŸ”Œ Qdrant ì—°ê²° ì¤‘... ({QDRANT_HOST}:{QDRANT_PORT})")

        # Embeddings (vLLM ì§ì ‘ í˜¸ì¶œ â€” OpenAI-compatible API)
        vllm_embed_url = os.getenv("VLLM_EMBED_URL", "http://localhost:8002/v1")
        embeddings = OpenAIEmbeddings(
            base_url=vllm_embed_url,
            api_key="EMPTY",  # vLLMì€ ì¸ì¦ ë¶ˆí•„ìš”
            model="nlpai-lab/KURE-v1",
        )

        from qdrant_client.http import models

        # Qdrant Client
        _qdrant_client = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)

        # Ensure Collection Exists (+ source_url ì¸ë±ìŠ¤)
        if not _qdrant_client.collection_exists(COLLECTION_NAME):
            logger.info(f"ğŸ†• Qdrant Collection ìƒì„±: {COLLECTION_NAME} (size=1024)")
            _qdrant_client.create_collection(
                collection_name=COLLECTION_NAME,
                vectors_config=models.VectorParams(
                    size=1024,  # kure-v1 dimension
                    distance=models.Distance.COSINE
                )
            )

        # source_url í˜ì´ë¡œë“œ ì¸ë±ìŠ¤ ìƒì„± (ì¤‘ë³µ ì²´í¬ ì„±ëŠ¥)
        try:
            _qdrant_client.create_payload_index(
                collection_name=COLLECTION_NAME,
                field_name="metadata.source_url",
                field_schema=models.PayloadSchemaType.KEYWORD,
            )
            logger.info("âœ… source_url í˜ì´ë¡œë“œ ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
        except Exception:
            pass  # ì´ë¯¸ ì¡´ì¬í•˜ë©´ ë¬´ì‹œ

        # Vectorstore
        _vectorstore = QdrantVectorStore(
            client=_qdrant_client,
            collection_name=COLLECTION_NAME,
            embedding=embeddings,
        )

        # Text Splitter
        _text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=50
        )

        logger.info(f"âœ… Qdrant ì—°ê²° ì™„ë£Œ (collection: {COLLECTION_NAME})")

    return _vectorstore, _text_splitter


# ==============================================================================
# Message Handler
# ==============================================================================

def _check_duplicate(source_url: str) -> bool:
    """source_url ê¸°ë°˜ Qdrant ì¤‘ë³µ ì²´í¬"""
    if not source_url or not _qdrant_client:
        return False
    try:
        from qdrant_client.http import models
        result = _qdrant_client.scroll(
            collection_name=COLLECTION_NAME,
            scroll_filter=models.Filter(
                must=[models.FieldCondition(
                    key="metadata.source_url",
                    match=models.MatchValue(value=source_url),
                )]
            ),
            limit=1,
            with_payload=False,
            with_vectors=False,
        )
        return len(result[0]) > 0
    except Exception as e:
        logger.warning(f"âš ï¸ [Archive] ì¤‘ë³µ ì²´í¬ ì‹¤íŒ¨ (ì €ì¥ ì§„í–‰): {e}")
        return False


def handle_archive_message(page_content: str, metadata: Dict[str, Any]) -> bool:
    """
    ë‰´ìŠ¤ ë©”ì‹œì§€ë¥¼ Qdrantì— ì €ì¥í•©ë‹ˆë‹¤. (source_url ì¤‘ë³µ ì²´í¬ í¬í•¨)

    Args:
        page_content: ë‰´ìŠ¤ ë³¸ë¬¸
        metadata: ë©”íƒ€ë°ì´í„°

    Returns:
        ì„±ê³µ ì—¬ë¶€
    """
    try:
        from langchain_core.documents import Document

        vectorstore, text_splitter = get_vectorstore()

        # source_url ì¤‘ë³µ ì²´í¬
        source_url = metadata.get("source_url", "")
        if source_url and _check_duplicate(source_url):
            logger.debug(f"â„¹ï¸ [Archive] ì¤‘ë³µ ë‰´ìŠ¤ ìŠ¤í‚µ: {metadata.get('stock_name', '?')} - {source_url[:60]}")
            return True  # ACK (ìŠ¤íŠ¸ë¦¼ì—ì„œ ì œê±°)

        # Create Document
        doc = Document(page_content=page_content, metadata=metadata)

        # Split and embed
        chunks = text_splitter.split_documents([doc])

        # Add to vectorstore
        vectorstore.add_documents(chunks)

        stock_info = f"{metadata.get('stock_name', 'General')} ({metadata.get('stock_code', 'N/A')})"
        logger.debug(f"âœ… [Archive] ì €ì¥ ì™„ë£Œ: {stock_info}")

        return True

    except Exception as e:
        logger.error(f"âŒ [Archive] ì €ì¥ ì‹¤íŒ¨: {e}")
        return False


# ==============================================================================
# Main Archiver
# ==============================================================================

def run_archiver_daemon(consumer_name: str = "archiver_1"):
    """
    Archiver ë°ëª¬ ì‹¤í–‰ (ë¬´í•œ ë£¨í”„)
    Redis Streamì—ì„œ ë©”ì‹œì§€ë¥¼ ì†Œë¹„í•˜ì—¬ ChromaDBì— ì €ì¥í•©ë‹ˆë‹¤.
    """
    logger.info("=" * 60)
    logger.info("ğŸš€ [Archiver Daemon] ì‹œì‘")
    logger.info(f"   Stream: {STREAM_NEWS_RAW}")
    logger.info(f"   Group: {GROUP_ARCHIVER}")
    logger.info(f"   Consumer: {consumer_name}")
    logger.info(f"   Qdrant: {QDRANT_HOST}:{QDRANT_PORT}")
    logger.info("=" * 60)
    
    # Pre-initialize VectorDB connection
    try:
        get_vectorstore()
    except Exception as e:
        logger.error(f"âŒ Qdrant ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return
    
    # Start consuming
    processed = consume_messages(
        group_name=GROUP_ARCHIVER,
        consumer_name=consumer_name,
        handler=handle_archive_message,
        stream_name=STREAM_NEWS_RAW,
        batch_size=20,  # Archiver is fast, process more at once
        block_ms=2000,
        max_iterations=None  # Infinite
    )
    
    logger.info(f"âœ… [Archiver] ì¢…ë£Œ - ì´ {processed}ê°œ ì²˜ë¦¬")


def run_archiver_once(max_messages: int = 1000):
    """
    Archiver 1íšŒ ì‹¤í–‰ (Airflow Taskìš©)
    """
    logger.info(f"ğŸš€ [Archiver] 1íšŒ ì‹¤í–‰ (max: {max_messages})")
    
    stream_len = get_stream_length(STREAM_NEWS_RAW)
    pending = get_pending_count(STREAM_NEWS_RAW, GROUP_ARCHIVER)
    logger.info(f"ğŸ“Š Stream ìƒíƒœ: ê¸¸ì´={stream_len}, ëŒ€ê¸°={pending}")
    
    try:
        get_vectorstore()
    except Exception as e:
        logger.error(f"âŒ ChromaDB ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return
    
    # Calculate iterations (batch_size=20)
    iterations = (max_messages // 20) + 1
    
    processed = consume_messages(
        group_name=GROUP_ARCHIVER,
        consumer_name="archiver_batch",
        handler=handle_archive_message,
        stream_name=STREAM_NEWS_RAW,
        batch_size=20,
        block_ms=1000,
        max_iterations=iterations
    )
    
    logger.info(f"âœ… [Archiver] ì™„ë£Œ - {processed}ê°œ ì²˜ë¦¬")


if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--daemon", action="store_true", help="Run as daemon (infinite loop)")
    parser.add_argument("--name", type=str, default="archiver_1", help="Consumer name")
    parser.add_argument("--max", type=int, default=1000, help="Max messages for one-shot mode")
    args = parser.parse_args()
    
    if args.daemon:
        run_archiver_daemon(args.name)
    else:
        run_archiver_once(args.max)
